<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>audiocraft.optim.dadam API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>audiocraft.optim.dadam</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import logging
from typing import Any

import torch
import torch.optim
import torch.distributed as dist


logger = logging.getLogger(__name__)
_params_t = Any


def to_real(x):
    if torch.is_complex(x):
        return x.real
    else:
        return x


class DAdaptAdam(torch.optim.Optimizer):
    &#34;&#34;&#34;Adam with D-Adaptation automatic step-sizes.
    Leave LR set to 1 unless you encounter instability.

    Args:
        params (iterable):
            Iterable of parameters to optimize or dicts defining parameter groups.
        lr (float):
            Learning rate adjustment parameter. Increases or decreases the D-adapted learning rate.
        betas (tuple[float, float], optional): coefficients used for computing
            running averages of gradient and its square (default: (0.9, 0.999))
        momentum (float):
            Momentum value in  the range [0,1) (default: 0.9).
        eps (float):
            Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-8).
        weight_decay (float):
            Weight decay, i.e. a L2 penalty (default: 0).
        log_every (int):
            Log using print every k steps, default 0 (no logging).
        decouple (boolean):
            Use AdamW style decoupled weight decay
        d0 (float):
            Initial D estimate for D-adaptation (default 1e-6). Rarely needs changing.
        growth_rate (float):
            prevent the D estimate from growing faster than this multiplicative rate.
            Default is inf, for unrestricted. Values like 1.02 give a kind of learning
            rate warmup effect.
        fsdp_in_use (bool):
            If you&#39;re using sharded parameters, this should be set to True. The optimizer
            will attempt to auto-detect this, but if you&#39;re using an implementation other
            than PyTorch&#39;s builtin version, the auto-detection won&#39;t work.
    &#34;&#34;&#34;
    def __init__(self, params, lr=1.0,
                 betas=(0.9, 0.999),
                 eps=1e-8,
                 weight_decay=0,
                 log_every=0,
                 decouple=True,
                 d0=1e-6,
                 growth_rate=float(&#39;inf&#39;)):
        if not 0.0 &lt; d0:
            raise ValueError(&#34;Invalid d0 value: {}&#34;.format(d0))
        if not 0.0 &lt; lr:
            raise ValueError(&#34;Invalid learning rate: {}&#34;.format(lr))
        if not 0.0 &lt; eps:
            raise ValueError(&#34;Invalid epsilon value: {}&#34;.format(eps))
        if not 0.0 &lt;= betas[0] &lt; 1.0:
            raise ValueError(&#34;Invalid beta parameter at index 0: {}&#34;.format(betas[0]))
        if not 0.0 &lt;= betas[1] &lt; 1.0:
            raise ValueError(&#34;Invalid beta parameter at index 1: {}&#34;.format(betas[1]))

        if decouple:
            logger.info(&#34;Using decoupled weight decay&#34;)

        from .fsdp import is_fsdp_used
        fsdp_in_use = is_fsdp_used()
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay,
                        d=d0,
                        k=0,
                        gsq_weighted=0.0,
                        log_every=log_every,
                        decouple=decouple,
                        growth_rate=growth_rate,
                        fsdp_in_use=fsdp_in_use)

        super().__init__(params, defaults)

    @property
    def supports_memory_efficient_fp16(self):
        return False

    @property
    def supports_flat_params(self):
        return True

    def step(self, closure=None):
        &#34;&#34;&#34;Performs a single optimization step.

        Args:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        &#34;&#34;&#34;
        loss = None
        if closure is not None:
            loss = closure()

        g_sq = 0.0
        sksq_weighted = 0.0
        sk_l1 = 0.0

        lr = max(group[&#39;lr&#39;] for group in self.param_groups)

        group = self.param_groups[0]
        gsq_weighted = group[&#39;gsq_weighted&#39;]
        d = group[&#39;d&#39;]
        dlr = d*lr

        growth_rate = group[&#39;growth_rate&#39;]
        decouple = group[&#39;decouple&#39;]
        fsdp_in_use = group[&#39;fsdp_in_use&#39;]
        log_every = group[&#39;log_every&#39;]

        beta1, beta2 = group[&#39;betas&#39;]

        for group in self.param_groups:
            group_lr = group[&#39;lr&#39;]
            decay = group[&#39;weight_decay&#39;]
            k = group[&#39;k&#39;]
            eps = group[&#39;eps&#39;]

            if group_lr not in [lr, 0.0]:
                raise RuntimeError(&#34;Setting different lr values in different parameter &#34;
                                   &#34;groups is only supported for values of 0&#34;)

            for p in group[&#39;params&#39;]:
                if p.grad is None:
                    continue
                if hasattr(p, &#34;_fsdp_flattened&#34;):
                    fsdp_in_use = True
                grad = p.grad.data

                # Apply weight decay (coupled variant)
                if decay != 0 and not decouple:
                    grad.add_(p.data, alpha=decay)

                state = self.state[p]

                # State initialization
                if &#39;step&#39; not in state:
                    state[&#39;step&#39;] = 0
                    state[&#39;s&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format).detach()
                    # Exponential moving average of gradient values
                    state[&#39;exp_avg&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format).detach()
                    # Exponential moving average of squared gradient values
                    state[&#39;exp_avg_sq&#39;] = torch.zeros_like(
                        to_real(p.data), memory_format=torch.preserve_format).detach()

                exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]

                grad_grad = to_real(grad * grad.conj())

                # Adam EMA updates
                if group_lr &gt; 0:
                    exp_avg.mul_(beta1).add_(grad, alpha=dlr*(1-beta1))
                    exp_avg_sq.mul_(beta2).add_(grad_grad, alpha=1-beta2)

                    denom = exp_avg_sq.sqrt().add_(eps)

                    g_sq += grad_grad.div_(denom).sum().item()

                    s = state[&#39;s&#39;]
                    s.mul_(beta2).add_(grad, alpha=dlr*(1-beta2))
                    sksq_weighted += to_real(s * s.conj()).div_(denom).sum().item()
                    sk_l1 += s.abs().sum().item()

            ######

        gsq_weighted = beta2*gsq_weighted + g_sq*(dlr**2)*(1-beta2)
        d_hat = d

        # if we have not done any progres, return
        # if we have any gradients available, will have sk_l1 &gt; 0 (unless \|g\|=0)
        if sk_l1 == 0:
            return loss

        if lr &gt; 0.0:
            if fsdp_in_use:
                dist_tensor = torch.zeros(3, device=&#39;cuda&#39;)
                dist_tensor[0] = sksq_weighted
                dist_tensor[1] = gsq_weighted
                dist_tensor[2] = sk_l1
                dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM)
                global_sksq_weighted = dist_tensor[0]
                global_gsq_weighted = dist_tensor[1]
                global_sk_l1 = dist_tensor[2]
            else:
                global_sksq_weighted = sksq_weighted
                global_gsq_weighted = gsq_weighted
                global_sk_l1 = sk_l1

            d_hat = (global_sksq_weighted/(1-beta2) - global_gsq_weighted)/global_sk_l1
            d = max(d, min(d_hat, d*growth_rate))

        if log_every &gt; 0 and k % log_every == 0:
            logger.info(
                f&#34;(k={k}) dlr: {dlr:1.1e} d_hat: {d_hat:1.1e}, d: {d:1.8}. &#34;
                f&#34;sksq_weighted={global_sksq_weighted:1.1e} gsq_weighted={global_gsq_weighted:1.1e} &#34;
                f&#34;sk_l1={global_sk_l1:1.1e}{&#39; (FSDP)&#39; if fsdp_in_use else &#39;&#39;}&#34;)

        for group in self.param_groups:
            group[&#39;gsq_weighted&#39;] = gsq_weighted
            group[&#39;d&#39;] = d

            group_lr = group[&#39;lr&#39;]
            decay = group[&#39;weight_decay&#39;]
            k = group[&#39;k&#39;]
            eps = group[&#39;eps&#39;]

            for p in group[&#39;params&#39;]:
                if p.grad is None:
                    continue
                grad = p.grad.data

                state = self.state[p]

                exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]

                state[&#39;step&#39;] += 1

                denom = exp_avg_sq.sqrt().add_(eps)
                denom = denom.type(p.type())

                # Apply weight decay (decoupled variant)
                if decay != 0 and decouple and group_lr &gt; 0:
                    p.data.add_(p.data, alpha=-decay * dlr)

                # Take step
                p.data.addcdiv_(exp_avg, denom, value=-1)

            group[&#39;k&#39;] = k + 1

        return loss</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="audiocraft.optim.dadam.to_real"><code class="name flex">
<span>def <span class="ident">to_real</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_real(x):
    if torch.is_complex(x):
        return x.real
    else:
        return x</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="audiocraft.optim.dadam.DAdaptAdam"><code class="flex name class">
<span>class <span class="ident">DAdaptAdam</span></span>
<span>(</span><span>params, lr=1.0, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, log_every=0, decouple=True, d0=1e-06, growth_rate=inf)</span>
</code></dt>
<dd>
<div class="desc"><p>Adam with D-Adaptation automatic step-sizes.
Leave LR set to 1 unless you encounter instability.</p>
<h2 id="args">Args</h2>
<dl>
<dt>params (iterable):</dt>
<dt>Iterable of parameters to optimize or dicts defining parameter groups.</dt>
<dt>lr (float):</dt>
<dt>Learning rate adjustment parameter. Increases or decreases the D-adapted learning rate.</dt>
<dt><strong><code>betas</code></strong> :&ensp;<code>tuple[float, float]</code>, optional</dt>
<dd>coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999))</dd>
</dl>
<p>momentum (float):
Momentum value in
the range [0,1) (default: 0.9).
eps (float):
Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-8).
weight_decay (float):
Weight decay, i.e. a L2 penalty (default: 0).
log_every (int):
Log using print every k steps, default 0 (no logging).
decouple (boolean):
Use AdamW style decoupled weight decay
d0 (float):
Initial D estimate for D-adaptation (default 1e-6). Rarely needs changing.
growth_rate (float):
prevent the D estimate from growing faster than this multiplicative rate.
Default is inf, for unrestricted. Values like 1.02 give a kind of learning
rate warmup effect.
fsdp_in_use (bool):
If you're using sharded parameters, this should be set to True. The optimizer
will attempt to auto-detect this, but if you're using an implementation other
than PyTorch's builtin version, the auto-detection won't work.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DAdaptAdam(torch.optim.Optimizer):
    &#34;&#34;&#34;Adam with D-Adaptation automatic step-sizes.
    Leave LR set to 1 unless you encounter instability.

    Args:
        params (iterable):
            Iterable of parameters to optimize or dicts defining parameter groups.
        lr (float):
            Learning rate adjustment parameter. Increases or decreases the D-adapted learning rate.
        betas (tuple[float, float], optional): coefficients used for computing
            running averages of gradient and its square (default: (0.9, 0.999))
        momentum (float):
            Momentum value in  the range [0,1) (default: 0.9).
        eps (float):
            Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-8).
        weight_decay (float):
            Weight decay, i.e. a L2 penalty (default: 0).
        log_every (int):
            Log using print every k steps, default 0 (no logging).
        decouple (boolean):
            Use AdamW style decoupled weight decay
        d0 (float):
            Initial D estimate for D-adaptation (default 1e-6). Rarely needs changing.
        growth_rate (float):
            prevent the D estimate from growing faster than this multiplicative rate.
            Default is inf, for unrestricted. Values like 1.02 give a kind of learning
            rate warmup effect.
        fsdp_in_use (bool):
            If you&#39;re using sharded parameters, this should be set to True. The optimizer
            will attempt to auto-detect this, but if you&#39;re using an implementation other
            than PyTorch&#39;s builtin version, the auto-detection won&#39;t work.
    &#34;&#34;&#34;
    def __init__(self, params, lr=1.0,
                 betas=(0.9, 0.999),
                 eps=1e-8,
                 weight_decay=0,
                 log_every=0,
                 decouple=True,
                 d0=1e-6,
                 growth_rate=float(&#39;inf&#39;)):
        if not 0.0 &lt; d0:
            raise ValueError(&#34;Invalid d0 value: {}&#34;.format(d0))
        if not 0.0 &lt; lr:
            raise ValueError(&#34;Invalid learning rate: {}&#34;.format(lr))
        if not 0.0 &lt; eps:
            raise ValueError(&#34;Invalid epsilon value: {}&#34;.format(eps))
        if not 0.0 &lt;= betas[0] &lt; 1.0:
            raise ValueError(&#34;Invalid beta parameter at index 0: {}&#34;.format(betas[0]))
        if not 0.0 &lt;= betas[1] &lt; 1.0:
            raise ValueError(&#34;Invalid beta parameter at index 1: {}&#34;.format(betas[1]))

        if decouple:
            logger.info(&#34;Using decoupled weight decay&#34;)

        from .fsdp import is_fsdp_used
        fsdp_in_use = is_fsdp_used()
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay,
                        d=d0,
                        k=0,
                        gsq_weighted=0.0,
                        log_every=log_every,
                        decouple=decouple,
                        growth_rate=growth_rate,
                        fsdp_in_use=fsdp_in_use)

        super().__init__(params, defaults)

    @property
    def supports_memory_efficient_fp16(self):
        return False

    @property
    def supports_flat_params(self):
        return True

    def step(self, closure=None):
        &#34;&#34;&#34;Performs a single optimization step.

        Args:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        &#34;&#34;&#34;
        loss = None
        if closure is not None:
            loss = closure()

        g_sq = 0.0
        sksq_weighted = 0.0
        sk_l1 = 0.0

        lr = max(group[&#39;lr&#39;] for group in self.param_groups)

        group = self.param_groups[0]
        gsq_weighted = group[&#39;gsq_weighted&#39;]
        d = group[&#39;d&#39;]
        dlr = d*lr

        growth_rate = group[&#39;growth_rate&#39;]
        decouple = group[&#39;decouple&#39;]
        fsdp_in_use = group[&#39;fsdp_in_use&#39;]
        log_every = group[&#39;log_every&#39;]

        beta1, beta2 = group[&#39;betas&#39;]

        for group in self.param_groups:
            group_lr = group[&#39;lr&#39;]
            decay = group[&#39;weight_decay&#39;]
            k = group[&#39;k&#39;]
            eps = group[&#39;eps&#39;]

            if group_lr not in [lr, 0.0]:
                raise RuntimeError(&#34;Setting different lr values in different parameter &#34;
                                   &#34;groups is only supported for values of 0&#34;)

            for p in group[&#39;params&#39;]:
                if p.grad is None:
                    continue
                if hasattr(p, &#34;_fsdp_flattened&#34;):
                    fsdp_in_use = True
                grad = p.grad.data

                # Apply weight decay (coupled variant)
                if decay != 0 and not decouple:
                    grad.add_(p.data, alpha=decay)

                state = self.state[p]

                # State initialization
                if &#39;step&#39; not in state:
                    state[&#39;step&#39;] = 0
                    state[&#39;s&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format).detach()
                    # Exponential moving average of gradient values
                    state[&#39;exp_avg&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format).detach()
                    # Exponential moving average of squared gradient values
                    state[&#39;exp_avg_sq&#39;] = torch.zeros_like(
                        to_real(p.data), memory_format=torch.preserve_format).detach()

                exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]

                grad_grad = to_real(grad * grad.conj())

                # Adam EMA updates
                if group_lr &gt; 0:
                    exp_avg.mul_(beta1).add_(grad, alpha=dlr*(1-beta1))
                    exp_avg_sq.mul_(beta2).add_(grad_grad, alpha=1-beta2)

                    denom = exp_avg_sq.sqrt().add_(eps)

                    g_sq += grad_grad.div_(denom).sum().item()

                    s = state[&#39;s&#39;]
                    s.mul_(beta2).add_(grad, alpha=dlr*(1-beta2))
                    sksq_weighted += to_real(s * s.conj()).div_(denom).sum().item()
                    sk_l1 += s.abs().sum().item()

            ######

        gsq_weighted = beta2*gsq_weighted + g_sq*(dlr**2)*(1-beta2)
        d_hat = d

        # if we have not done any progres, return
        # if we have any gradients available, will have sk_l1 &gt; 0 (unless \|g\|=0)
        if sk_l1 == 0:
            return loss

        if lr &gt; 0.0:
            if fsdp_in_use:
                dist_tensor = torch.zeros(3, device=&#39;cuda&#39;)
                dist_tensor[0] = sksq_weighted
                dist_tensor[1] = gsq_weighted
                dist_tensor[2] = sk_l1
                dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM)
                global_sksq_weighted = dist_tensor[0]
                global_gsq_weighted = dist_tensor[1]
                global_sk_l1 = dist_tensor[2]
            else:
                global_sksq_weighted = sksq_weighted
                global_gsq_weighted = gsq_weighted
                global_sk_l1 = sk_l1

            d_hat = (global_sksq_weighted/(1-beta2) - global_gsq_weighted)/global_sk_l1
            d = max(d, min(d_hat, d*growth_rate))

        if log_every &gt; 0 and k % log_every == 0:
            logger.info(
                f&#34;(k={k}) dlr: {dlr:1.1e} d_hat: {d_hat:1.1e}, d: {d:1.8}. &#34;
                f&#34;sksq_weighted={global_sksq_weighted:1.1e} gsq_weighted={global_gsq_weighted:1.1e} &#34;
                f&#34;sk_l1={global_sk_l1:1.1e}{&#39; (FSDP)&#39; if fsdp_in_use else &#39;&#39;}&#34;)

        for group in self.param_groups:
            group[&#39;gsq_weighted&#39;] = gsq_weighted
            group[&#39;d&#39;] = d

            group_lr = group[&#39;lr&#39;]
            decay = group[&#39;weight_decay&#39;]
            k = group[&#39;k&#39;]
            eps = group[&#39;eps&#39;]

            for p in group[&#39;params&#39;]:
                if p.grad is None:
                    continue
                grad = p.grad.data

                state = self.state[p]

                exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]

                state[&#39;step&#39;] += 1

                denom = exp_avg_sq.sqrt().add_(eps)
                denom = denom.type(p.type())

                # Apply weight decay (decoupled variant)
                if decay != 0 and decouple and group_lr &gt; 0:
                    p.data.add_(p.data, alpha=-decay * dlr)

                # Take step
                p.data.addcdiv_(exp_avg, denom, value=-1)

            group[&#39;k&#39;] = k + 1

        return loss</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.optim.optimizer.Optimizer</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="audiocraft.optim.dadam.DAdaptAdam.OptimizerPostHook"><code class="name">var <span class="ident">OptimizerPostHook</span> : typing_extensions.TypeAlias</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.optim.dadam.DAdaptAdam.OptimizerPreHook"><code class="name">var <span class="ident">OptimizerPreHook</span> : typing_extensions.TypeAlias</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="audiocraft.optim.dadam.DAdaptAdam.supports_flat_params"><code class="name">var <span class="ident">supports_flat_params</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def supports_flat_params(self):
    return True</code></pre>
</details>
</dd>
<dt id="audiocraft.optim.dadam.DAdaptAdam.supports_memory_efficient_fp16"><code class="name">var <span class="ident">supports_memory_efficient_fp16</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def supports_memory_efficient_fp16(self):
    return False</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="audiocraft.optim.dadam.DAdaptAdam.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, closure=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a single optimization step.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>closure</code></strong> :&ensp;<code>callable</code>, optional</dt>
<dd>A closure that reevaluates the model
and returns the loss.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, closure=None):
    &#34;&#34;&#34;Performs a single optimization step.

    Args:
        closure (callable, optional): A closure that reevaluates the model
            and returns the loss.
    &#34;&#34;&#34;
    loss = None
    if closure is not None:
        loss = closure()

    g_sq = 0.0
    sksq_weighted = 0.0
    sk_l1 = 0.0

    lr = max(group[&#39;lr&#39;] for group in self.param_groups)

    group = self.param_groups[0]
    gsq_weighted = group[&#39;gsq_weighted&#39;]
    d = group[&#39;d&#39;]
    dlr = d*lr

    growth_rate = group[&#39;growth_rate&#39;]
    decouple = group[&#39;decouple&#39;]
    fsdp_in_use = group[&#39;fsdp_in_use&#39;]
    log_every = group[&#39;log_every&#39;]

    beta1, beta2 = group[&#39;betas&#39;]

    for group in self.param_groups:
        group_lr = group[&#39;lr&#39;]
        decay = group[&#39;weight_decay&#39;]
        k = group[&#39;k&#39;]
        eps = group[&#39;eps&#39;]

        if group_lr not in [lr, 0.0]:
            raise RuntimeError(&#34;Setting different lr values in different parameter &#34;
                               &#34;groups is only supported for values of 0&#34;)

        for p in group[&#39;params&#39;]:
            if p.grad is None:
                continue
            if hasattr(p, &#34;_fsdp_flattened&#34;):
                fsdp_in_use = True
            grad = p.grad.data

            # Apply weight decay (coupled variant)
            if decay != 0 and not decouple:
                grad.add_(p.data, alpha=decay)

            state = self.state[p]

            # State initialization
            if &#39;step&#39; not in state:
                state[&#39;step&#39;] = 0
                state[&#39;s&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format).detach()
                # Exponential moving average of gradient values
                state[&#39;exp_avg&#39;] = torch.zeros_like(p.data, memory_format=torch.preserve_format).detach()
                # Exponential moving average of squared gradient values
                state[&#39;exp_avg_sq&#39;] = torch.zeros_like(
                    to_real(p.data), memory_format=torch.preserve_format).detach()

            exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]

            grad_grad = to_real(grad * grad.conj())

            # Adam EMA updates
            if group_lr &gt; 0:
                exp_avg.mul_(beta1).add_(grad, alpha=dlr*(1-beta1))
                exp_avg_sq.mul_(beta2).add_(grad_grad, alpha=1-beta2)

                denom = exp_avg_sq.sqrt().add_(eps)

                g_sq += grad_grad.div_(denom).sum().item()

                s = state[&#39;s&#39;]
                s.mul_(beta2).add_(grad, alpha=dlr*(1-beta2))
                sksq_weighted += to_real(s * s.conj()).div_(denom).sum().item()
                sk_l1 += s.abs().sum().item()

        ######

    gsq_weighted = beta2*gsq_weighted + g_sq*(dlr**2)*(1-beta2)
    d_hat = d

    # if we have not done any progres, return
    # if we have any gradients available, will have sk_l1 &gt; 0 (unless \|g\|=0)
    if sk_l1 == 0:
        return loss

    if lr &gt; 0.0:
        if fsdp_in_use:
            dist_tensor = torch.zeros(3, device=&#39;cuda&#39;)
            dist_tensor[0] = sksq_weighted
            dist_tensor[1] = gsq_weighted
            dist_tensor[2] = sk_l1
            dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM)
            global_sksq_weighted = dist_tensor[0]
            global_gsq_weighted = dist_tensor[1]
            global_sk_l1 = dist_tensor[2]
        else:
            global_sksq_weighted = sksq_weighted
            global_gsq_weighted = gsq_weighted
            global_sk_l1 = sk_l1

        d_hat = (global_sksq_weighted/(1-beta2) - global_gsq_weighted)/global_sk_l1
        d = max(d, min(d_hat, d*growth_rate))

    if log_every &gt; 0 and k % log_every == 0:
        logger.info(
            f&#34;(k={k}) dlr: {dlr:1.1e} d_hat: {d_hat:1.1e}, d: {d:1.8}. &#34;
            f&#34;sksq_weighted={global_sksq_weighted:1.1e} gsq_weighted={global_gsq_weighted:1.1e} &#34;
            f&#34;sk_l1={global_sk_l1:1.1e}{&#39; (FSDP)&#39; if fsdp_in_use else &#39;&#39;}&#34;)

    for group in self.param_groups:
        group[&#39;gsq_weighted&#39;] = gsq_weighted
        group[&#39;d&#39;] = d

        group_lr = group[&#39;lr&#39;]
        decay = group[&#39;weight_decay&#39;]
        k = group[&#39;k&#39;]
        eps = group[&#39;eps&#39;]

        for p in group[&#39;params&#39;]:
            if p.grad is None:
                continue
            grad = p.grad.data

            state = self.state[p]

            exp_avg, exp_avg_sq = state[&#39;exp_avg&#39;], state[&#39;exp_avg_sq&#39;]

            state[&#39;step&#39;] += 1

            denom = exp_avg_sq.sqrt().add_(eps)
            denom = denom.type(p.type())

            # Apply weight decay (decoupled variant)
            if decay != 0 and decouple and group_lr &gt; 0:
                p.data.add_(p.data, alpha=-decay * dlr)

            # Take step
            p.data.addcdiv_(exp_avg, denom, value=-1)

        group[&#39;k&#39;] = k + 1

    return loss</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="audiocraft.optim" href="index.html">audiocraft.optim</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="audiocraft.optim.dadam.to_real" href="#audiocraft.optim.dadam.to_real">to_real</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="audiocraft.optim.dadam.DAdaptAdam" href="#audiocraft.optim.dadam.DAdaptAdam">DAdaptAdam</a></code></h4>
<ul class="">
<li><code><a title="audiocraft.optim.dadam.DAdaptAdam.OptimizerPostHook" href="#audiocraft.optim.dadam.DAdaptAdam.OptimizerPostHook">OptimizerPostHook</a></code></li>
<li><code><a title="audiocraft.optim.dadam.DAdaptAdam.OptimizerPreHook" href="#audiocraft.optim.dadam.DAdaptAdam.OptimizerPreHook">OptimizerPreHook</a></code></li>
<li><code><a title="audiocraft.optim.dadam.DAdaptAdam.step" href="#audiocraft.optim.dadam.DAdaptAdam.step">step</a></code></li>
<li><code><a title="audiocraft.optim.dadam.DAdaptAdam.supports_flat_params" href="#audiocraft.optim.dadam.DAdaptAdam.supports_flat_params">supports_flat_params</a></code></li>
<li><code><a title="audiocraft.optim.dadam.DAdaptAdam.supports_memory_efficient_fp16" href="#audiocraft.optim.dadam.DAdaptAdam.supports_memory_efficient_fp16">supports_memory_efficient_fp16</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>