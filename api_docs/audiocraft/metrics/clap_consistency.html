<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>audiocraft.metrics.clap_consistency API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>audiocraft.metrics.clap_consistency</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from pathlib import Path
import typing as tp

import torch
import torchmetrics
from transformers import RobertaTokenizer  # type: ignore

from ..data.audio_utils import convert_audio
from ..environment import AudioCraftEnvironment
from ..utils.utils import load_clap_state_dict

try:
    import laion_clap  # type: ignore
except ImportError:
    laion_clap = None


class TextConsistencyMetric(torchmetrics.Metric):
    &#34;&#34;&#34;Text consistency metric measuring consistency between audio and text pairs.&#34;&#34;&#34;

    def update(self, audio: torch.Tensor, text: tp.List[str], sizes: torch.Tensor, sample_rates: torch.Tensor) -&gt; None:
        raise NotImplementedError(&#34;implement how to update the metric from the audio and text pairs.&#34;)

    def compute(self):
        raise NotImplementedError(&#34;implement how to compute the final metric score.&#34;)


class CLAPTextConsistencyMetric(TextConsistencyMetric):
    &#34;&#34;&#34;Text consistency metric relying on Contrastive Language-Audio Pretraining (CLAP).

    This metric is similar to the MuLan Cycle Consistency from MusicLM (https://arxiv.org/pdf/2301.11325.pdf)
    or the CLAP score used in Make-An-Audio (https://arxiv.org/pdf/2301.12661v1.pdf).

    As a joint audio-text embedding model, a pretrained CLAP model can be used to quantify the
    similarity between audio-text pairs. We compute the CLAP embeddings from the text descriptions as
    well as the generated audio based on them, and define the MCC metric as the average cosine similarity
    between these embeddings.

    Model implementation &amp; pre-trained checkpoints: https://github.com/LAION-AI/CLAP
    &#34;&#34;&#34;
    def __init__(self, model_path: tp.Union[str, Path], model_arch: str = &#39;HTSAT-tiny&#39;, enable_fusion: bool = False):
        super().__init__()
        if laion_clap is None:
            raise ImportError(&#34;Please install CLAP to compute text consistency: &#39;pip install laion_clap&#39;&#34;)
        self.add_state(&#34;cosine_sum&#34;, default=torch.tensor(0.), dist_reduce_fx=&#34;sum&#34;)
        self.add_state(&#34;weight&#34;, default=torch.tensor(0.), dist_reduce_fx=&#34;sum&#34;)
        self._initialize_model(model_path, model_arch, enable_fusion)

    def _initialize_model(self, model_path: tp.Union[str, Path], model_arch: str, enable_fusion: bool):
        model_path = AudioCraftEnvironment.resolve_reference_path(model_path)
        self.tokenize = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)
        self.model = laion_clap.CLAP_Module(enable_fusion=enable_fusion, amodel=model_arch)
        self.model_sample_rate = 48_000
        load_clap_state_dict(self.model, model_path)
        self.model.eval()

    def _tokenizer(self, texts: tp.Union[str, tp.List[str]]) -&gt; dict:
        # we use the default params from CLAP module here as well
        return self.tokenize(texts, padding=&#34;max_length&#34;, truncation=True, max_length=77, return_tensors=&#34;pt&#34;)

    def update(self, audio: torch.Tensor, text: tp.List[str], sizes: torch.Tensor, sample_rates: torch.Tensor) -&gt; None:
        &#34;&#34;&#34;Compute cosine similarity between audio and text pairs and accumulate scores over the dataset.&#34;&#34;&#34;
        assert audio.size(0) == len(text), &#34;Number of audio and text samples should match&#34;
        assert torch.all(sample_rates == sample_rates[0].item()), &#34;All items in batch should have the same sample rate&#34;
        sample_rate = int(sample_rates[0].item())
        # convert audio batch to 48kHz monophonic audio with no channel dimension: [B, C, T] -&gt; [B, T]
        audio = convert_audio(audio, from_rate=sample_rate, to_rate=self.model_sample_rate, to_channels=1).mean(dim=1)
        audio_embeddings = self.model.get_audio_embedding_from_data(audio, use_tensor=True)
        text_embeddings = self.model.get_text_embedding(text, tokenizer=self._tokenizer, use_tensor=True)
        # cosine similarity between the text and the audio embedding
        cosine_sim = torch.nn.functional.cosine_similarity(audio_embeddings, text_embeddings, dim=1, eps=1e-8)
        self.cosine_sum += cosine_sim.sum(dim=0)
        self.weight += torch.tensor(cosine_sim.size(0))

    def compute(self):
        &#34;&#34;&#34;Computes the average cosine similarty across all audio/text pairs.&#34;&#34;&#34;
        assert self.weight.item() &gt; 0, &#34;Unable to compute with total number of comparisons &lt;= 0&#34;  # type: ignore
        return (self.cosine_sum / self.weight).item()  # type: ignore</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric"><code class="flex name class">
<span>class <span class="ident">CLAPTextConsistencyMetric</span></span>
<span>(</span><span>model_path: Union[str, pathlib.Path], model_arch: str = 'HTSAT-tiny', enable_fusion: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Text consistency metric relying on Contrastive Language-Audio Pretraining (CLAP).</p>
<p>This metric is similar to the MuLan Cycle Consistency from MusicLM (<a href="https://arxiv.org/pdf/2301.11325.pdf">https://arxiv.org/pdf/2301.11325.pdf</a>)
or the CLAP score used in Make-An-Audio (<a href="https://arxiv.org/pdf/2301.12661v1.pdf">https://arxiv.org/pdf/2301.12661v1.pdf</a>).</p>
<p>As a joint audio-text embedding model, a pretrained CLAP model can be used to quantify the
similarity between audio-text pairs. We compute the CLAP embeddings from the text descriptions as
well as the generated audio based on them, and define the MCC metric as the average cosine similarity
between these embeddings.</p>
<p>Model implementation &amp; pre-trained checkpoints: <a href="https://github.com/LAION-AI/CLAP">https://github.com/LAION-AI/CLAP</a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CLAPTextConsistencyMetric(TextConsistencyMetric):
    &#34;&#34;&#34;Text consistency metric relying on Contrastive Language-Audio Pretraining (CLAP).

    This metric is similar to the MuLan Cycle Consistency from MusicLM (https://arxiv.org/pdf/2301.11325.pdf)
    or the CLAP score used in Make-An-Audio (https://arxiv.org/pdf/2301.12661v1.pdf).

    As a joint audio-text embedding model, a pretrained CLAP model can be used to quantify the
    similarity between audio-text pairs. We compute the CLAP embeddings from the text descriptions as
    well as the generated audio based on them, and define the MCC metric as the average cosine similarity
    between these embeddings.

    Model implementation &amp; pre-trained checkpoints: https://github.com/LAION-AI/CLAP
    &#34;&#34;&#34;
    def __init__(self, model_path: tp.Union[str, Path], model_arch: str = &#39;HTSAT-tiny&#39;, enable_fusion: bool = False):
        super().__init__()
        if laion_clap is None:
            raise ImportError(&#34;Please install CLAP to compute text consistency: &#39;pip install laion_clap&#39;&#34;)
        self.add_state(&#34;cosine_sum&#34;, default=torch.tensor(0.), dist_reduce_fx=&#34;sum&#34;)
        self.add_state(&#34;weight&#34;, default=torch.tensor(0.), dist_reduce_fx=&#34;sum&#34;)
        self._initialize_model(model_path, model_arch, enable_fusion)

    def _initialize_model(self, model_path: tp.Union[str, Path], model_arch: str, enable_fusion: bool):
        model_path = AudioCraftEnvironment.resolve_reference_path(model_path)
        self.tokenize = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)
        self.model = laion_clap.CLAP_Module(enable_fusion=enable_fusion, amodel=model_arch)
        self.model_sample_rate = 48_000
        load_clap_state_dict(self.model, model_path)
        self.model.eval()

    def _tokenizer(self, texts: tp.Union[str, tp.List[str]]) -&gt; dict:
        # we use the default params from CLAP module here as well
        return self.tokenize(texts, padding=&#34;max_length&#34;, truncation=True, max_length=77, return_tensors=&#34;pt&#34;)

    def update(self, audio: torch.Tensor, text: tp.List[str], sizes: torch.Tensor, sample_rates: torch.Tensor) -&gt; None:
        &#34;&#34;&#34;Compute cosine similarity between audio and text pairs and accumulate scores over the dataset.&#34;&#34;&#34;
        assert audio.size(0) == len(text), &#34;Number of audio and text samples should match&#34;
        assert torch.all(sample_rates == sample_rates[0].item()), &#34;All items in batch should have the same sample rate&#34;
        sample_rate = int(sample_rates[0].item())
        # convert audio batch to 48kHz monophonic audio with no channel dimension: [B, C, T] -&gt; [B, T]
        audio = convert_audio(audio, from_rate=sample_rate, to_rate=self.model_sample_rate, to_channels=1).mean(dim=1)
        audio_embeddings = self.model.get_audio_embedding_from_data(audio, use_tensor=True)
        text_embeddings = self.model.get_text_embedding(text, tokenizer=self._tokenizer, use_tensor=True)
        # cosine similarity between the text and the audio embedding
        cosine_sim = torch.nn.functional.cosine_similarity(audio_embeddings, text_embeddings, dim=1, eps=1e-8)
        self.cosine_sum += cosine_sim.sum(dim=0)
        self.weight += torch.tensor(cosine_sim.size(0))

    def compute(self):
        &#34;&#34;&#34;Computes the average cosine similarty across all audio/text pairs.&#34;&#34;&#34;
        assert self.weight.item() &gt; 0, &#34;Unable to compute with total number of comparisons &lt;= 0&#34;  # type: ignore
        return (self.cosine_sum / self.weight).item()  # type: ignore</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="audiocraft.metrics.clap_consistency.TextConsistencyMetric" href="#audiocraft.metrics.clap_consistency.TextConsistencyMetric">TextConsistencyMetric</a></li>
<li>torchmetrics.metric.Metric</li>
<li>torch.nn.modules.module.Module</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.full_state_update"><code class="name">var <span class="ident">full_state_update</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.higher_is_better"><code class="name">var <span class="ident">higher_is_better</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.is_differentiable"><code class="name">var <span class="ident">is_differentiable</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.plot_legend_name"><code class="name">var <span class="ident">plot_legend_name</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.plot_lower_bound"><code class="name">var <span class="ident">plot_lower_bound</span> : Optional[float]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.plot_upper_bound"><code class="name">var <span class="ident">plot_upper_bound</span> : Optional[float]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.compute"><code class="name flex">
<span>def <span class="ident">compute</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the average cosine similarty across all audio/text pairs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute(self):
    &#34;&#34;&#34;Computes the average cosine similarty across all audio/text pairs.&#34;&#34;&#34;
    assert self.weight.item() &gt; 0, &#34;Unable to compute with total number of comparisons &lt;= 0&#34;  # type: ignore
    return (self.cosine_sum / self.weight).item()  # type: ignore</code></pre>
</details>
</dd>
<dt id="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, audio: torch.Tensor, text: List[str], sizes: torch.Tensor, sample_rates: torch.Tensor) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Compute cosine similarity between audio and text pairs and accumulate scores over the dataset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, audio: torch.Tensor, text: tp.List[str], sizes: torch.Tensor, sample_rates: torch.Tensor) -&gt; None:
    &#34;&#34;&#34;Compute cosine similarity between audio and text pairs and accumulate scores over the dataset.&#34;&#34;&#34;
    assert audio.size(0) == len(text), &#34;Number of audio and text samples should match&#34;
    assert torch.all(sample_rates == sample_rates[0].item()), &#34;All items in batch should have the same sample rate&#34;
    sample_rate = int(sample_rates[0].item())
    # convert audio batch to 48kHz monophonic audio with no channel dimension: [B, C, T] -&gt; [B, T]
    audio = convert_audio(audio, from_rate=sample_rate, to_rate=self.model_sample_rate, to_channels=1).mean(dim=1)
    audio_embeddings = self.model.get_audio_embedding_from_data(audio, use_tensor=True)
    text_embeddings = self.model.get_text_embedding(text, tokenizer=self._tokenizer, use_tensor=True)
    # cosine similarity between the text and the audio embedding
    cosine_sim = torch.nn.functional.cosine_similarity(audio_embeddings, text_embeddings, dim=1, eps=1e-8)
    self.cosine_sum += cosine_sim.sum(dim=0)
    self.weight += torch.tensor(cosine_sim.size(0))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="audiocraft.metrics.clap_consistency.TextConsistencyMetric"><code class="flex name class">
<span>class <span class="ident">TextConsistencyMetric</span></span>
<span>(</span><span>**kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Text consistency metric measuring consistency between audio and text pairs.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TextConsistencyMetric(torchmetrics.Metric):
    &#34;&#34;&#34;Text consistency metric measuring consistency between audio and text pairs.&#34;&#34;&#34;

    def update(self, audio: torch.Tensor, text: tp.List[str], sizes: torch.Tensor, sample_rates: torch.Tensor) -&gt; None:
        raise NotImplementedError(&#34;implement how to update the metric from the audio and text pairs.&#34;)

    def compute(self):
        raise NotImplementedError(&#34;implement how to compute the final metric score.&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torchmetrics.metric.Metric</li>
<li>torch.nn.modules.module.Module</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric" href="#audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric">CLAPTextConsistencyMetric</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="audiocraft.metrics.clap_consistency.TextConsistencyMetric.full_state_update"><code class="name">var <span class="ident">full_state_update</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.metrics.clap_consistency.TextConsistencyMetric.higher_is_better"><code class="name">var <span class="ident">higher_is_better</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.metrics.clap_consistency.TextConsistencyMetric.is_differentiable"><code class="name">var <span class="ident">is_differentiable</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.metrics.clap_consistency.TextConsistencyMetric.plot_legend_name"><code class="name">var <span class="ident">plot_legend_name</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.metrics.clap_consistency.TextConsistencyMetric.plot_lower_bound"><code class="name">var <span class="ident">plot_lower_bound</span> : Optional[float]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.metrics.clap_consistency.TextConsistencyMetric.plot_upper_bound"><code class="name">var <span class="ident">plot_upper_bound</span> : Optional[float]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="audiocraft.metrics.clap_consistency.TextConsistencyMetric.compute"><code class="name flex">
<span>def <span class="ident">compute</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Override this method to compute the final metric value.</p>
<p>This method will automatically synchronize state variables when running in distributed backend.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute(self):
    raise NotImplementedError(&#34;implement how to compute the final metric score.&#34;)</code></pre>
</details>
</dd>
<dt id="audiocraft.metrics.clap_consistency.TextConsistencyMetric.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, audio: torch.Tensor, text: List[str], sizes: torch.Tensor, sample_rates: torch.Tensor) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Override this method to update the state variables of your metric class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, audio: torch.Tensor, text: tp.List[str], sizes: torch.Tensor, sample_rates: torch.Tensor) -&gt; None:
    raise NotImplementedError(&#34;implement how to update the metric from the audio and text pairs.&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="audiocraft.metrics" href="index.html">audiocraft.metrics</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric" href="#audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric">CLAPTextConsistencyMetric</a></code></h4>
<ul class="two-column">
<li><code><a title="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.compute" href="#audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.compute">compute</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.full_state_update" href="#audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.full_state_update">full_state_update</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.higher_is_better" href="#audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.higher_is_better">higher_is_better</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.is_differentiable" href="#audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.is_differentiable">is_differentiable</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.plot_legend_name" href="#audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.plot_legend_name">plot_legend_name</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.plot_lower_bound" href="#audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.plot_lower_bound">plot_lower_bound</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.plot_upper_bound" href="#audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.plot_upper_bound">plot_upper_bound</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.update" href="#audiocraft.metrics.clap_consistency.CLAPTextConsistencyMetric.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="audiocraft.metrics.clap_consistency.TextConsistencyMetric" href="#audiocraft.metrics.clap_consistency.TextConsistencyMetric">TextConsistencyMetric</a></code></h4>
<ul class="two-column">
<li><code><a title="audiocraft.metrics.clap_consistency.TextConsistencyMetric.compute" href="#audiocraft.metrics.clap_consistency.TextConsistencyMetric.compute">compute</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.TextConsistencyMetric.full_state_update" href="#audiocraft.metrics.clap_consistency.TextConsistencyMetric.full_state_update">full_state_update</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.TextConsistencyMetric.higher_is_better" href="#audiocraft.metrics.clap_consistency.TextConsistencyMetric.higher_is_better">higher_is_better</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.TextConsistencyMetric.is_differentiable" href="#audiocraft.metrics.clap_consistency.TextConsistencyMetric.is_differentiable">is_differentiable</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.TextConsistencyMetric.plot_legend_name" href="#audiocraft.metrics.clap_consistency.TextConsistencyMetric.plot_legend_name">plot_legend_name</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.TextConsistencyMetric.plot_lower_bound" href="#audiocraft.metrics.clap_consistency.TextConsistencyMetric.plot_lower_bound">plot_lower_bound</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.TextConsistencyMetric.plot_upper_bound" href="#audiocraft.metrics.clap_consistency.TextConsistencyMetric.plot_upper_bound">plot_upper_bound</a></code></li>
<li><code><a title="audiocraft.metrics.clap_consistency.TextConsistencyMetric.update" href="#audiocraft.metrics.clap_consistency.TextConsistencyMetric.update">update</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>