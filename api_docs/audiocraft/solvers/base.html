<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>audiocraft.solvers.base API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>audiocraft.solvers.base</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from abc import ABC, abstractmethod
from contextlib import contextmanager
from pathlib import Path
import typing as tp

import flashy
import omegaconf
import torch
from torch import nn

from .. import optim
from ..optim import fsdp
from ..utils import checkpoint
from ..utils.autocast import TorchAutocast
from ..utils.best_state import BestStateDictManager
from ..utils.deadlock import DeadlockDetect
from ..utils.profiler import Profiler
from ..utils.utils import copy_state, dict_from_config, model_hash, with_rank_rng


class StandardSolver(ABC, flashy.BaseSolver):
    &#34;&#34;&#34;Standard solver for AudioCraft.

    The standard solver implements a base training loop with the following stages:
    train, valid, evaluate and generate that are expected to be all defined for
    solvers in AudioCraft. It also provides a nice default management of Dora history replay,
    checkpoint management across epoch, and logging configuration.

    AudioCraft solvers must inherit from the StandardSolver and define the methods
    associated to each stage as well as the show, build_model and build_dataloaders methods.
    &#34;&#34;&#34;
    def __init__(self, cfg: omegaconf.DictConfig):
        super().__init__()
        self.logger.info(f&#34;Instantiating solver {self.__class__.__name__} for XP {self.xp.sig}&#34;)
        self.logger.info(f&#34;All XP logs are stored in {self.xp.folder}&#34;)
        self.cfg = cfg
        self.device = cfg.device
        self.model: nn.Module
        self._continue_best_source_keys = [&#39;best_state&#39;, &#39;fsdp_best_state&#39;]
        self._fsdp_modules: tp.List[fsdp.FSDP] = []
        self._ema_sources: nn.ModuleDict = nn.ModuleDict()
        self.ema: tp.Optional[optim.ModuleDictEMA] = None
        self.dataloaders: tp.Dict[str, torch.utils.data.DataLoader] = dict()
        self._log_updates = self.cfg.logging.get(&#39;log_updates&#39;, 10)
        if self.cfg.logging.log_tensorboard:
            self.init_tensorboard(**self.cfg.get(&#39;tensorboard&#39;))
        if self.cfg.logging.log_wandb and self:
            self.init_wandb(**self.cfg.get(&#39;wandb&#39;))
        # keep a copy of the best performing state for stateful objects
        # used for evaluation and generation stages
        dtype_best: tp.Optional[torch.dtype] = None
        if self.cfg.fsdp.use:
            dtype_best = getattr(torch, self.cfg.fsdp.param_dtype)  # type: ignore
            assert isinstance(dtype_best, torch.dtype)
        elif self.cfg.autocast:
            dtype_best = getattr(torch, self.cfg.autocast_dtype)  # type: ignore
            assert isinstance(dtype_best, torch.dtype)
        self.best_state: BestStateDictManager = BestStateDictManager(dtype=dtype_best)
        # Hacky support for keeping a copy of the full best state in rank0.
        self.fsdp_best_state: tp.Dict[str, tp.Any] = {}
        self.register_stateful(&#39;best_state&#39;, &#39;fsdp_best_state&#39;)  # register best_state object to keep it in state_dict
        self._new_best_state: bool = False  # should save a new checkpoint
        # instantiate datasets and appropriate number of updates per epoch
        self.build_dataloaders()
        if self.cfg.execute_only is None:
            assert &#39;train&#39; in self.dataloaders, &#34;The train dataset split must be provided.&#34;
            assert &#39;valid&#39; in self.dataloaders, &#34;The valid dataset split must be provided.&#34;
        self.train_updates_per_epoch = len(self.dataloaders[&#39;train&#39;]) if &#39;train&#39; in self.dataloaders else 0
        if self.cfg.optim.updates_per_epoch:
            self.train_updates_per_epoch = self.cfg.optim.updates_per_epoch
        self.total_updates = self.train_updates_per_epoch * self.cfg.optim.epochs
        # instantiate model &amp; exponential moving average on the model
        self.build_model()
        self.logger.info(&#34;Model hash: %s&#34;, model_hash(self.model))
        assert &#39;model&#39; in self.stateful.sources, \
            &#34;Please register the model to stateful with self.register_stateful(&#39;model&#39;) in build_model.&#34;
        self.profiler = Profiler(self.model, **self.cfg.profiler)
        self.initialize_ema()
        self.register_stateful(&#39;ema&#39;)
        assert self.ema is None or &#39;ema&#39; in self.stateful.sources, \
            &#34;Please register the ema to stateful with self.register_stateful(&#39;ema&#39;) in build_model.&#34;
        self.deadlock_detect = DeadlockDetect(**self.cfg.deadlock)
        # basic statistics on the trained model
        model_size = sum(p.numel() for p in self.model.parameters() if p.requires_grad) / 1e6
        # one copy of grad, one copy of momentum, one copy of denominator and model weights.
        # and 4 bytes for each float!
        mem_usage = model_size * 4 * 4 / 1000
        self.logger.info(&#34;Model size: %.2f M params&#34;, model_size)
        self.logger.info(&#34;Base memory usage, with model, grad and optim: %.2f GB&#34;, mem_usage)

    @property
    def autocast(self):
        &#34;&#34;&#34;Convenient autocast (or not) using the solver configuration.&#34;&#34;&#34;
        return TorchAutocast(enabled=self.cfg.autocast, device_type=self.device, dtype=self.autocast_dtype)

    def _get_state_source(self, name) -&gt; flashy.state.StateDictSource:
        # Internal utility to get a state source from the solver
        return self.stateful.sources[name]

    @property
    def best_metric_name(self) -&gt; tp.Optional[str]:
        &#34;&#34;&#34;Metric name used to identify the best state. This metric should be stored in the metrics
        used on the stage for best state identification (most likely, `valid`). If None, then
        no best state is saved.
        &#34;&#34;&#34;
        return None

    def register_best_state(self, *args: str):
        &#34;&#34;&#34;Register state sources in `BestStateDictManager` to keep their best states along with their
        latest states. The best state will be used at evaluation stages instead of the latest states.

        Shortcut around `BestStateDictManager.register` method. You can pass any number of
        attribute, included nested attributes and those will be included into the checkpoints
        and automatically restored when `BaseSolver.restore` is called.
        &#34;&#34;&#34;
        for name in args:
            state_source = self._get_state_source(name)
            assert name in self.stateful.sources, &#34;Registered states in best should be registered in stateful first!&#34;
            self.best_state.register(name, state_source)

    def register_ema(self, *args: str):
        &#34;&#34;&#34;Register state sources for exponential moving average.

        The registered sources are used to instantiate a ModuleDictEMA instance.
        The ModuleDictEMA keeps a `nn.ModuleDict` module that is updated when self.ema.step() is called
        and swapped with the original state sources with self.swap_ema_state() method.

        Usage:
            self.register_ema(&#39;model&#39;)
        &#34;&#34;&#34;
        assert self.ema is None, &#34;Cannot register state source to already instantiated EMA.&#34;
        for name in args:
            self._ema_sources[name] = getattr(self, name)

    def wrap_with_fsdp(self, model: torch.nn.Module, *args, **kwargs):
        model = fsdp.wrap_with_fsdp(self.cfg.fsdp, model, *args, **kwargs)
        if isinstance(model, fsdp.FSDP):
            self._fsdp_modules.append(model)
        return model

    def update_best_state_from_stage(self, stage_name: str = &#39;valid&#39;):
        &#34;&#34;&#34;Update latest best state based on pending metrics of a given stage. This method relies
        on the `BestStateDictManager.update` method to update the best state_dict with latest weights
        if the registered states happen to match to the best performing setup.
        &#34;&#34;&#34;
        if self.best_metric_name is None:
            # when no best metric is defined, the last state is always the best
            self._new_best_state = True
            self.logger.info(&#34;Updating best state with current state.&#34;)
        else:
            assert stage_name in self._pending_metrics, f&#34;Metrics for stage {stage_name} not found.&#34;
            assert self.best_metric_name in self._pending_metrics[stage_name], \
                f&#34;Best metric not found in {stage_name} metrics. Cannot register best state&#34;
            current_score = self._pending_metrics[stage_name][self.best_metric_name]
            all_best_metric_scores = [
                past_metrics[stage_name][self.best_metric_name]
                for past_metrics in self.history
            ]
            all_best_metric_scores.append(current_score)
            best_score = min(all_best_metric_scores)
            self._new_best_state = current_score == best_score
            if self._new_best_state:
                old_best = min(all_best_metric_scores[:-1] + [float(&#39;inf&#39;)])
                self.logger.info(
                    f&#34;New best state with {self.best_metric_name}={current_score:.3f} (was {old_best:.3f})&#34;)

        if self._new_best_state:
            if self.cfg.fsdp.use:
                # this will give an empty state dict on all ranks but the rank 0
                # which will have a copy in memory of the full model.
                with fsdp.switch_to_full_state_dict(self._fsdp_modules):
                    for name in self.best_state.states.keys():
                        state_source = self._get_state_source(name)
                        self.best_state.update(name, state_source)
                    # we save to a different dict.
                    self.fsdp_best_state.update(self.best_state.state_dict())
                # We cannot efficiently load fsdp_best_state when using FSDP,
                # so we have do do a second pass, with the local shards.
            for name in self.best_state.states.keys():
                state_source = self._get_state_source(name)
                self.best_state.update(name, state_source)

    def _load_new_state_dict(self, state_dict: dict) -&gt; dict:
        old_states = {}
        for name, new_state in state_dict.items():
            state_source = self._get_state_source(name)
            old_states[name] = copy_state(state_source.state_dict())
            state_source.load_state_dict(new_state)
        return old_states

    @contextmanager
    def swap_best_state(self):
        self.logger.debug(f&#34;Swapping to best state for: {&#39;, &#39;.join(self.best_state.state_dict().keys())}&#34;)
        old_states = self._load_new_state_dict(self.best_state.state_dict())
        try:
            yield
        finally:
            self.logger.debug(&#34;Swapping back from best to original state&#34;)
            for name, old_state in old_states.items():
                state_source = self._get_state_source(name)
                state_source.load_state_dict(old_state)

    @contextmanager
    def swap_ema_state(self):
        if self.ema is None:
            yield
        else:
            ema_state_dict = self.ema.state_dict()[&#39;state&#39;]
            self.logger.debug(f&#34;Swapping to EMA state for: {&#39;, &#39;.join(ema_state_dict.keys())}&#34;)
            old_states = self._load_new_state_dict(ema_state_dict)
            try:
                yield
            finally:
                self.logger.debug(&#34;Swapping back from EMA state to original state&#34;)
                for name, old_state in old_states.items():
                    state_source = self._get_state_source(name)
                    state_source.load_state_dict(old_state)

    @property
    def is_training(self):
        return self.current_stage == &#39;train&#39;

    def log_model_summary(self, model: nn.Module):
        &#34;&#34;&#34;Log model summary, architecture and size of the model.&#34;&#34;&#34;
        self.logger.info(model)
        mb = sum(p.numel() for p in model.parameters()) * 4 / 2 ** 20
        self.logger.info(&#34;Size: %.1f MB&#34;, mb)

    @abstractmethod
    def build_model(self):
        &#34;&#34;&#34;Method to implement to initialize model.&#34;&#34;&#34;
        ...

    def initialize_ema(self):
        &#34;&#34;&#34;Initialize exponential moving average with the registered sources.
        EMA object is created if the optim.ema.model.decay value is non-null.
        &#34;&#34;&#34;
        from .builders import get_ema
        self.ema = get_ema(self._ema_sources, self.cfg.optim.ema)
        if self.ema is None:
            self.logger.info(&#39;No EMA on the model.&#39;)
        else:
            assert self.cfg.optim.ema.updates &gt; 0
            self.logger.info(
                f&#39;Initializing EMA on the model with decay = {self.ema.decay}&#39;
                f&#39; every {self.cfg.optim.ema.updates} updates&#39;
            )

    @abstractmethod
    def build_dataloaders(self):
        &#34;&#34;&#34;Method to implement to initialize dataloaders.&#34;&#34;&#34;
        ...

    @abstractmethod
    def show(self):
        &#34;&#34;&#34;Method to log any information without running the job.&#34;&#34;&#34;
        ...

    @property
    def log_updates(self):
        # convenient access to log updates
        return self._log_updates

    def checkpoint_path(self, **kwargs):
        kwargs.setdefault(&#39;use_fsdp&#39;, self.cfg.fsdp.use)
        return self.folder / checkpoint.checkpoint_name(**kwargs)

    def epoch_checkpoint_path(self, epoch: int, **kwargs):
        kwargs.setdefault(&#39;use_fsdp&#39;, self.cfg.fsdp.use)
        return self.folder / checkpoint.checkpoint_name(str(epoch), **kwargs)

    def checkpoint_path_with_name(self, name: str, **kwargs):
        kwargs.setdefault(&#39;use_fsdp&#39;, self.cfg.fsdp.use)
        return self.folder / checkpoint.checkpoint_name(name=name, **kwargs)

    def save_checkpoints(self):
        &#34;&#34;&#34;Save checkpoint, optionally keeping a copy for a given epoch.&#34;&#34;&#34;
        is_sharded = self.cfg.fsdp.use
        if not flashy.distrib.is_rank_zero() and not is_sharded:
            return
        self.logger.info(&#34;Model hash: %s&#34;, model_hash(self.model))
        state = self.state_dict()
        epoch = self.epoch - 1  # pushing metrics will increase the epoch in Flashy, so we do -1 here

        # save minimal state_dict as new checkpoint every X epoch
        if self.cfg.checkpoint.save_every:
            if epoch % self.cfg.checkpoint.save_every == 0:
                minimal_state = state
                if self.cfg.checkpoint.keep_every_states is not None and len(self.cfg.checkpoint.keep_every_states) &gt; 0:
                    minimal_state = {
                        name: source for name, source in state.items()
                        if name in self.cfg.checkpoint.keep_every_states
                    }
                epoch_checkpoint_path = self.epoch_checkpoint_path(epoch)
                checkpoint.save_checkpoint(minimal_state, epoch_checkpoint_path, is_sharded)

        # save checkpoint as latest checkpoint
        if self.cfg.checkpoint.save_last:
            last_checkpoint_path = self.checkpoint_path()
            checkpoint.save_checkpoint(state, last_checkpoint_path, is_sharded)

        # flush any stale checkpoint to reduce disk footprint
        checkpoint.flush_stale_checkpoints(self.checkpoint_path())

    def load_from_pretrained(self, name: str) -&gt; dict:
        raise NotImplementedError(&#34;Solver does not provide a way to load pretrained models.&#34;)

    def load_checkpoints(self, load_best: bool = False, ignore_state_keys: tp.List[str] = []) -&gt; tp.Optional[dict]:
        &#34;&#34;&#34;Load last checkpoint or the one specified in continue_from.

        Args:
            load_best (bool): Whether to load from best state dict or not.
                Best state dict is always used when not loading the current xp.
            ignore_state_keys (list of str): List of sources to ignore when loading the state, e.g. `optimizer`.
        Returns:
            state (dict, optional): The loaded state dictionary.
        &#34;&#34;&#34;
        # load checkpoints from xp folder or cfg.continue_from
        is_sharded = self.cfg.fsdp.use
        load_from_path: tp.Optional[Path] = None
        checkpoint_source: tp.Optional[checkpoint.CheckpointSource] = None

        if load_best:
            self.logger.info(&#34;Trying to load state_dict from best state.&#34;)

        state: tp.Optional[dict] = None
        rank0_checkpoint_path = self.checkpoint_path(use_fsdp=False)
        current_checkpoint_path = self.checkpoint_path()
        _pretrained_prefix = &#39;//pretrained/&#39;
        continue_pretrained = (self.cfg.continue_from or &#39;&#39;).startswith(_pretrained_prefix)
        if rank0_checkpoint_path.exists():
            self.logger.info(f&#34;Loading existing checkpoint: {current_checkpoint_path}&#34;)
            load_from_path = current_checkpoint_path
            checkpoint.check_sharded_checkpoint(current_checkpoint_path, rank0_checkpoint_path)
            checkpoint_source = checkpoint.CheckpointSource.CURRENT_XP
        elif self.cfg.continue_from and not continue_pretrained:
            self.logger.info(f&#34;Continuing from provided checkpoint: {self.cfg.continue_from}&#34;)
            # we&#39;re always continuing from consolidated checkpoints: self.cfg.use_fsdp and not continue_best
            load_from_path = checkpoint.resolve_checkpoint_path(self.cfg.continue_from, use_fsdp=False)
            if load_from_path is None:
                self.logger.error(&#39;Could not resolve the continue_from checkpoint %s&#39;, self.cfg.continue_from)
                raise RuntimeError(f&#39;Could not resolve continue_from checkpoint {self.cfg.continue_from}&#39;)
            checkpoint_source = checkpoint.CheckpointSource.OTHER

        if load_from_path is not None:
            state = checkpoint.load_checkpoint(load_from_path, is_sharded)
        elif continue_pretrained:
            self.logger.info(&#34;Loading a pretrained model. Ignoring &#39;load_best&#39; and &#39;ignore_state_keys&#39; params.&#34;)
            state = self.load_from_pretrained(self.cfg.continue_from[len(_pretrained_prefix):])
            checkpoint_source = checkpoint.CheckpointSource.PRETRAINED
            load_best = True

        # checkpoints are not from the current xp, we only retrieve the best state
        if checkpoint_source is not None and checkpoint_source != checkpoint.CheckpointSource.CURRENT_XP:
            assert state is not None
            self.logger.info(&#34;Checkpoint source is not the current xp: Load state_dict from best state.&#34;)
            load_best = True
            state = {key: state[key] for key in self._continue_best_source_keys if key in state}
            # loaded checkpoints are FSDP checkpoints: we&#39;re reading the best state
            # from FSDP and we drop the regular best_state
            if &#39;fsdp_best_state&#39; in state and state[&#39;fsdp_best_state&#39;]:
                state.pop(&#39;best_state&#39;, None)
                self.logger.info(&#34;... Loaded checkpoint has FSDP best state&#34;)
            # FSDP is enabled in the solver, if the loaded checkpoints do not have FSDP support
            # then we&#39;re initializing FSDP best state with the regular best state
            elif self.cfg.fsdp.use:
                if &#39;fsdp_best_state&#39; not in state or not state[&#39;fsdp_best_state&#39;]:
                    # we swap non-FSDP checkpoints best_state to FSDP-compatible best state
                    state[&#39;fsdp_best_state&#39;] = state.pop(&#39;best_state&#39;)
                    self.logger.info(&#34;... Loaded checkpoint does not have FSDP best state. Use regular best state&#34;)

        if state is not None:
            if load_best:
                self.logger.info(&#34;Ignoring keys when loading best %r&#34;, ignore_state_keys)
                for key in set(ignore_state_keys):
                    if key in state:
                        state.pop(key)
                has_best_state = &#39;best_state&#39; in state or &#39;fsdp_best_state&#39; in state
                assert has_best_state, (&#34;Trying to load best state but neither &#39;best_state&#39;&#34;,
                                        &#34; or &#39;fsdp_best_state&#39; found in checkpoints.&#34;)
            self.load_state_dict(state)

        # for FSDP, let&#39;s make extra sure nothing bad happened with out of sync
        # checkpoints across workers.
        epoch = float(self.epoch)
        avg_epoch = flashy.distrib.average_metrics({&#39;epoch&#39;: epoch})[&#39;epoch&#39;]
        if avg_epoch != epoch:
            raise RuntimeError(
                f&#34;Inconsistent loading of checkpoints happened, our epoch is {epoch} &#34;
                f&#34;but average of epochs is {avg_epoch}, at least one gpu must have a &#34;
                &#34;different epoch number.&#34;)

        # on load_best, properly reinitialize state_dict, best states and ema
        # otherwise we load from the current xp and don&#39;t alter anything
        if load_best:
            self.logger.info(&#34;Loading state_dict from best state.&#34;)
            if not self.cfg.fsdp.use and self.fsdp_best_state:
                # loading from an FSDP checkpoint but with FSDP deactivated
                self.logger.info(&#34;... Loading from FSDP best state dict.&#34;)
                self.best_state.load_state_dict(self.fsdp_best_state)

            # if load_best, we permanently override the regular state_dict with the best state
            if self.cfg.fsdp.use:
                self.logger.info(&#34;FSDP is used, loading from FSDP best state.&#34;)
                with fsdp.switch_to_full_state_dict(self._fsdp_modules):
                    # this might be really fragile but okay for now.
                    self.load_state_dict(self.fsdp_best_state)
            else:
                # we permanently swap the stateful objects to their best state
                self._load_new_state_dict(self.best_state.state_dict())

            # the EMA modules should also be instantiated with best state.
            # the easiest way to do so is to reinitialize a new EMA with best state loaded.
            if self.ema is not None:
                self.logger.info(&#34;Re-initializing EMA from best state&#34;)
                self.initialize_ema()

            if self.cfg.fsdp.use:
                self.logger.info(&#34;Re-initializing best state after using FSDP best state.&#34;)
                for name in self.best_state.states.keys():
                    state_source = self._get_state_source(name)
                    self.best_state.update(name, state_source)

        return state

    def restore(self, load_best: bool = False, replay_metrics: bool = False,
                ignore_state_keys: tp.List[str] = []) -&gt; bool:
        &#34;&#34;&#34;Restore the status of a solver for a given xp.

        Args:
            load_best (bool): if `True`, load the best state from the checkpoint.
            replay_metrics (bool): if `True`, logs all the metrics from past epochs.
            ignore_state_keys (list of str): list of sources to ignore when loading the state, e.g. `optimizer`.
        &#34;&#34;&#34;
        self.logger.info(&#34;Restoring weights and history.&#34;)
        restored_checkpoints = self.load_checkpoints(load_best, ignore_state_keys)

        self.logger.info(&#34;Model hash: %s&#34;, model_hash(self.model))

        if replay_metrics and len(self.history) &gt; 0:
            self.logger.info(&#34;Replaying past metrics...&#34;)
            for epoch, stages in enumerate(self.history):
                for stage_name, metrics in stages.items():
                    # We manually log the metrics summary to the result logger
                    # as we don&#39;t want to add them to the pending metrics
                    self.result_logger._log_summary(stage_name, metrics, step=epoch + 1, step_name=&#39;epoch&#39;,
                                                    formatter=self.get_formatter(stage_name))
        return restored_checkpoints is not None

    def commit(self, save_checkpoints: bool = True):
        &#34;&#34;&#34;Commit metrics to dora and save checkpoints at the end of an epoch.&#34;&#34;&#34;
        # we override commit to introduce more complex checkpoint saving behaviors
        self.history.append(self._pending_metrics)  # This will increase self.epoch
        if save_checkpoints:
            self.save_checkpoints()
        self._start_epoch()
        if flashy.distrib.is_rank_zero():
            self.xp.link.update_history(self.history)

    def run_epoch(self):
        &#34;&#34;&#34;Run a single epoch with all stages.

        Metrics for a given stage are stored in _pending_metrics and committed by the solver afterwards.
        Children solvers can extend this method with custom behavior, e.g.:

            def run_epoch(self):
                ... # custom code
                super().run_epoch()
                ... # custom code
        &#34;&#34;&#34;
        self.run_stage(&#39;train&#39;, self.train)
        with torch.no_grad():
            with self.swap_ema_state():
                self.run_stage(&#39;valid&#39;, self.valid)
                # the best state is updated with EMA states if available
                self.update_best_state_from_stage(&#39;valid&#39;)
            with self.swap_best_state():
                if self.should_run_stage(&#39;evaluate&#39;):
                    self.run_stage(&#39;evaluate&#39;, self.evaluate)
                if self.should_run_stage(&#39;generate&#39;):
                    self.run_stage(&#39;generate&#39;, with_rank_rng()(self.generate))

    def run(self):
        &#34;&#34;&#34;Training loop.&#34;&#34;&#34;
        assert len(self.state_dict()) &gt; 0
        self.restore(replay_metrics=True)  # load checkpoint and replay history
        self.log_hyperparams(dict_from_config(self.cfg))
        for epoch in range(self.epoch, self.cfg.optim.epochs + 1):
            if self.should_stop_training():
                return
            self.run_epoch()
            # Commit will send the metrics to Dora and save checkpoints by default.
            self.commit()

    def should_stop_training(self) -&gt; bool:
        &#34;&#34;&#34;Check whether we should stop training or not.&#34;&#34;&#34;
        return self.epoch &gt; self.cfg.optim.epochs

    def should_run_stage(self, stage_name) -&gt; bool:
        &#34;&#34;&#34;Check whether we want to run the specified stages.&#34;&#34;&#34;
        stage_every = self.cfg[stage_name].get(&#39;every&#39;, None)
        is_last_epoch = self.epoch == self.cfg.optim.epochs
        is_epoch_every = (stage_every and self.epoch % stage_every == 0)
        return is_last_epoch or is_epoch_every

    @abstractmethod
    def run_step(self, idx: int, batch: tp.Any, metrics: dict):
        &#34;&#34;&#34;Perform one training or valid step on a given batch.&#34;&#34;&#34;
        ...

    def common_train_valid(self, dataset_split: str, **kwargs: tp.Any):
        &#34;&#34;&#34;Common logic for train and valid stages.&#34;&#34;&#34;
        self.model.train(self.is_training)

        loader = self.dataloaders[dataset_split]
        # get a different order for distributed training, otherwise this will get ignored
        if flashy.distrib.world_size() &gt; 1 \
           and isinstance(loader.sampler, torch.utils.data.distributed.DistributedSampler):
            loader.sampler.set_epoch(self.epoch)
        updates_per_epoch = self.train_updates_per_epoch if self.is_training else len(loader)
        if self.cfg.benchmark_no_load:
            self.logger.warning(&#34;Fake loading for benchmarking: re-using first batch&#34;)
            batch = next(iter(loader))
            loader = [batch] * updates_per_epoch  # type: ignore
        lp = self.log_progress(self.current_stage, loader, total=updates_per_epoch, updates=self.log_updates)
        average = flashy.averager()  # epoch wise average
        instant_average = flashy.averager()  # average between two logging
        metrics: dict = {}

        with self.profiler, self.deadlock_detect:  # profiler will only run for the first 20 updates.
            for idx, batch in enumerate(lp):
                self.deadlock_detect.update(&#39;batch&#39;)
                if idx &gt;= updates_per_epoch:
                    break
                metrics = {}
                metrics = self.run_step(idx, batch, metrics)
                self.deadlock_detect.update(&#39;step&#39;)
                # run EMA step
                if self.ema is not None and self.is_training and (idx + 1) % self.cfg.optim.ema.updates == 0:
                    self.logger.debug(&#34;EMA model step&#34;)
                    self.ema.step()
                self.deadlock_detect.update(&#39;ema&#39;)
                self.profiler.step()
                instant_metrics = instant_average(metrics)
                if lp.update(**instant_metrics):
                    instant_average = flashy.averager()  # reset averager between two logging
                metrics = average(metrics)  # epoch wise average
                self.deadlock_detect.update(&#39;end_batch&#39;)

        metrics = flashy.distrib.average_metrics(metrics, updates_per_epoch)
        return metrics

    def train(self):
        &#34;&#34;&#34;Train stage.&#34;&#34;&#34;
        return self.common_train_valid(&#39;train&#39;)

    def valid(self):
        &#34;&#34;&#34;Valid stage.&#34;&#34;&#34;
        return self.common_train_valid(&#39;valid&#39;)

    @abstractmethod
    def evaluate(self):
        &#34;&#34;&#34;Evaluate stage.&#34;&#34;&#34;
        ...

    @abstractmethod
    def generate(self):
        &#34;&#34;&#34;Generate stage.&#34;&#34;&#34;
        ...

    def run_one_stage(self, stage_name: str):
        &#34;&#34;&#34;Run only the specified stage.
        This method is useful to only generate samples from a trained experiment
        or rerun the validation or evaluation stages.
        &#34;&#34;&#34;
        fn = {
            &#39;generate&#39;: with_rank_rng()(self.generate),
            &#39;evaluate&#39;: self.evaluate,
            &#39;valid&#39;: self.valid,
        }
        if stage_name not in fn:
            raise ValueError(f&#39;Trying to run stage {stage_name} is not supported.&#39;)
        assert len(self.state_dict()) &gt; 0
        self._start_epoch()
        with torch.no_grad(), self.swap_best_state():
            self.run_stage(stage_name, fn[stage_name])
        if not self.cfg.execute_inplace:
            self.commit(save_checkpoints=False)

    @staticmethod
    def get_eval_solver_from_sig(sig: str, dtype: tp.Optional[str] = None,
                                 device: tp.Optional[str] = None, autocast: bool = True,
                                 batch_size: tp.Optional[int] = None,
                                 override_cfg: tp.Optional[tp.Union[dict, omegaconf.DictConfig]] = None,
                                 **kwargs):
        &#34;&#34;&#34;Mostly a convenience function around audiocraft.train.get_solver_from_sig,
        populating all the proper param, deactivating EMA, FSDP, loading the best state,
        basically all you need to get a solver ready to &#34;play&#34; with in single GPU mode
        and with minimal memory overhead.

        Args:
            sig (str): signature to load.
            dtype (str or None): potential dtype, as a string, i.e. &#39;float16&#39;.
            device (str or None): potential device, as a string, i.e. &#39;cuda&#39;.
            override_cfg (dict or omegaconf.DictConfig or None): potential device, as a string, i.e. &#39;cuda&#39;.
        &#34;&#34;&#34;
        from audiocraft import train
        our_override_cfg: tp.Dict[str, tp.Any] = {&#39;optim&#39;: {&#39;ema&#39;: {&#39;use&#39;: False}}}
        our_override_cfg[&#39;autocast&#39;] = autocast
        if dtype is not None:
            our_override_cfg[&#39;dtype&#39;] = dtype
        if device is not None:
            our_override_cfg[&#39;device&#39;] = device
        if batch_size is not None:
            our_override_cfg[&#39;dataset&#39;] = {&#39;batch_size&#39;: batch_size}
        if override_cfg is None:
            override_cfg = {}
        override_cfg = omegaconf.OmegaConf.merge(
            omegaconf.DictConfig(override_cfg), omegaconf.DictConfig(our_override_cfg))  # type: ignore
        solver = train.get_solver_from_sig(
            sig, override_cfg=override_cfg,
            load_best=True, disable_fsdp=True,
            ignore_state_keys=[&#39;optimizer&#39;, &#39;ema&#39;], **kwargs)
        solver.model.eval()
        return solver</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="audiocraft.solvers.base.StandardSolver"><code class="flex name class">
<span>class <span class="ident">StandardSolver</span></span>
<span>(</span><span>cfg: omegaconf.dictconfig.DictConfig)</span>
</code></dt>
<dd>
<div class="desc"><p>Standard solver for AudioCraft.</p>
<p>The standard solver implements a base training loop with the following stages:
train, valid, evaluate and generate that are expected to be all defined for
solvers in AudioCraft. It also provides a nice default management of Dora history replay,
checkpoint management across epoch, and logging configuration.</p>
<p>AudioCraft solvers must inherit from the StandardSolver and define the methods
associated to each stage as well as the show, build_model and build_dataloaders methods.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StandardSolver(ABC, flashy.BaseSolver):
    &#34;&#34;&#34;Standard solver for AudioCraft.

    The standard solver implements a base training loop with the following stages:
    train, valid, evaluate and generate that are expected to be all defined for
    solvers in AudioCraft. It also provides a nice default management of Dora history replay,
    checkpoint management across epoch, and logging configuration.

    AudioCraft solvers must inherit from the StandardSolver and define the methods
    associated to each stage as well as the show, build_model and build_dataloaders methods.
    &#34;&#34;&#34;
    def __init__(self, cfg: omegaconf.DictConfig):
        super().__init__()
        self.logger.info(f&#34;Instantiating solver {self.__class__.__name__} for XP {self.xp.sig}&#34;)
        self.logger.info(f&#34;All XP logs are stored in {self.xp.folder}&#34;)
        self.cfg = cfg
        self.device = cfg.device
        self.model: nn.Module
        self._continue_best_source_keys = [&#39;best_state&#39;, &#39;fsdp_best_state&#39;]
        self._fsdp_modules: tp.List[fsdp.FSDP] = []
        self._ema_sources: nn.ModuleDict = nn.ModuleDict()
        self.ema: tp.Optional[optim.ModuleDictEMA] = None
        self.dataloaders: tp.Dict[str, torch.utils.data.DataLoader] = dict()
        self._log_updates = self.cfg.logging.get(&#39;log_updates&#39;, 10)
        if self.cfg.logging.log_tensorboard:
            self.init_tensorboard(**self.cfg.get(&#39;tensorboard&#39;))
        if self.cfg.logging.log_wandb and self:
            self.init_wandb(**self.cfg.get(&#39;wandb&#39;))
        # keep a copy of the best performing state for stateful objects
        # used for evaluation and generation stages
        dtype_best: tp.Optional[torch.dtype] = None
        if self.cfg.fsdp.use:
            dtype_best = getattr(torch, self.cfg.fsdp.param_dtype)  # type: ignore
            assert isinstance(dtype_best, torch.dtype)
        elif self.cfg.autocast:
            dtype_best = getattr(torch, self.cfg.autocast_dtype)  # type: ignore
            assert isinstance(dtype_best, torch.dtype)
        self.best_state: BestStateDictManager = BestStateDictManager(dtype=dtype_best)
        # Hacky support for keeping a copy of the full best state in rank0.
        self.fsdp_best_state: tp.Dict[str, tp.Any] = {}
        self.register_stateful(&#39;best_state&#39;, &#39;fsdp_best_state&#39;)  # register best_state object to keep it in state_dict
        self._new_best_state: bool = False  # should save a new checkpoint
        # instantiate datasets and appropriate number of updates per epoch
        self.build_dataloaders()
        if self.cfg.execute_only is None:
            assert &#39;train&#39; in self.dataloaders, &#34;The train dataset split must be provided.&#34;
            assert &#39;valid&#39; in self.dataloaders, &#34;The valid dataset split must be provided.&#34;
        self.train_updates_per_epoch = len(self.dataloaders[&#39;train&#39;]) if &#39;train&#39; in self.dataloaders else 0
        if self.cfg.optim.updates_per_epoch:
            self.train_updates_per_epoch = self.cfg.optim.updates_per_epoch
        self.total_updates = self.train_updates_per_epoch * self.cfg.optim.epochs
        # instantiate model &amp; exponential moving average on the model
        self.build_model()
        self.logger.info(&#34;Model hash: %s&#34;, model_hash(self.model))
        assert &#39;model&#39; in self.stateful.sources, \
            &#34;Please register the model to stateful with self.register_stateful(&#39;model&#39;) in build_model.&#34;
        self.profiler = Profiler(self.model, **self.cfg.profiler)
        self.initialize_ema()
        self.register_stateful(&#39;ema&#39;)
        assert self.ema is None or &#39;ema&#39; in self.stateful.sources, \
            &#34;Please register the ema to stateful with self.register_stateful(&#39;ema&#39;) in build_model.&#34;
        self.deadlock_detect = DeadlockDetect(**self.cfg.deadlock)
        # basic statistics on the trained model
        model_size = sum(p.numel() for p in self.model.parameters() if p.requires_grad) / 1e6
        # one copy of grad, one copy of momentum, one copy of denominator and model weights.
        # and 4 bytes for each float!
        mem_usage = model_size * 4 * 4 / 1000
        self.logger.info(&#34;Model size: %.2f M params&#34;, model_size)
        self.logger.info(&#34;Base memory usage, with model, grad and optim: %.2f GB&#34;, mem_usage)

    @property
    def autocast(self):
        &#34;&#34;&#34;Convenient autocast (or not) using the solver configuration.&#34;&#34;&#34;
        return TorchAutocast(enabled=self.cfg.autocast, device_type=self.device, dtype=self.autocast_dtype)

    def _get_state_source(self, name) -&gt; flashy.state.StateDictSource:
        # Internal utility to get a state source from the solver
        return self.stateful.sources[name]

    @property
    def best_metric_name(self) -&gt; tp.Optional[str]:
        &#34;&#34;&#34;Metric name used to identify the best state. This metric should be stored in the metrics
        used on the stage for best state identification (most likely, `valid`). If None, then
        no best state is saved.
        &#34;&#34;&#34;
        return None

    def register_best_state(self, *args: str):
        &#34;&#34;&#34;Register state sources in `BestStateDictManager` to keep their best states along with their
        latest states. The best state will be used at evaluation stages instead of the latest states.

        Shortcut around `BestStateDictManager.register` method. You can pass any number of
        attribute, included nested attributes and those will be included into the checkpoints
        and automatically restored when `BaseSolver.restore` is called.
        &#34;&#34;&#34;
        for name in args:
            state_source = self._get_state_source(name)
            assert name in self.stateful.sources, &#34;Registered states in best should be registered in stateful first!&#34;
            self.best_state.register(name, state_source)

    def register_ema(self, *args: str):
        &#34;&#34;&#34;Register state sources for exponential moving average.

        The registered sources are used to instantiate a ModuleDictEMA instance.
        The ModuleDictEMA keeps a `nn.ModuleDict` module that is updated when self.ema.step() is called
        and swapped with the original state sources with self.swap_ema_state() method.

        Usage:
            self.register_ema(&#39;model&#39;)
        &#34;&#34;&#34;
        assert self.ema is None, &#34;Cannot register state source to already instantiated EMA.&#34;
        for name in args:
            self._ema_sources[name] = getattr(self, name)

    def wrap_with_fsdp(self, model: torch.nn.Module, *args, **kwargs):
        model = fsdp.wrap_with_fsdp(self.cfg.fsdp, model, *args, **kwargs)
        if isinstance(model, fsdp.FSDP):
            self._fsdp_modules.append(model)
        return model

    def update_best_state_from_stage(self, stage_name: str = &#39;valid&#39;):
        &#34;&#34;&#34;Update latest best state based on pending metrics of a given stage. This method relies
        on the `BestStateDictManager.update` method to update the best state_dict with latest weights
        if the registered states happen to match to the best performing setup.
        &#34;&#34;&#34;
        if self.best_metric_name is None:
            # when no best metric is defined, the last state is always the best
            self._new_best_state = True
            self.logger.info(&#34;Updating best state with current state.&#34;)
        else:
            assert stage_name in self._pending_metrics, f&#34;Metrics for stage {stage_name} not found.&#34;
            assert self.best_metric_name in self._pending_metrics[stage_name], \
                f&#34;Best metric not found in {stage_name} metrics. Cannot register best state&#34;
            current_score = self._pending_metrics[stage_name][self.best_metric_name]
            all_best_metric_scores = [
                past_metrics[stage_name][self.best_metric_name]
                for past_metrics in self.history
            ]
            all_best_metric_scores.append(current_score)
            best_score = min(all_best_metric_scores)
            self._new_best_state = current_score == best_score
            if self._new_best_state:
                old_best = min(all_best_metric_scores[:-1] + [float(&#39;inf&#39;)])
                self.logger.info(
                    f&#34;New best state with {self.best_metric_name}={current_score:.3f} (was {old_best:.3f})&#34;)

        if self._new_best_state:
            if self.cfg.fsdp.use:
                # this will give an empty state dict on all ranks but the rank 0
                # which will have a copy in memory of the full model.
                with fsdp.switch_to_full_state_dict(self._fsdp_modules):
                    for name in self.best_state.states.keys():
                        state_source = self._get_state_source(name)
                        self.best_state.update(name, state_source)
                    # we save to a different dict.
                    self.fsdp_best_state.update(self.best_state.state_dict())
                # We cannot efficiently load fsdp_best_state when using FSDP,
                # so we have do do a second pass, with the local shards.
            for name in self.best_state.states.keys():
                state_source = self._get_state_source(name)
                self.best_state.update(name, state_source)

    def _load_new_state_dict(self, state_dict: dict) -&gt; dict:
        old_states = {}
        for name, new_state in state_dict.items():
            state_source = self._get_state_source(name)
            old_states[name] = copy_state(state_source.state_dict())
            state_source.load_state_dict(new_state)
        return old_states

    @contextmanager
    def swap_best_state(self):
        self.logger.debug(f&#34;Swapping to best state for: {&#39;, &#39;.join(self.best_state.state_dict().keys())}&#34;)
        old_states = self._load_new_state_dict(self.best_state.state_dict())
        try:
            yield
        finally:
            self.logger.debug(&#34;Swapping back from best to original state&#34;)
            for name, old_state in old_states.items():
                state_source = self._get_state_source(name)
                state_source.load_state_dict(old_state)

    @contextmanager
    def swap_ema_state(self):
        if self.ema is None:
            yield
        else:
            ema_state_dict = self.ema.state_dict()[&#39;state&#39;]
            self.logger.debug(f&#34;Swapping to EMA state for: {&#39;, &#39;.join(ema_state_dict.keys())}&#34;)
            old_states = self._load_new_state_dict(ema_state_dict)
            try:
                yield
            finally:
                self.logger.debug(&#34;Swapping back from EMA state to original state&#34;)
                for name, old_state in old_states.items():
                    state_source = self._get_state_source(name)
                    state_source.load_state_dict(old_state)

    @property
    def is_training(self):
        return self.current_stage == &#39;train&#39;

    def log_model_summary(self, model: nn.Module):
        &#34;&#34;&#34;Log model summary, architecture and size of the model.&#34;&#34;&#34;
        self.logger.info(model)
        mb = sum(p.numel() for p in model.parameters()) * 4 / 2 ** 20
        self.logger.info(&#34;Size: %.1f MB&#34;, mb)

    @abstractmethod
    def build_model(self):
        &#34;&#34;&#34;Method to implement to initialize model.&#34;&#34;&#34;
        ...

    def initialize_ema(self):
        &#34;&#34;&#34;Initialize exponential moving average with the registered sources.
        EMA object is created if the optim.ema.model.decay value is non-null.
        &#34;&#34;&#34;
        from .builders import get_ema
        self.ema = get_ema(self._ema_sources, self.cfg.optim.ema)
        if self.ema is None:
            self.logger.info(&#39;No EMA on the model.&#39;)
        else:
            assert self.cfg.optim.ema.updates &gt; 0
            self.logger.info(
                f&#39;Initializing EMA on the model with decay = {self.ema.decay}&#39;
                f&#39; every {self.cfg.optim.ema.updates} updates&#39;
            )

    @abstractmethod
    def build_dataloaders(self):
        &#34;&#34;&#34;Method to implement to initialize dataloaders.&#34;&#34;&#34;
        ...

    @abstractmethod
    def show(self):
        &#34;&#34;&#34;Method to log any information without running the job.&#34;&#34;&#34;
        ...

    @property
    def log_updates(self):
        # convenient access to log updates
        return self._log_updates

    def checkpoint_path(self, **kwargs):
        kwargs.setdefault(&#39;use_fsdp&#39;, self.cfg.fsdp.use)
        return self.folder / checkpoint.checkpoint_name(**kwargs)

    def epoch_checkpoint_path(self, epoch: int, **kwargs):
        kwargs.setdefault(&#39;use_fsdp&#39;, self.cfg.fsdp.use)
        return self.folder / checkpoint.checkpoint_name(str(epoch), **kwargs)

    def checkpoint_path_with_name(self, name: str, **kwargs):
        kwargs.setdefault(&#39;use_fsdp&#39;, self.cfg.fsdp.use)
        return self.folder / checkpoint.checkpoint_name(name=name, **kwargs)

    def save_checkpoints(self):
        &#34;&#34;&#34;Save checkpoint, optionally keeping a copy for a given epoch.&#34;&#34;&#34;
        is_sharded = self.cfg.fsdp.use
        if not flashy.distrib.is_rank_zero() and not is_sharded:
            return
        self.logger.info(&#34;Model hash: %s&#34;, model_hash(self.model))
        state = self.state_dict()
        epoch = self.epoch - 1  # pushing metrics will increase the epoch in Flashy, so we do -1 here

        # save minimal state_dict as new checkpoint every X epoch
        if self.cfg.checkpoint.save_every:
            if epoch % self.cfg.checkpoint.save_every == 0:
                minimal_state = state
                if self.cfg.checkpoint.keep_every_states is not None and len(self.cfg.checkpoint.keep_every_states) &gt; 0:
                    minimal_state = {
                        name: source for name, source in state.items()
                        if name in self.cfg.checkpoint.keep_every_states
                    }
                epoch_checkpoint_path = self.epoch_checkpoint_path(epoch)
                checkpoint.save_checkpoint(minimal_state, epoch_checkpoint_path, is_sharded)

        # save checkpoint as latest checkpoint
        if self.cfg.checkpoint.save_last:
            last_checkpoint_path = self.checkpoint_path()
            checkpoint.save_checkpoint(state, last_checkpoint_path, is_sharded)

        # flush any stale checkpoint to reduce disk footprint
        checkpoint.flush_stale_checkpoints(self.checkpoint_path())

    def load_from_pretrained(self, name: str) -&gt; dict:
        raise NotImplementedError(&#34;Solver does not provide a way to load pretrained models.&#34;)

    def load_checkpoints(self, load_best: bool = False, ignore_state_keys: tp.List[str] = []) -&gt; tp.Optional[dict]:
        &#34;&#34;&#34;Load last checkpoint or the one specified in continue_from.

        Args:
            load_best (bool): Whether to load from best state dict or not.
                Best state dict is always used when not loading the current xp.
            ignore_state_keys (list of str): List of sources to ignore when loading the state, e.g. `optimizer`.
        Returns:
            state (dict, optional): The loaded state dictionary.
        &#34;&#34;&#34;
        # load checkpoints from xp folder or cfg.continue_from
        is_sharded = self.cfg.fsdp.use
        load_from_path: tp.Optional[Path] = None
        checkpoint_source: tp.Optional[checkpoint.CheckpointSource] = None

        if load_best:
            self.logger.info(&#34;Trying to load state_dict from best state.&#34;)

        state: tp.Optional[dict] = None
        rank0_checkpoint_path = self.checkpoint_path(use_fsdp=False)
        current_checkpoint_path = self.checkpoint_path()
        _pretrained_prefix = &#39;//pretrained/&#39;
        continue_pretrained = (self.cfg.continue_from or &#39;&#39;).startswith(_pretrained_prefix)
        if rank0_checkpoint_path.exists():
            self.logger.info(f&#34;Loading existing checkpoint: {current_checkpoint_path}&#34;)
            load_from_path = current_checkpoint_path
            checkpoint.check_sharded_checkpoint(current_checkpoint_path, rank0_checkpoint_path)
            checkpoint_source = checkpoint.CheckpointSource.CURRENT_XP
        elif self.cfg.continue_from and not continue_pretrained:
            self.logger.info(f&#34;Continuing from provided checkpoint: {self.cfg.continue_from}&#34;)
            # we&#39;re always continuing from consolidated checkpoints: self.cfg.use_fsdp and not continue_best
            load_from_path = checkpoint.resolve_checkpoint_path(self.cfg.continue_from, use_fsdp=False)
            if load_from_path is None:
                self.logger.error(&#39;Could not resolve the continue_from checkpoint %s&#39;, self.cfg.continue_from)
                raise RuntimeError(f&#39;Could not resolve continue_from checkpoint {self.cfg.continue_from}&#39;)
            checkpoint_source = checkpoint.CheckpointSource.OTHER

        if load_from_path is not None:
            state = checkpoint.load_checkpoint(load_from_path, is_sharded)
        elif continue_pretrained:
            self.logger.info(&#34;Loading a pretrained model. Ignoring &#39;load_best&#39; and &#39;ignore_state_keys&#39; params.&#34;)
            state = self.load_from_pretrained(self.cfg.continue_from[len(_pretrained_prefix):])
            checkpoint_source = checkpoint.CheckpointSource.PRETRAINED
            load_best = True

        # checkpoints are not from the current xp, we only retrieve the best state
        if checkpoint_source is not None and checkpoint_source != checkpoint.CheckpointSource.CURRENT_XP:
            assert state is not None
            self.logger.info(&#34;Checkpoint source is not the current xp: Load state_dict from best state.&#34;)
            load_best = True
            state = {key: state[key] for key in self._continue_best_source_keys if key in state}
            # loaded checkpoints are FSDP checkpoints: we&#39;re reading the best state
            # from FSDP and we drop the regular best_state
            if &#39;fsdp_best_state&#39; in state and state[&#39;fsdp_best_state&#39;]:
                state.pop(&#39;best_state&#39;, None)
                self.logger.info(&#34;... Loaded checkpoint has FSDP best state&#34;)
            # FSDP is enabled in the solver, if the loaded checkpoints do not have FSDP support
            # then we&#39;re initializing FSDP best state with the regular best state
            elif self.cfg.fsdp.use:
                if &#39;fsdp_best_state&#39; not in state or not state[&#39;fsdp_best_state&#39;]:
                    # we swap non-FSDP checkpoints best_state to FSDP-compatible best state
                    state[&#39;fsdp_best_state&#39;] = state.pop(&#39;best_state&#39;)
                    self.logger.info(&#34;... Loaded checkpoint does not have FSDP best state. Use regular best state&#34;)

        if state is not None:
            if load_best:
                self.logger.info(&#34;Ignoring keys when loading best %r&#34;, ignore_state_keys)
                for key in set(ignore_state_keys):
                    if key in state:
                        state.pop(key)
                has_best_state = &#39;best_state&#39; in state or &#39;fsdp_best_state&#39; in state
                assert has_best_state, (&#34;Trying to load best state but neither &#39;best_state&#39;&#34;,
                                        &#34; or &#39;fsdp_best_state&#39; found in checkpoints.&#34;)
            self.load_state_dict(state)

        # for FSDP, let&#39;s make extra sure nothing bad happened with out of sync
        # checkpoints across workers.
        epoch = float(self.epoch)
        avg_epoch = flashy.distrib.average_metrics({&#39;epoch&#39;: epoch})[&#39;epoch&#39;]
        if avg_epoch != epoch:
            raise RuntimeError(
                f&#34;Inconsistent loading of checkpoints happened, our epoch is {epoch} &#34;
                f&#34;but average of epochs is {avg_epoch}, at least one gpu must have a &#34;
                &#34;different epoch number.&#34;)

        # on load_best, properly reinitialize state_dict, best states and ema
        # otherwise we load from the current xp and don&#39;t alter anything
        if load_best:
            self.logger.info(&#34;Loading state_dict from best state.&#34;)
            if not self.cfg.fsdp.use and self.fsdp_best_state:
                # loading from an FSDP checkpoint but with FSDP deactivated
                self.logger.info(&#34;... Loading from FSDP best state dict.&#34;)
                self.best_state.load_state_dict(self.fsdp_best_state)

            # if load_best, we permanently override the regular state_dict with the best state
            if self.cfg.fsdp.use:
                self.logger.info(&#34;FSDP is used, loading from FSDP best state.&#34;)
                with fsdp.switch_to_full_state_dict(self._fsdp_modules):
                    # this might be really fragile but okay for now.
                    self.load_state_dict(self.fsdp_best_state)
            else:
                # we permanently swap the stateful objects to their best state
                self._load_new_state_dict(self.best_state.state_dict())

            # the EMA modules should also be instantiated with best state.
            # the easiest way to do so is to reinitialize a new EMA with best state loaded.
            if self.ema is not None:
                self.logger.info(&#34;Re-initializing EMA from best state&#34;)
                self.initialize_ema()

            if self.cfg.fsdp.use:
                self.logger.info(&#34;Re-initializing best state after using FSDP best state.&#34;)
                for name in self.best_state.states.keys():
                    state_source = self._get_state_source(name)
                    self.best_state.update(name, state_source)

        return state

    def restore(self, load_best: bool = False, replay_metrics: bool = False,
                ignore_state_keys: tp.List[str] = []) -&gt; bool:
        &#34;&#34;&#34;Restore the status of a solver for a given xp.

        Args:
            load_best (bool): if `True`, load the best state from the checkpoint.
            replay_metrics (bool): if `True`, logs all the metrics from past epochs.
            ignore_state_keys (list of str): list of sources to ignore when loading the state, e.g. `optimizer`.
        &#34;&#34;&#34;
        self.logger.info(&#34;Restoring weights and history.&#34;)
        restored_checkpoints = self.load_checkpoints(load_best, ignore_state_keys)

        self.logger.info(&#34;Model hash: %s&#34;, model_hash(self.model))

        if replay_metrics and len(self.history) &gt; 0:
            self.logger.info(&#34;Replaying past metrics...&#34;)
            for epoch, stages in enumerate(self.history):
                for stage_name, metrics in stages.items():
                    # We manually log the metrics summary to the result logger
                    # as we don&#39;t want to add them to the pending metrics
                    self.result_logger._log_summary(stage_name, metrics, step=epoch + 1, step_name=&#39;epoch&#39;,
                                                    formatter=self.get_formatter(stage_name))
        return restored_checkpoints is not None

    def commit(self, save_checkpoints: bool = True):
        &#34;&#34;&#34;Commit metrics to dora and save checkpoints at the end of an epoch.&#34;&#34;&#34;
        # we override commit to introduce more complex checkpoint saving behaviors
        self.history.append(self._pending_metrics)  # This will increase self.epoch
        if save_checkpoints:
            self.save_checkpoints()
        self._start_epoch()
        if flashy.distrib.is_rank_zero():
            self.xp.link.update_history(self.history)

    def run_epoch(self):
        &#34;&#34;&#34;Run a single epoch with all stages.

        Metrics for a given stage are stored in _pending_metrics and committed by the solver afterwards.
        Children solvers can extend this method with custom behavior, e.g.:

            def run_epoch(self):
                ... # custom code
                super().run_epoch()
                ... # custom code
        &#34;&#34;&#34;
        self.run_stage(&#39;train&#39;, self.train)
        with torch.no_grad():
            with self.swap_ema_state():
                self.run_stage(&#39;valid&#39;, self.valid)
                # the best state is updated with EMA states if available
                self.update_best_state_from_stage(&#39;valid&#39;)
            with self.swap_best_state():
                if self.should_run_stage(&#39;evaluate&#39;):
                    self.run_stage(&#39;evaluate&#39;, self.evaluate)
                if self.should_run_stage(&#39;generate&#39;):
                    self.run_stage(&#39;generate&#39;, with_rank_rng()(self.generate))

    def run(self):
        &#34;&#34;&#34;Training loop.&#34;&#34;&#34;
        assert len(self.state_dict()) &gt; 0
        self.restore(replay_metrics=True)  # load checkpoint and replay history
        self.log_hyperparams(dict_from_config(self.cfg))
        for epoch in range(self.epoch, self.cfg.optim.epochs + 1):
            if self.should_stop_training():
                return
            self.run_epoch()
            # Commit will send the metrics to Dora and save checkpoints by default.
            self.commit()

    def should_stop_training(self) -&gt; bool:
        &#34;&#34;&#34;Check whether we should stop training or not.&#34;&#34;&#34;
        return self.epoch &gt; self.cfg.optim.epochs

    def should_run_stage(self, stage_name) -&gt; bool:
        &#34;&#34;&#34;Check whether we want to run the specified stages.&#34;&#34;&#34;
        stage_every = self.cfg[stage_name].get(&#39;every&#39;, None)
        is_last_epoch = self.epoch == self.cfg.optim.epochs
        is_epoch_every = (stage_every and self.epoch % stage_every == 0)
        return is_last_epoch or is_epoch_every

    @abstractmethod
    def run_step(self, idx: int, batch: tp.Any, metrics: dict):
        &#34;&#34;&#34;Perform one training or valid step on a given batch.&#34;&#34;&#34;
        ...

    def common_train_valid(self, dataset_split: str, **kwargs: tp.Any):
        &#34;&#34;&#34;Common logic for train and valid stages.&#34;&#34;&#34;
        self.model.train(self.is_training)

        loader = self.dataloaders[dataset_split]
        # get a different order for distributed training, otherwise this will get ignored
        if flashy.distrib.world_size() &gt; 1 \
           and isinstance(loader.sampler, torch.utils.data.distributed.DistributedSampler):
            loader.sampler.set_epoch(self.epoch)
        updates_per_epoch = self.train_updates_per_epoch if self.is_training else len(loader)
        if self.cfg.benchmark_no_load:
            self.logger.warning(&#34;Fake loading for benchmarking: re-using first batch&#34;)
            batch = next(iter(loader))
            loader = [batch] * updates_per_epoch  # type: ignore
        lp = self.log_progress(self.current_stage, loader, total=updates_per_epoch, updates=self.log_updates)
        average = flashy.averager()  # epoch wise average
        instant_average = flashy.averager()  # average between two logging
        metrics: dict = {}

        with self.profiler, self.deadlock_detect:  # profiler will only run for the first 20 updates.
            for idx, batch in enumerate(lp):
                self.deadlock_detect.update(&#39;batch&#39;)
                if idx &gt;= updates_per_epoch:
                    break
                metrics = {}
                metrics = self.run_step(idx, batch, metrics)
                self.deadlock_detect.update(&#39;step&#39;)
                # run EMA step
                if self.ema is not None and self.is_training and (idx + 1) % self.cfg.optim.ema.updates == 0:
                    self.logger.debug(&#34;EMA model step&#34;)
                    self.ema.step()
                self.deadlock_detect.update(&#39;ema&#39;)
                self.profiler.step()
                instant_metrics = instant_average(metrics)
                if lp.update(**instant_metrics):
                    instant_average = flashy.averager()  # reset averager between two logging
                metrics = average(metrics)  # epoch wise average
                self.deadlock_detect.update(&#39;end_batch&#39;)

        metrics = flashy.distrib.average_metrics(metrics, updates_per_epoch)
        return metrics

    def train(self):
        &#34;&#34;&#34;Train stage.&#34;&#34;&#34;
        return self.common_train_valid(&#39;train&#39;)

    def valid(self):
        &#34;&#34;&#34;Valid stage.&#34;&#34;&#34;
        return self.common_train_valid(&#39;valid&#39;)

    @abstractmethod
    def evaluate(self):
        &#34;&#34;&#34;Evaluate stage.&#34;&#34;&#34;
        ...

    @abstractmethod
    def generate(self):
        &#34;&#34;&#34;Generate stage.&#34;&#34;&#34;
        ...

    def run_one_stage(self, stage_name: str):
        &#34;&#34;&#34;Run only the specified stage.
        This method is useful to only generate samples from a trained experiment
        or rerun the validation or evaluation stages.
        &#34;&#34;&#34;
        fn = {
            &#39;generate&#39;: with_rank_rng()(self.generate),
            &#39;evaluate&#39;: self.evaluate,
            &#39;valid&#39;: self.valid,
        }
        if stage_name not in fn:
            raise ValueError(f&#39;Trying to run stage {stage_name} is not supported.&#39;)
        assert len(self.state_dict()) &gt; 0
        self._start_epoch()
        with torch.no_grad(), self.swap_best_state():
            self.run_stage(stage_name, fn[stage_name])
        if not self.cfg.execute_inplace:
            self.commit(save_checkpoints=False)

    @staticmethod
    def get_eval_solver_from_sig(sig: str, dtype: tp.Optional[str] = None,
                                 device: tp.Optional[str] = None, autocast: bool = True,
                                 batch_size: tp.Optional[int] = None,
                                 override_cfg: tp.Optional[tp.Union[dict, omegaconf.DictConfig]] = None,
                                 **kwargs):
        &#34;&#34;&#34;Mostly a convenience function around audiocraft.train.get_solver_from_sig,
        populating all the proper param, deactivating EMA, FSDP, loading the best state,
        basically all you need to get a solver ready to &#34;play&#34; with in single GPU mode
        and with minimal memory overhead.

        Args:
            sig (str): signature to load.
            dtype (str or None): potential dtype, as a string, i.e. &#39;float16&#39;.
            device (str or None): potential device, as a string, i.e. &#39;cuda&#39;.
            override_cfg (dict or omegaconf.DictConfig or None): potential device, as a string, i.e. &#39;cuda&#39;.
        &#34;&#34;&#34;
        from audiocraft import train
        our_override_cfg: tp.Dict[str, tp.Any] = {&#39;optim&#39;: {&#39;ema&#39;: {&#39;use&#39;: False}}}
        our_override_cfg[&#39;autocast&#39;] = autocast
        if dtype is not None:
            our_override_cfg[&#39;dtype&#39;] = dtype
        if device is not None:
            our_override_cfg[&#39;device&#39;] = device
        if batch_size is not None:
            our_override_cfg[&#39;dataset&#39;] = {&#39;batch_size&#39;: batch_size}
        if override_cfg is None:
            override_cfg = {}
        override_cfg = omegaconf.OmegaConf.merge(
            omegaconf.DictConfig(override_cfg), omegaconf.DictConfig(our_override_cfg))  # type: ignore
        solver = train.get_solver_from_sig(
            sig, override_cfg=override_cfg,
            load_best=True, disable_fsdp=True,
            ignore_state_keys=[&#39;optimizer&#39;, &#39;ema&#39;], **kwargs)
        solver.model.eval()
        return solver</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
<li>flashy.solver.BaseSolver</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="audiocraft.solvers.compression.CompressionSolver" href="compression.html#audiocraft.solvers.compression.CompressionSolver">CompressionSolver</a></li>
<li><a title="audiocraft.solvers.diffusion.DiffusionSolver" href="diffusion.html#audiocraft.solvers.diffusion.DiffusionSolver">DiffusionSolver</a></li>
<li><a title="audiocraft.solvers.musicgen.MusicGenSolver" href="musicgen.html#audiocraft.solvers.musicgen.MusicGenSolver">MusicGenSolver</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="audiocraft.solvers.base.StandardSolver.get_eval_solver_from_sig"><code class="name flex">
<span>def <span class="ident">get_eval_solver_from_sig</span></span>(<span>sig: str, dtype: Optional[str] = None, device: Optional[str] = None, autocast: bool = True, batch_size: Optional[int] = None, override_cfg: Union[dict, omegaconf.dictconfig.DictConfig, None] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Mostly a convenience function around audiocraft.train.get_solver_from_sig,
populating all the proper param, deactivating EMA, FSDP, loading the best state,
basically all you need to get a solver ready to "play" with in single GPU mode
and with minimal memory overhead.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sig</code></strong> :&ensp;<code>str</code></dt>
<dd>signature to load.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>potential dtype, as a string, i.e. 'float16'.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>potential device, as a string, i.e. 'cuda'.</dd>
<dt><strong><code>override_cfg</code></strong> :&ensp;<code>dict</code> or <code>omegaconf.DictConfig</code> or <code>None</code></dt>
<dd>potential device, as a string, i.e. 'cuda'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_eval_solver_from_sig(sig: str, dtype: tp.Optional[str] = None,
                             device: tp.Optional[str] = None, autocast: bool = True,
                             batch_size: tp.Optional[int] = None,
                             override_cfg: tp.Optional[tp.Union[dict, omegaconf.DictConfig]] = None,
                             **kwargs):
    &#34;&#34;&#34;Mostly a convenience function around audiocraft.train.get_solver_from_sig,
    populating all the proper param, deactivating EMA, FSDP, loading the best state,
    basically all you need to get a solver ready to &#34;play&#34; with in single GPU mode
    and with minimal memory overhead.

    Args:
        sig (str): signature to load.
        dtype (str or None): potential dtype, as a string, i.e. &#39;float16&#39;.
        device (str or None): potential device, as a string, i.e. &#39;cuda&#39;.
        override_cfg (dict or omegaconf.DictConfig or None): potential device, as a string, i.e. &#39;cuda&#39;.
    &#34;&#34;&#34;
    from audiocraft import train
    our_override_cfg: tp.Dict[str, tp.Any] = {&#39;optim&#39;: {&#39;ema&#39;: {&#39;use&#39;: False}}}
    our_override_cfg[&#39;autocast&#39;] = autocast
    if dtype is not None:
        our_override_cfg[&#39;dtype&#39;] = dtype
    if device is not None:
        our_override_cfg[&#39;device&#39;] = device
    if batch_size is not None:
        our_override_cfg[&#39;dataset&#39;] = {&#39;batch_size&#39;: batch_size}
    if override_cfg is None:
        override_cfg = {}
    override_cfg = omegaconf.OmegaConf.merge(
        omegaconf.DictConfig(override_cfg), omegaconf.DictConfig(our_override_cfg))  # type: ignore
    solver = train.get_solver_from_sig(
        sig, override_cfg=override_cfg,
        load_best=True, disable_fsdp=True,
        ignore_state_keys=[&#39;optimizer&#39;, &#39;ema&#39;], **kwargs)
    solver.model.eval()
    return solver</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="audiocraft.solvers.base.StandardSolver.autocast"><code class="name">var <span class="ident">autocast</span></code></dt>
<dd>
<div class="desc"><p>Convenient autocast (or not) using the solver configuration.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def autocast(self):
    &#34;&#34;&#34;Convenient autocast (or not) using the solver configuration.&#34;&#34;&#34;
    return TorchAutocast(enabled=self.cfg.autocast, device_type=self.device, dtype=self.autocast_dtype)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.best_metric_name"><code class="name">var <span class="ident">best_metric_name</span> : Optional[str]</code></dt>
<dd>
<div class="desc"><p>Metric name used to identify the best state. This metric should be stored in the metrics
used on the stage for best state identification (most likely, <code>valid</code>). If None, then
no best state is saved.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def best_metric_name(self) -&gt; tp.Optional[str]:
    &#34;&#34;&#34;Metric name used to identify the best state. This metric should be stored in the metrics
    used on the stage for best state identification (most likely, `valid`). If None, then
    no best state is saved.
    &#34;&#34;&#34;
    return None</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.is_training"><code class="name">var <span class="ident">is_training</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_training(self):
    return self.current_stage == &#39;train&#39;</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.log_updates"><code class="name">var <span class="ident">log_updates</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def log_updates(self):
    # convenient access to log updates
    return self._log_updates</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="audiocraft.solvers.base.StandardSolver.build_dataloaders"><code class="name flex">
<span>def <span class="ident">build_dataloaders</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to implement to initialize dataloaders.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def build_dataloaders(self):
    &#34;&#34;&#34;Method to implement to initialize dataloaders.&#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.build_model"><code class="name flex">
<span>def <span class="ident">build_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to implement to initialize model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def build_model(self):
    &#34;&#34;&#34;Method to implement to initialize model.&#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.checkpoint_path"><code class="name flex">
<span>def <span class="ident">checkpoint_path</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def checkpoint_path(self, **kwargs):
    kwargs.setdefault(&#39;use_fsdp&#39;, self.cfg.fsdp.use)
    return self.folder / checkpoint.checkpoint_name(**kwargs)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.checkpoint_path_with_name"><code class="name flex">
<span>def <span class="ident">checkpoint_path_with_name</span></span>(<span>self, name: str, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def checkpoint_path_with_name(self, name: str, **kwargs):
    kwargs.setdefault(&#39;use_fsdp&#39;, self.cfg.fsdp.use)
    return self.folder / checkpoint.checkpoint_name(name=name, **kwargs)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.commit"><code class="name flex">
<span>def <span class="ident">commit</span></span>(<span>self, save_checkpoints: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Commit metrics to dora and save checkpoints at the end of an epoch.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def commit(self, save_checkpoints: bool = True):
    &#34;&#34;&#34;Commit metrics to dora and save checkpoints at the end of an epoch.&#34;&#34;&#34;
    # we override commit to introduce more complex checkpoint saving behaviors
    self.history.append(self._pending_metrics)  # This will increase self.epoch
    if save_checkpoints:
        self.save_checkpoints()
    self._start_epoch()
    if flashy.distrib.is_rank_zero():
        self.xp.link.update_history(self.history)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.common_train_valid"><code class="name flex">
<span>def <span class="ident">common_train_valid</span></span>(<span>self, dataset_split: str, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Common logic for train and valid stages.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def common_train_valid(self, dataset_split: str, **kwargs: tp.Any):
    &#34;&#34;&#34;Common logic for train and valid stages.&#34;&#34;&#34;
    self.model.train(self.is_training)

    loader = self.dataloaders[dataset_split]
    # get a different order for distributed training, otherwise this will get ignored
    if flashy.distrib.world_size() &gt; 1 \
       and isinstance(loader.sampler, torch.utils.data.distributed.DistributedSampler):
        loader.sampler.set_epoch(self.epoch)
    updates_per_epoch = self.train_updates_per_epoch if self.is_training else len(loader)
    if self.cfg.benchmark_no_load:
        self.logger.warning(&#34;Fake loading for benchmarking: re-using first batch&#34;)
        batch = next(iter(loader))
        loader = [batch] * updates_per_epoch  # type: ignore
    lp = self.log_progress(self.current_stage, loader, total=updates_per_epoch, updates=self.log_updates)
    average = flashy.averager()  # epoch wise average
    instant_average = flashy.averager()  # average between two logging
    metrics: dict = {}

    with self.profiler, self.deadlock_detect:  # profiler will only run for the first 20 updates.
        for idx, batch in enumerate(lp):
            self.deadlock_detect.update(&#39;batch&#39;)
            if idx &gt;= updates_per_epoch:
                break
            metrics = {}
            metrics = self.run_step(idx, batch, metrics)
            self.deadlock_detect.update(&#39;step&#39;)
            # run EMA step
            if self.ema is not None and self.is_training and (idx + 1) % self.cfg.optim.ema.updates == 0:
                self.logger.debug(&#34;EMA model step&#34;)
                self.ema.step()
            self.deadlock_detect.update(&#39;ema&#39;)
            self.profiler.step()
            instant_metrics = instant_average(metrics)
            if lp.update(**instant_metrics):
                instant_average = flashy.averager()  # reset averager between two logging
            metrics = average(metrics)  # epoch wise average
            self.deadlock_detect.update(&#39;end_batch&#39;)

    metrics = flashy.distrib.average_metrics(metrics, updates_per_epoch)
    return metrics</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.epoch_checkpoint_path"><code class="name flex">
<span>def <span class="ident">epoch_checkpoint_path</span></span>(<span>self, epoch: int, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def epoch_checkpoint_path(self, epoch: int, **kwargs):
    kwargs.setdefault(&#39;use_fsdp&#39;, self.cfg.fsdp.use)
    return self.folder / checkpoint.checkpoint_name(str(epoch), **kwargs)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate stage.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def evaluate(self):
    &#34;&#34;&#34;Evaluate stage.&#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.generate"><code class="name flex">
<span>def <span class="ident">generate</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate stage.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def generate(self):
    &#34;&#34;&#34;Generate stage.&#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.initialize_ema"><code class="name flex">
<span>def <span class="ident">initialize_ema</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize exponential moving average with the registered sources.
EMA object is created if the optim.ema.model.decay value is non-null.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_ema(self):
    &#34;&#34;&#34;Initialize exponential moving average with the registered sources.
    EMA object is created if the optim.ema.model.decay value is non-null.
    &#34;&#34;&#34;
    from .builders import get_ema
    self.ema = get_ema(self._ema_sources, self.cfg.optim.ema)
    if self.ema is None:
        self.logger.info(&#39;No EMA on the model.&#39;)
    else:
        assert self.cfg.optim.ema.updates &gt; 0
        self.logger.info(
            f&#39;Initializing EMA on the model with decay = {self.ema.decay}&#39;
            f&#39; every {self.cfg.optim.ema.updates} updates&#39;
        )</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.load_checkpoints"><code class="name flex">
<span>def <span class="ident">load_checkpoints</span></span>(<span>self, load_best: bool = False, ignore_state_keys: List[str] = []) ‑> Optional[dict]</span>
</code></dt>
<dd>
<div class="desc"><p>Load last checkpoint or the one specified in continue_from.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>load_best</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to load from best state dict or not.
Best state dict is always used when not loading the current xp.</dd>
<dt><strong><code>ignore_state_keys</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>List of sources to ignore when loading the state, e.g. <code>optimizer</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>state (dict, optional): The loaded state dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_checkpoints(self, load_best: bool = False, ignore_state_keys: tp.List[str] = []) -&gt; tp.Optional[dict]:
    &#34;&#34;&#34;Load last checkpoint or the one specified in continue_from.

    Args:
        load_best (bool): Whether to load from best state dict or not.
            Best state dict is always used when not loading the current xp.
        ignore_state_keys (list of str): List of sources to ignore when loading the state, e.g. `optimizer`.
    Returns:
        state (dict, optional): The loaded state dictionary.
    &#34;&#34;&#34;
    # load checkpoints from xp folder or cfg.continue_from
    is_sharded = self.cfg.fsdp.use
    load_from_path: tp.Optional[Path] = None
    checkpoint_source: tp.Optional[checkpoint.CheckpointSource] = None

    if load_best:
        self.logger.info(&#34;Trying to load state_dict from best state.&#34;)

    state: tp.Optional[dict] = None
    rank0_checkpoint_path = self.checkpoint_path(use_fsdp=False)
    current_checkpoint_path = self.checkpoint_path()
    _pretrained_prefix = &#39;//pretrained/&#39;
    continue_pretrained = (self.cfg.continue_from or &#39;&#39;).startswith(_pretrained_prefix)
    if rank0_checkpoint_path.exists():
        self.logger.info(f&#34;Loading existing checkpoint: {current_checkpoint_path}&#34;)
        load_from_path = current_checkpoint_path
        checkpoint.check_sharded_checkpoint(current_checkpoint_path, rank0_checkpoint_path)
        checkpoint_source = checkpoint.CheckpointSource.CURRENT_XP
    elif self.cfg.continue_from and not continue_pretrained:
        self.logger.info(f&#34;Continuing from provided checkpoint: {self.cfg.continue_from}&#34;)
        # we&#39;re always continuing from consolidated checkpoints: self.cfg.use_fsdp and not continue_best
        load_from_path = checkpoint.resolve_checkpoint_path(self.cfg.continue_from, use_fsdp=False)
        if load_from_path is None:
            self.logger.error(&#39;Could not resolve the continue_from checkpoint %s&#39;, self.cfg.continue_from)
            raise RuntimeError(f&#39;Could not resolve continue_from checkpoint {self.cfg.continue_from}&#39;)
        checkpoint_source = checkpoint.CheckpointSource.OTHER

    if load_from_path is not None:
        state = checkpoint.load_checkpoint(load_from_path, is_sharded)
    elif continue_pretrained:
        self.logger.info(&#34;Loading a pretrained model. Ignoring &#39;load_best&#39; and &#39;ignore_state_keys&#39; params.&#34;)
        state = self.load_from_pretrained(self.cfg.continue_from[len(_pretrained_prefix):])
        checkpoint_source = checkpoint.CheckpointSource.PRETRAINED
        load_best = True

    # checkpoints are not from the current xp, we only retrieve the best state
    if checkpoint_source is not None and checkpoint_source != checkpoint.CheckpointSource.CURRENT_XP:
        assert state is not None
        self.logger.info(&#34;Checkpoint source is not the current xp: Load state_dict from best state.&#34;)
        load_best = True
        state = {key: state[key] for key in self._continue_best_source_keys if key in state}
        # loaded checkpoints are FSDP checkpoints: we&#39;re reading the best state
        # from FSDP and we drop the regular best_state
        if &#39;fsdp_best_state&#39; in state and state[&#39;fsdp_best_state&#39;]:
            state.pop(&#39;best_state&#39;, None)
            self.logger.info(&#34;... Loaded checkpoint has FSDP best state&#34;)
        # FSDP is enabled in the solver, if the loaded checkpoints do not have FSDP support
        # then we&#39;re initializing FSDP best state with the regular best state
        elif self.cfg.fsdp.use:
            if &#39;fsdp_best_state&#39; not in state or not state[&#39;fsdp_best_state&#39;]:
                # we swap non-FSDP checkpoints best_state to FSDP-compatible best state
                state[&#39;fsdp_best_state&#39;] = state.pop(&#39;best_state&#39;)
                self.logger.info(&#34;... Loaded checkpoint does not have FSDP best state. Use regular best state&#34;)

    if state is not None:
        if load_best:
            self.logger.info(&#34;Ignoring keys when loading best %r&#34;, ignore_state_keys)
            for key in set(ignore_state_keys):
                if key in state:
                    state.pop(key)
            has_best_state = &#39;best_state&#39; in state or &#39;fsdp_best_state&#39; in state
            assert has_best_state, (&#34;Trying to load best state but neither &#39;best_state&#39;&#34;,
                                    &#34; or &#39;fsdp_best_state&#39; found in checkpoints.&#34;)
        self.load_state_dict(state)

    # for FSDP, let&#39;s make extra sure nothing bad happened with out of sync
    # checkpoints across workers.
    epoch = float(self.epoch)
    avg_epoch = flashy.distrib.average_metrics({&#39;epoch&#39;: epoch})[&#39;epoch&#39;]
    if avg_epoch != epoch:
        raise RuntimeError(
            f&#34;Inconsistent loading of checkpoints happened, our epoch is {epoch} &#34;
            f&#34;but average of epochs is {avg_epoch}, at least one gpu must have a &#34;
            &#34;different epoch number.&#34;)

    # on load_best, properly reinitialize state_dict, best states and ema
    # otherwise we load from the current xp and don&#39;t alter anything
    if load_best:
        self.logger.info(&#34;Loading state_dict from best state.&#34;)
        if not self.cfg.fsdp.use and self.fsdp_best_state:
            # loading from an FSDP checkpoint but with FSDP deactivated
            self.logger.info(&#34;... Loading from FSDP best state dict.&#34;)
            self.best_state.load_state_dict(self.fsdp_best_state)

        # if load_best, we permanently override the regular state_dict with the best state
        if self.cfg.fsdp.use:
            self.logger.info(&#34;FSDP is used, loading from FSDP best state.&#34;)
            with fsdp.switch_to_full_state_dict(self._fsdp_modules):
                # this might be really fragile but okay for now.
                self.load_state_dict(self.fsdp_best_state)
        else:
            # we permanently swap the stateful objects to their best state
            self._load_new_state_dict(self.best_state.state_dict())

        # the EMA modules should also be instantiated with best state.
        # the easiest way to do so is to reinitialize a new EMA with best state loaded.
        if self.ema is not None:
            self.logger.info(&#34;Re-initializing EMA from best state&#34;)
            self.initialize_ema()

        if self.cfg.fsdp.use:
            self.logger.info(&#34;Re-initializing best state after using FSDP best state.&#34;)
            for name in self.best_state.states.keys():
                state_source = self._get_state_source(name)
                self.best_state.update(name, state_source)

    return state</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.load_from_pretrained"><code class="name flex">
<span>def <span class="ident">load_from_pretrained</span></span>(<span>self, name: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_from_pretrained(self, name: str) -&gt; dict:
    raise NotImplementedError(&#34;Solver does not provide a way to load pretrained models.&#34;)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.log_model_summary"><code class="name flex">
<span>def <span class="ident">log_model_summary</span></span>(<span>self, model: torch.nn.modules.module.Module)</span>
</code></dt>
<dd>
<div class="desc"><p>Log model summary, architecture and size of the model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_model_summary(self, model: nn.Module):
    &#34;&#34;&#34;Log model summary, architecture and size of the model.&#34;&#34;&#34;
    self.logger.info(model)
    mb = sum(p.numel() for p in model.parameters()) * 4 / 2 ** 20
    self.logger.info(&#34;Size: %.1f MB&#34;, mb)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.register_best_state"><code class="name flex">
<span>def <span class="ident">register_best_state</span></span>(<span>self, *args: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Register state sources in <code>BestStateDictManager</code> to keep their best states along with their
latest states. The best state will be used at evaluation stages instead of the latest states.</p>
<p>Shortcut around <code>BestStateDictManager.register</code> method. You can pass any number of
attribute, included nested attributes and those will be included into the checkpoints
and automatically restored when <code>BaseSolver.restore</code> is called.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def register_best_state(self, *args: str):
    &#34;&#34;&#34;Register state sources in `BestStateDictManager` to keep their best states along with their
    latest states. The best state will be used at evaluation stages instead of the latest states.

    Shortcut around `BestStateDictManager.register` method. You can pass any number of
    attribute, included nested attributes and those will be included into the checkpoints
    and automatically restored when `BaseSolver.restore` is called.
    &#34;&#34;&#34;
    for name in args:
        state_source = self._get_state_source(name)
        assert name in self.stateful.sources, &#34;Registered states in best should be registered in stateful first!&#34;
        self.best_state.register(name, state_source)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.register_ema"><code class="name flex">
<span>def <span class="ident">register_ema</span></span>(<span>self, *args: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Register state sources for exponential moving average.</p>
<p>The registered sources are used to instantiate a ModuleDictEMA instance.
The ModuleDictEMA keeps a <code>nn.ModuleDict</code> module that is updated when self.ema.step() is called
and swapped with the original state sources with self.swap_ema_state() method.</p>
<h2 id="usage">Usage</h2>
<p>self.register_ema('model')</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def register_ema(self, *args: str):
    &#34;&#34;&#34;Register state sources for exponential moving average.

    The registered sources are used to instantiate a ModuleDictEMA instance.
    The ModuleDictEMA keeps a `nn.ModuleDict` module that is updated when self.ema.step() is called
    and swapped with the original state sources with self.swap_ema_state() method.

    Usage:
        self.register_ema(&#39;model&#39;)
    &#34;&#34;&#34;
    assert self.ema is None, &#34;Cannot register state source to already instantiated EMA.&#34;
    for name in args:
        self._ema_sources[name] = getattr(self, name)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.restore"><code class="name flex">
<span>def <span class="ident">restore</span></span>(<span>self, load_best: bool = False, replay_metrics: bool = False, ignore_state_keys: List[str] = []) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Restore the status of a solver for a given xp.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>load_best</code></strong> :&ensp;<code>bool</code></dt>
<dd>if <code>True</code>, load the best state from the checkpoint.</dd>
<dt><strong><code>replay_metrics</code></strong> :&ensp;<code>bool</code></dt>
<dd>if <code>True</code>, logs all the metrics from past epochs.</dd>
<dt><strong><code>ignore_state_keys</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>list of sources to ignore when loading the state, e.g. <code>optimizer</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def restore(self, load_best: bool = False, replay_metrics: bool = False,
            ignore_state_keys: tp.List[str] = []) -&gt; bool:
    &#34;&#34;&#34;Restore the status of a solver for a given xp.

    Args:
        load_best (bool): if `True`, load the best state from the checkpoint.
        replay_metrics (bool): if `True`, logs all the metrics from past epochs.
        ignore_state_keys (list of str): list of sources to ignore when loading the state, e.g. `optimizer`.
    &#34;&#34;&#34;
    self.logger.info(&#34;Restoring weights and history.&#34;)
    restored_checkpoints = self.load_checkpoints(load_best, ignore_state_keys)

    self.logger.info(&#34;Model hash: %s&#34;, model_hash(self.model))

    if replay_metrics and len(self.history) &gt; 0:
        self.logger.info(&#34;Replaying past metrics...&#34;)
        for epoch, stages in enumerate(self.history):
            for stage_name, metrics in stages.items():
                # We manually log the metrics summary to the result logger
                # as we don&#39;t want to add them to the pending metrics
                self.result_logger._log_summary(stage_name, metrics, step=epoch + 1, step_name=&#39;epoch&#39;,
                                                formatter=self.get_formatter(stage_name))
    return restored_checkpoints is not None</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Training loop.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34;Training loop.&#34;&#34;&#34;
    assert len(self.state_dict()) &gt; 0
    self.restore(replay_metrics=True)  # load checkpoint and replay history
    self.log_hyperparams(dict_from_config(self.cfg))
    for epoch in range(self.epoch, self.cfg.optim.epochs + 1):
        if self.should_stop_training():
            return
        self.run_epoch()
        # Commit will send the metrics to Dora and save checkpoints by default.
        self.commit()</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.run_epoch"><code class="name flex">
<span>def <span class="ident">run_epoch</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Run a single epoch with all stages.</p>
<p>Metrics for a given stage are stored in _pending_metrics and committed by the solver afterwards.
Children solvers can extend this method with custom behavior, e.g.:</p>
<pre><code>def run_epoch(self):
    ... # custom code
    super().run_epoch()
    ... # custom code
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_epoch(self):
    &#34;&#34;&#34;Run a single epoch with all stages.

    Metrics for a given stage are stored in _pending_metrics and committed by the solver afterwards.
    Children solvers can extend this method with custom behavior, e.g.:

        def run_epoch(self):
            ... # custom code
            super().run_epoch()
            ... # custom code
    &#34;&#34;&#34;
    self.run_stage(&#39;train&#39;, self.train)
    with torch.no_grad():
        with self.swap_ema_state():
            self.run_stage(&#39;valid&#39;, self.valid)
            # the best state is updated with EMA states if available
            self.update_best_state_from_stage(&#39;valid&#39;)
        with self.swap_best_state():
            if self.should_run_stage(&#39;evaluate&#39;):
                self.run_stage(&#39;evaluate&#39;, self.evaluate)
            if self.should_run_stage(&#39;generate&#39;):
                self.run_stage(&#39;generate&#39;, with_rank_rng()(self.generate))</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.run_one_stage"><code class="name flex">
<span>def <span class="ident">run_one_stage</span></span>(<span>self, stage_name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Run only the specified stage.
This method is useful to only generate samples from a trained experiment
or rerun the validation or evaluation stages.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_one_stage(self, stage_name: str):
    &#34;&#34;&#34;Run only the specified stage.
    This method is useful to only generate samples from a trained experiment
    or rerun the validation or evaluation stages.
    &#34;&#34;&#34;
    fn = {
        &#39;generate&#39;: with_rank_rng()(self.generate),
        &#39;evaluate&#39;: self.evaluate,
        &#39;valid&#39;: self.valid,
    }
    if stage_name not in fn:
        raise ValueError(f&#39;Trying to run stage {stage_name} is not supported.&#39;)
    assert len(self.state_dict()) &gt; 0
    self._start_epoch()
    with torch.no_grad(), self.swap_best_state():
        self.run_stage(stage_name, fn[stage_name])
    if not self.cfg.execute_inplace:
        self.commit(save_checkpoints=False)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.run_step"><code class="name flex">
<span>def <span class="ident">run_step</span></span>(<span>self, idx: int, batch: Any, metrics: dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform one training or valid step on a given batch.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def run_step(self, idx: int, batch: tp.Any, metrics: dict):
    &#34;&#34;&#34;Perform one training or valid step on a given batch.&#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.save_checkpoints"><code class="name flex">
<span>def <span class="ident">save_checkpoints</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Save checkpoint, optionally keeping a copy for a given epoch.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_checkpoints(self):
    &#34;&#34;&#34;Save checkpoint, optionally keeping a copy for a given epoch.&#34;&#34;&#34;
    is_sharded = self.cfg.fsdp.use
    if not flashy.distrib.is_rank_zero() and not is_sharded:
        return
    self.logger.info(&#34;Model hash: %s&#34;, model_hash(self.model))
    state = self.state_dict()
    epoch = self.epoch - 1  # pushing metrics will increase the epoch in Flashy, so we do -1 here

    # save minimal state_dict as new checkpoint every X epoch
    if self.cfg.checkpoint.save_every:
        if epoch % self.cfg.checkpoint.save_every == 0:
            minimal_state = state
            if self.cfg.checkpoint.keep_every_states is not None and len(self.cfg.checkpoint.keep_every_states) &gt; 0:
                minimal_state = {
                    name: source for name, source in state.items()
                    if name in self.cfg.checkpoint.keep_every_states
                }
            epoch_checkpoint_path = self.epoch_checkpoint_path(epoch)
            checkpoint.save_checkpoint(minimal_state, epoch_checkpoint_path, is_sharded)

    # save checkpoint as latest checkpoint
    if self.cfg.checkpoint.save_last:
        last_checkpoint_path = self.checkpoint_path()
        checkpoint.save_checkpoint(state, last_checkpoint_path, is_sharded)

    # flush any stale checkpoint to reduce disk footprint
    checkpoint.flush_stale_checkpoints(self.checkpoint_path())</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.should_run_stage"><code class="name flex">
<span>def <span class="ident">should_run_stage</span></span>(<span>self, stage_name) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Check whether we want to run the specified stages.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def should_run_stage(self, stage_name) -&gt; bool:
    &#34;&#34;&#34;Check whether we want to run the specified stages.&#34;&#34;&#34;
    stage_every = self.cfg[stage_name].get(&#39;every&#39;, None)
    is_last_epoch = self.epoch == self.cfg.optim.epochs
    is_epoch_every = (stage_every and self.epoch % stage_every == 0)
    return is_last_epoch or is_epoch_every</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.should_stop_training"><code class="name flex">
<span>def <span class="ident">should_stop_training</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Check whether we should stop training or not.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def should_stop_training(self) -&gt; bool:
    &#34;&#34;&#34;Check whether we should stop training or not.&#34;&#34;&#34;
    return self.epoch &gt; self.cfg.optim.epochs</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.show"><code class="name flex">
<span>def <span class="ident">show</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to log any information without running the job.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def show(self):
    &#34;&#34;&#34;Method to log any information without running the job.&#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.swap_best_state"><code class="name flex">
<span>def <span class="ident">swap_best_state</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def swap_best_state(self):
    self.logger.debug(f&#34;Swapping to best state for: {&#39;, &#39;.join(self.best_state.state_dict().keys())}&#34;)
    old_states = self._load_new_state_dict(self.best_state.state_dict())
    try:
        yield
    finally:
        self.logger.debug(&#34;Swapping back from best to original state&#34;)
        for name, old_state in old_states.items():
            state_source = self._get_state_source(name)
            state_source.load_state_dict(old_state)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.swap_ema_state"><code class="name flex">
<span>def <span class="ident">swap_ema_state</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def swap_ema_state(self):
    if self.ema is None:
        yield
    else:
        ema_state_dict = self.ema.state_dict()[&#39;state&#39;]
        self.logger.debug(f&#34;Swapping to EMA state for: {&#39;, &#39;.join(ema_state_dict.keys())}&#34;)
        old_states = self._load_new_state_dict(ema_state_dict)
        try:
            yield
        finally:
            self.logger.debug(&#34;Swapping back from EMA state to original state&#34;)
            for name, old_state in old_states.items():
                state_source = self._get_state_source(name)
                state_source.load_state_dict(old_state)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Train stage.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self):
    &#34;&#34;&#34;Train stage.&#34;&#34;&#34;
    return self.common_train_valid(&#39;train&#39;)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.update_best_state_from_stage"><code class="name flex">
<span>def <span class="ident">update_best_state_from_stage</span></span>(<span>self, stage_name: str = 'valid')</span>
</code></dt>
<dd>
<div class="desc"><p>Update latest best state based on pending metrics of a given stage. This method relies
on the <code>BestStateDictManager.update</code> method to update the best state_dict with latest weights
if the registered states happen to match to the best performing setup.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_best_state_from_stage(self, stage_name: str = &#39;valid&#39;):
    &#34;&#34;&#34;Update latest best state based on pending metrics of a given stage. This method relies
    on the `BestStateDictManager.update` method to update the best state_dict with latest weights
    if the registered states happen to match to the best performing setup.
    &#34;&#34;&#34;
    if self.best_metric_name is None:
        # when no best metric is defined, the last state is always the best
        self._new_best_state = True
        self.logger.info(&#34;Updating best state with current state.&#34;)
    else:
        assert stage_name in self._pending_metrics, f&#34;Metrics for stage {stage_name} not found.&#34;
        assert self.best_metric_name in self._pending_metrics[stage_name], \
            f&#34;Best metric not found in {stage_name} metrics. Cannot register best state&#34;
        current_score = self._pending_metrics[stage_name][self.best_metric_name]
        all_best_metric_scores = [
            past_metrics[stage_name][self.best_metric_name]
            for past_metrics in self.history
        ]
        all_best_metric_scores.append(current_score)
        best_score = min(all_best_metric_scores)
        self._new_best_state = current_score == best_score
        if self._new_best_state:
            old_best = min(all_best_metric_scores[:-1] + [float(&#39;inf&#39;)])
            self.logger.info(
                f&#34;New best state with {self.best_metric_name}={current_score:.3f} (was {old_best:.3f})&#34;)

    if self._new_best_state:
        if self.cfg.fsdp.use:
            # this will give an empty state dict on all ranks but the rank 0
            # which will have a copy in memory of the full model.
            with fsdp.switch_to_full_state_dict(self._fsdp_modules):
                for name in self.best_state.states.keys():
                    state_source = self._get_state_source(name)
                    self.best_state.update(name, state_source)
                # we save to a different dict.
                self.fsdp_best_state.update(self.best_state.state_dict())
            # We cannot efficiently load fsdp_best_state when using FSDP,
            # so we have do do a second pass, with the local shards.
        for name in self.best_state.states.keys():
            state_source = self._get_state_source(name)
            self.best_state.update(name, state_source)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.valid"><code class="name flex">
<span>def <span class="ident">valid</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Valid stage.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def valid(self):
    &#34;&#34;&#34;Valid stage.&#34;&#34;&#34;
    return self.common_train_valid(&#39;valid&#39;)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.base.StandardSolver.wrap_with_fsdp"><code class="name flex">
<span>def <span class="ident">wrap_with_fsdp</span></span>(<span>self, model: torch.nn.modules.module.Module, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wrap_with_fsdp(self, model: torch.nn.Module, *args, **kwargs):
    model = fsdp.wrap_with_fsdp(self.cfg.fsdp, model, *args, **kwargs)
    if isinstance(model, fsdp.FSDP):
        self._fsdp_modules.append(model)
    return model</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="audiocraft.solvers" href="index.html">audiocraft.solvers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="audiocraft.solvers.base.StandardSolver" href="#audiocraft.solvers.base.StandardSolver">StandardSolver</a></code></h4>
<ul class="">
<li><code><a title="audiocraft.solvers.base.StandardSolver.autocast" href="#audiocraft.solvers.base.StandardSolver.autocast">autocast</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.best_metric_name" href="#audiocraft.solvers.base.StandardSolver.best_metric_name">best_metric_name</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.build_dataloaders" href="#audiocraft.solvers.base.StandardSolver.build_dataloaders">build_dataloaders</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.build_model" href="#audiocraft.solvers.base.StandardSolver.build_model">build_model</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.checkpoint_path" href="#audiocraft.solvers.base.StandardSolver.checkpoint_path">checkpoint_path</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.checkpoint_path_with_name" href="#audiocraft.solvers.base.StandardSolver.checkpoint_path_with_name">checkpoint_path_with_name</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.commit" href="#audiocraft.solvers.base.StandardSolver.commit">commit</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.common_train_valid" href="#audiocraft.solvers.base.StandardSolver.common_train_valid">common_train_valid</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.epoch_checkpoint_path" href="#audiocraft.solvers.base.StandardSolver.epoch_checkpoint_path">epoch_checkpoint_path</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.evaluate" href="#audiocraft.solvers.base.StandardSolver.evaluate">evaluate</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.generate" href="#audiocraft.solvers.base.StandardSolver.generate">generate</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.get_eval_solver_from_sig" href="#audiocraft.solvers.base.StandardSolver.get_eval_solver_from_sig">get_eval_solver_from_sig</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.initialize_ema" href="#audiocraft.solvers.base.StandardSolver.initialize_ema">initialize_ema</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.is_training" href="#audiocraft.solvers.base.StandardSolver.is_training">is_training</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.load_checkpoints" href="#audiocraft.solvers.base.StandardSolver.load_checkpoints">load_checkpoints</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.load_from_pretrained" href="#audiocraft.solvers.base.StandardSolver.load_from_pretrained">load_from_pretrained</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.log_model_summary" href="#audiocraft.solvers.base.StandardSolver.log_model_summary">log_model_summary</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.log_updates" href="#audiocraft.solvers.base.StandardSolver.log_updates">log_updates</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.register_best_state" href="#audiocraft.solvers.base.StandardSolver.register_best_state">register_best_state</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.register_ema" href="#audiocraft.solvers.base.StandardSolver.register_ema">register_ema</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.restore" href="#audiocraft.solvers.base.StandardSolver.restore">restore</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.run" href="#audiocraft.solvers.base.StandardSolver.run">run</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.run_epoch" href="#audiocraft.solvers.base.StandardSolver.run_epoch">run_epoch</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.run_one_stage" href="#audiocraft.solvers.base.StandardSolver.run_one_stage">run_one_stage</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.run_step" href="#audiocraft.solvers.base.StandardSolver.run_step">run_step</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.save_checkpoints" href="#audiocraft.solvers.base.StandardSolver.save_checkpoints">save_checkpoints</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.should_run_stage" href="#audiocraft.solvers.base.StandardSolver.should_run_stage">should_run_stage</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.should_stop_training" href="#audiocraft.solvers.base.StandardSolver.should_stop_training">should_stop_training</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.show" href="#audiocraft.solvers.base.StandardSolver.show">show</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.swap_best_state" href="#audiocraft.solvers.base.StandardSolver.swap_best_state">swap_best_state</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.swap_ema_state" href="#audiocraft.solvers.base.StandardSolver.swap_ema_state">swap_ema_state</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.train" href="#audiocraft.solvers.base.StandardSolver.train">train</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.update_best_state_from_stage" href="#audiocraft.solvers.base.StandardSolver.update_best_state_from_stage">update_best_state_from_stage</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.valid" href="#audiocraft.solvers.base.StandardSolver.valid">valid</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.wrap_with_fsdp" href="#audiocraft.solvers.base.StandardSolver.wrap_with_fsdp">wrap_with_fsdp</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>