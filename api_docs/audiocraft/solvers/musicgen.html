<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>audiocraft.solvers.musicgen API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>audiocraft.solvers.musicgen</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from pathlib import Path
import time
import typing as tp
import warnings

import flashy
import math
import omegaconf
import torch
from torch.nn import functional as F

from . import base, builders
from .compression import CompressionSolver
from .. import metrics as eval_metrics
from .. import models
from ..data.audio_dataset import AudioDataset
from ..data.music_dataset import MusicDataset, MusicInfo, AudioInfo
from ..data.audio_utils import normalize_audio
from ..modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition
from ..utils.cache import CachedBatchWriter, CachedBatchLoader
from ..utils.samples.manager import SampleManager
from ..utils.utils import get_dataset_from_loader, is_jsonable, warn_once, model_hash


class MusicGenSolver(base.StandardSolver):
    &#34;&#34;&#34;Solver for MusicGen training task.

    Used in: https://arxiv.org/abs/2306.05284
    &#34;&#34;&#34;
    DATASET_TYPE: builders.DatasetType = builders.DatasetType.MUSIC

    def __init__(self, cfg: omegaconf.DictConfig):
        super().__init__(cfg)
        # easier access to sampling parameters
        self.generation_params = {
            &#39;use_sampling&#39;: self.cfg.generate.lm.use_sampling,
            &#39;temp&#39;: self.cfg.generate.lm.temp,
            &#39;top_k&#39;: self.cfg.generate.lm.top_k,
            &#39;top_p&#39;: self.cfg.generate.lm.top_p,
        }
        self._best_metric_name: tp.Optional[str] = &#39;ce&#39;

        self._cached_batch_writer = None
        self._cached_batch_loader = None
        if cfg.cache.path:
            if cfg.cache.write:
                self._cached_batch_writer = CachedBatchWriter(Path(cfg.cache.path))
                if self.cfg.cache.write_num_shards:
                    self.logger.warning(&#34;Multiple shard cache, best_metric_name will be set to None.&#34;)
                    self._best_metric_name = None
            else:
                self._cached_batch_loader = CachedBatchLoader(
                    Path(cfg.cache.path), cfg.dataset.batch_size, cfg.dataset.num_workers,
                    min_length=self.cfg.optim.updates_per_epoch or 1)
                self.dataloaders[&#39;original_train&#39;] = self.dataloaders[&#39;train&#39;]
                self.dataloaders[&#39;train&#39;] = self._cached_batch_loader  # type: ignore

    @staticmethod
    def get_eval_solver_from_sig(sig: str, dtype: tp.Optional[str] = None,
                                 device: tp.Optional[str] = None, autocast: bool = True,
                                 batch_size: tp.Optional[int] = None,
                                 override_cfg: tp.Optional[tp.Union[dict, omegaconf.DictConfig]] = None,
                                 **kwargs):
        &#34;&#34;&#34;Mostly a convenience function around magma.train.get_solver_from_sig,
        populating all the proper param, deactivating EMA, FSDP, loading the best state,
        basically all you need to get a solver ready to &#34;play&#34; with in single GPU mode
        and with minimal memory overhead.

        Args:
            sig (str): signature to load.
            dtype (str or None): potential dtype, as a string, i.e. &#39;float16&#39;.
            device (str or None): potential device, as a string, i.e. &#39;cuda&#39;.
            override_cfg (dict or omegaconf.DictConfig or None): potential device, as a string, i.e. &#39;cuda&#39;.
        &#34;&#34;&#34;
        from audiocraft import train
        our_override_cfg: tp.Dict[str, tp.Any] = {&#39;optim&#39;: {&#39;ema&#39;: {&#39;use&#39;: False}}}
        our_override_cfg[&#39;autocast&#39;] = autocast
        if dtype is not None:
            our_override_cfg[&#39;dtype&#39;] = dtype
        if device is not None:
            our_override_cfg[&#39;device&#39;] = device
        if batch_size is not None:
            our_override_cfg[&#39;dataset&#39;] = {&#39;batch_size&#39;: batch_size}
        if override_cfg is None:
            override_cfg = {}
        override_cfg = omegaconf.OmegaConf.merge(
            omegaconf.DictConfig(override_cfg), omegaconf.DictConfig(our_override_cfg))  # type: ignore
        solver = train.get_solver_from_sig(
            sig, override_cfg=override_cfg,
            load_best=True, disable_fsdp=True,
            ignore_state_keys=[&#39;optimizer&#39;, &#39;ema&#39;], **kwargs)
        solver.model.eval()
        return solver

    def get_formatter(self, stage_name: str) -&gt; flashy.Formatter:
        return flashy.Formatter({
            &#39;lr&#39;: &#39;.2E&#39;,
            &#39;ce&#39;: &#39;.3f&#39;,
            &#39;ppl&#39;: &#39;.3f&#39;,
            &#39;grad_norm&#39;: &#39;.3E&#39;,
        }, exclude_keys=[&#39;ce_q*&#39;, &#39;ppl_q*&#39;])

    @property
    def best_metric_name(self) -&gt; tp.Optional[str]:
        return self._best_metric_name

    def build_model(self) -&gt; None:
        &#34;&#34;&#34;Instantiate models and optimizer.&#34;&#34;&#34;
        # we can potentially not use all quantizers with which the EnCodec model was trained
        # (e.g. we trained the model with quantizers dropout)
        self.compression_model = CompressionSolver.wrapped_model_from_checkpoint(
            self.cfg, self.cfg.compression_model_checkpoint, device=self.device)
        assert self.compression_model.sample_rate == self.cfg.sample_rate, (
            f&#34;Compression model sample rate is {self.compression_model.sample_rate} but &#34;
            f&#34;Solver sample rate is {self.cfg.sample_rate}.&#34;
            )
        # ensure we have matching configuration between LM and compression model
        assert self.cfg.transformer_lm.card == self.compression_model.cardinality, (
            &#34;Cardinalities of the LM and compression model don&#39;t match: &#34;,
            f&#34;LM cardinality is {self.cfg.transformer_lm.card} vs &#34;,
            f&#34;compression model cardinality is {self.compression_model.cardinality}&#34;
        )
        assert self.cfg.transformer_lm.n_q == self.compression_model.num_codebooks, (
            &#34;Numbers of codebooks of the LM and compression models don&#39;t match: &#34;,
            f&#34;LM number of codebooks is {self.cfg.transformer_lm.n_q} vs &#34;,
            f&#34;compression model numer of codebooks is {self.compression_model.num_codebooks}&#34;
        )
        self.logger.info(&#34;Compression model has %d codebooks with %d cardinality, and a framerate of %d&#34;,
                         self.compression_model.num_codebooks, self.compression_model.cardinality,
                         self.compression_model.frame_rate)
        # instantiate LM model
        self.model: models.LMModel = models.builders.get_lm_model(self.cfg).to(self.device)
        if self.cfg.fsdp.use:
            assert not self.cfg.autocast, &#34;Cannot use autocast with fsdp&#34;
            self.model = self.wrap_with_fsdp(self.model)
        self.register_ema(&#39;model&#39;)
        # initialize optimization
        self.optimizer = builders.get_optimizer(builders.get_optim_parameter_groups(self.model), self.cfg.optim)
        self.lr_scheduler = builders.get_lr_scheduler(self.optimizer, self.cfg.schedule, self.total_updates)
        self.register_stateful(&#39;model&#39;, &#39;optimizer&#39;, &#39;lr_scheduler&#39;)
        self.register_best_state(&#39;model&#39;)
        self.autocast_dtype = {
            &#39;float16&#39;: torch.float16, &#39;bfloat16&#39;: torch.bfloat16
        }[self.cfg.autocast_dtype]
        self.scaler: tp.Optional[torch.cuda.amp.GradScaler] = None
        if self.cfg.fsdp.use:
            need_scaler = self.cfg.fsdp.param_dtype == &#39;float16&#39;
        else:
            need_scaler = self.cfg.autocast and self.autocast_dtype is torch.float16
        if need_scaler:
            if self.cfg.fsdp.use:
                from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler
                self.scaler = ShardedGradScaler()  # type: ignore
            else:
                self.scaler = torch.cuda.amp.GradScaler()
            self.register_stateful(&#39;scaler&#39;)

    def build_dataloaders(self) -&gt; None:
        &#34;&#34;&#34;Instantiate audio dataloaders for each stage.&#34;&#34;&#34;
        self.dataloaders = builders.get_audio_datasets(self.cfg, dataset_type=self.DATASET_TYPE)

    def show(self) -&gt; None:
        &#34;&#34;&#34;Show the compression model and LM model.&#34;&#34;&#34;
        self.logger.info(&#34;Compression model:&#34;)
        self.log_model_summary(self.compression_model)
        self.logger.info(&#34;LM model:&#34;)
        self.log_model_summary(self.model)

    def load_state_dict(self, state: dict) -&gt; None:
        if &#39;condition_provider&#39; in state:
            model_state = state[&#39;model&#39;]
            condition_provider_state = state.pop(&#39;condition_provider&#39;)
            prefix = &#39;condition_provider.&#39;
            for key, value in condition_provider_state.items():
                key = prefix + key
                assert key not in model_state
                model_state[key] = value
        if &#39;compression_model&#39; in state:
            # We used to store the `compression_model` state in the checkpoint, however
            # this is in general not needed, as the compression model should always be readable
            # from the original `cfg.compression_model_checkpoint` location.
            compression_model_state = state.pop(&#39;compression_model&#39;)
            before_hash = model_hash(self.compression_model)
            self.compression_model.load_state_dict(compression_model_state)
            after_hash = model_hash(self.compression_model)
            if before_hash != after_hash:
                raise RuntimeError(
                    &#34;The compression model state inside the checkpoint is different&#34;
                    &#34; from the one obtained from compression_model_checkpoint...&#34;
                    &#34;We do not support altering the compression model inside the LM &#34;
                    &#34;checkpoint as parts of the code, in particular for running eval post-training &#34;
                    &#34;will use the compression_model_checkpoint as the source of truth.&#34;)

        super().load_state_dict(state)

    def load_from_pretrained(self, name: str):
        # TODO: support native HF versions of MusicGen.
        lm_pkg = models.loaders.load_lm_model_ckpt(name)
        state: dict = {
            &#39;best_state&#39;: {
                &#39;model&#39;: lm_pkg[&#39;best_state&#39;],
            },
        }
        return state

    def _compute_cross_entropy(
        self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor
    ) -&gt; tp.Tuple[torch.Tensor, tp.List[torch.Tensor]]:
        &#34;&#34;&#34;Compute cross entropy between multi-codebook targets and model&#39;s logits.
        The cross entropy is computed per codebook to provide codebook-level cross entropy.
        Valid timesteps for each of the codebook are pulled from the mask, where invalid
        timesteps are set to 0.

        Args:
            logits (torch.Tensor): Model&#39;s logits of shape [B, K, T, card].
            targets (torch.Tensor): Target codes, of shape [B, K, T].
            mask (torch.Tensor): Mask for valid target codes, of shape [B, K, T].
        Returns:
            ce (torch.Tensor): Cross entropy averaged over the codebooks
            ce_per_codebook (list of torch.Tensor): Cross entropy per codebook (detached).
        &#34;&#34;&#34;
        B, K, T = targets.shape
        assert logits.shape[:-1] == targets.shape
        assert mask.shape == targets.shape
        ce = torch.zeros([], device=targets.device)
        ce_per_codebook: tp.List[torch.Tensor] = []
        for k in range(K):
            logits_k = logits[:, k, ...].contiguous().view(-1, logits.size(-1))  # [B x T, card]
            targets_k = targets[:, k, ...].contiguous().view(-1)  # [B x T]
            mask_k = mask[:, k, ...].contiguous().view(-1)  # [B x T]
            ce_targets = targets_k[mask_k]
            ce_logits = logits_k[mask_k]
            q_ce = F.cross_entropy(ce_logits, ce_targets)
            ce += q_ce
            ce_per_codebook.append(q_ce.detach())
        # average cross entropy across codebooks
        ce = ce / K
        return ce, ce_per_codebook

    def _prepare_tokens_and_attributes(
        self, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]],
        check_synchronization_points: bool = False
    ) -&gt; tp.Tuple[dict, torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;Prepare input batchs for language model training.

        Args:
            batch (tuple[torch.Tensor, list[SegmentWithAttributes]]): Input batch with audio tensor of shape [B, C, T]
                and corresponding metadata as SegmentWithAttributes (with B items).
            check_synchronization_points (bool): Whether to check for synchronization points slowing down training.
        Returns:
            Condition tensors (dict[str, any]): Preprocessed condition attributes.
            Tokens (torch.Tensor): Audio tokens from compression model, of shape [B, K, T_s],
                with B the batch size, K the number of codebooks, T_s the token timesteps.
            Padding mask (torch.Tensor): Mask with valid positions in the tokens tensor, of shape [B, K, T_s].
        &#34;&#34;&#34;
        if self.model.training:
            warnings.warn(
                &#34;Up to version 1.0.1, the _prepare_tokens_and_attributes was evaluated with `torch.no_grad()`. &#34;
                &#34;This is inconsistent with how model were trained in the MusicGen paper. We removed the &#34;
                &#34;`torch.no_grad()` in version 1.1.0. Small changes to the final performance are expected. &#34;
                &#34;Really sorry about that.&#34;)
        if self._cached_batch_loader is None or self.current_stage != &#34;train&#34;:
            audio, infos = batch
            audio = audio.to(self.device)
            audio_tokens = None
            assert audio.size(0) == len(infos), (
                f&#34;Mismatch between number of items in audio batch ({audio.size(0)})&#34;,
                f&#34; and in metadata ({len(infos)})&#34;
            )
        else:
            audio = None
            # In that case the batch will be a tuple coming from the _cached_batch_writer bit below.
            infos, = batch  # type: ignore
            assert all([isinstance(info, AudioInfo) for info in infos])
            assert all([info.audio_tokens is not None for info in infos])  # type: ignore
            audio_tokens = torch.stack([info.audio_tokens for info in infos]).to(self.device)  # type: ignore
            audio_tokens = audio_tokens.long()
            for info in infos:
                if isinstance(info, MusicInfo):
                    # Careful here, if you want to use this condition_wav (e.b. chroma conditioning),
                    # then you must be using the chroma cache! otherwise the code will try
                    # to use this segment and fail (by that I mean you will see NaN everywhere).
                    info.self_wav = WavCondition(
                        torch.full([1, info.channels, info.total_frames], float(&#39;NaN&#39;)),
                        length=torch.tensor([info.n_frames]),
                        sample_rate=[info.sample_rate],
                        path=[info.meta.path],
                        seek_time=[info.seek_time])
                    dataset = get_dataset_from_loader(self.dataloaders[&#39;original_train&#39;])
                    assert isinstance(dataset, MusicDataset), type(dataset)
                    if dataset.paraphraser is not None and info.description is not None:
                        # Hackingly reapplying paraphraser when using cache.
                        info.description = dataset.paraphraser.sample_paraphrase(
                            info.meta.path, info.description)
        # prepare attributes
        attributes = [info.to_condition_attributes() for info in infos]
        attributes = self.model.cfg_dropout(attributes)
        attributes = self.model.att_dropout(attributes)
        tokenized = self.model.condition_provider.tokenize(attributes)

        # Now we should be synchronization free.
        if self.device == &#34;cuda&#34; and check_synchronization_points:
            torch.cuda.set_sync_debug_mode(&#34;warn&#34;)

        if audio_tokens is None:
            with torch.no_grad():
                audio_tokens, scale = self.compression_model.encode(audio)
                assert scale is None, &#34;Scaled compression model not supported with LM.&#34;

        with self.autocast:
            condition_tensors = self.model.condition_provider(tokenized)

        # create a padding mask to hold valid vs invalid positions
        padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)
        # replace encodec tokens from padded audio with special_token_id
        if self.cfg.tokens.padding_with_special_token:
            audio_tokens = audio_tokens.clone()
            padding_mask = padding_mask.clone()
            token_sample_rate = self.compression_model.frame_rate
            B, K, T_s = audio_tokens.shape
            for i in range(B):
                n_samples = infos[i].n_frames
                audio_sample_rate = infos[i].sample_rate
                # take the last token generated from actual audio frames (non-padded audio)
                valid_tokens = math.floor(float(n_samples) / audio_sample_rate * token_sample_rate)
                audio_tokens[i, :, valid_tokens:] = self.model.special_token_id
                padding_mask[i, :, valid_tokens:] = 0

        if self.device == &#34;cuda&#34; and check_synchronization_points:
            torch.cuda.set_sync_debug_mode(&#34;default&#34;)

        if self._cached_batch_writer is not None and self.current_stage == &#39;train&#39;:
            assert self._cached_batch_loader is None
            assert audio_tokens is not None
            for info, one_audio_tokens in zip(infos, audio_tokens):
                assert isinstance(info, AudioInfo)
                if isinstance(info, MusicInfo):
                    assert not info.joint_embed, &#34;joint_embed and cache not supported yet.&#34;
                    info.self_wav = None
                assert one_audio_tokens.max() &lt; 2**15, one_audio_tokens.max().item()
                info.audio_tokens = one_audio_tokens.short().cpu()
            self._cached_batch_writer.save(infos)

        return condition_tensors, audio_tokens, padding_mask

    def run_step(self, idx: int, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]], metrics: dict) -&gt; dict:
        &#34;&#34;&#34;Perform one training or valid step on a given batch.&#34;&#34;&#34;
        check_synchronization_points = idx == 1 and self.device == &#39;cuda&#39;

        condition_tensors, audio_tokens, padding_mask = self._prepare_tokens_and_attributes(
            batch, check_synchronization_points)

        self.deadlock_detect.update(&#39;tokens_and_conditions&#39;)

        if check_synchronization_points:
            torch.cuda.set_sync_debug_mode(&#39;warn&#39;)

        with self.autocast:
            model_output = self.model.compute_predictions(audio_tokens, [], condition_tensors)  # type: ignore
            logits = model_output.logits
            mask = padding_mask &amp; model_output.mask
            ce, ce_per_codebook = self._compute_cross_entropy(logits, audio_tokens, mask)
            loss = ce
        self.deadlock_detect.update(&#39;loss&#39;)

        if check_synchronization_points:
            torch.cuda.set_sync_debug_mode(&#39;default&#39;)

        if self.is_training:
            metrics[&#39;lr&#39;] = self.optimizer.param_groups[0][&#39;lr&#39;]
            if self.scaler is not None:
                loss = self.scaler.scale(loss)
            self.deadlock_detect.update(&#39;scale&#39;)
            if self.cfg.fsdp.use:
                loss.backward()
                flashy.distrib.average_tensors(self.model.buffers())
            elif self.cfg.optim.eager_sync:
                with flashy.distrib.eager_sync_model(self.model):
                    loss.backward()
            else:
                # this should always be slower but can be useful
                # for weird use cases like multiple backwards.
                loss.backward()
                flashy.distrib.sync_model(self.model)
            self.deadlock_detect.update(&#39;backward&#39;)

            if self.scaler is not None:
                self.scaler.unscale_(self.optimizer)
            if self.cfg.optim.max_norm:
                if self.cfg.fsdp.use:
                    metrics[&#39;grad_norm&#39;] = self.model.clip_grad_norm_(self.cfg.optim.max_norm)  # type: ignore
                else:
                    metrics[&#39;grad_norm&#39;] = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), self.cfg.optim.max_norm
                    )
            if self.scaler is None:
                self.optimizer.step()
            else:
                self.scaler.step(self.optimizer)
                self.scaler.update()
            if self.lr_scheduler:
                self.lr_scheduler.step()
            self.optimizer.zero_grad()
            self.deadlock_detect.update(&#39;optim&#39;)
            if self.scaler is not None:
                scale = self.scaler.get_scale()
                metrics[&#39;grad_scale&#39;] = scale
            if not loss.isfinite().all():
                raise RuntimeError(&#34;Model probably diverged.&#34;)

        metrics[&#39;ce&#39;] = ce
        metrics[&#39;ppl&#39;] = torch.exp(ce)
        for k, ce_q in enumerate(ce_per_codebook):
            metrics[f&#39;ce_q{k + 1}&#39;] = ce_q
            metrics[f&#39;ppl_q{k + 1}&#39;] = torch.exp(ce_q)

        return metrics

    @torch.no_grad()
    def run_generate_step(self, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]],
                          gen_duration: float, prompt_duration: tp.Optional[float] = None,
                          remove_prompt: bool = False,
                          **generation_params) -&gt; dict:
        &#34;&#34;&#34;Run generate step on a batch of optional audio tensor and corresponding attributes.

        Args:
            batch (tuple[torch.Tensor, list[SegmentWithAttributes]]):
            use_prompt (bool): Whether to do audio continuation generation with prompt from audio batch.
            gen_duration (float): Target audio duration for the generation.
            prompt_duration (float, optional): Duration for the audio prompt to use for continuation.
            remove_prompt (bool, optional): Whether to remove the prompt from the generated audio.
            generation_params: Additional generation parameters.
        Returns:
            gen_outputs (dict): Generation outputs, consisting in audio, audio tokens from both the generation
                and the prompt along with additional information.
        &#34;&#34;&#34;
        bench_start = time.time()
        audio, meta = batch
        assert audio.size(0) == len(meta), (
            f&#34;Mismatch between number of items in audio batch ({audio.size(0)})&#34;,
            f&#34; and in metadata ({len(meta)})&#34;
        )
        # prepare attributes
        attributes = [x.to_condition_attributes() for x in meta]
        # TODO: Add dropout for chroma?

        # prepare audio prompt
        if prompt_duration is None:
            prompt_audio = None
        else:
            assert prompt_duration &lt; gen_duration, &#34;Prompt duration must be lower than target generation duration&#34;
            prompt_audio_frames = int(prompt_duration * self.compression_model.sample_rate)
            prompt_audio = audio[..., :prompt_audio_frames]

        # get audio tokens from compression model
        if prompt_audio is None or prompt_audio.nelement() == 0:
            num_samples = len(attributes)
            prompt_tokens = None
        else:
            num_samples = None
            prompt_audio = prompt_audio.to(self.device)
            prompt_tokens, scale = self.compression_model.encode(prompt_audio)
            assert scale is None, &#34;Compression model in MusicGen should not require rescaling.&#34;

        # generate by sampling from the LM
        with self.autocast:
            total_gen_len = math.ceil(gen_duration * self.compression_model.frame_rate)
            gen_tokens = self.model.generate(
                prompt_tokens, attributes, max_gen_len=total_gen_len,
                num_samples=num_samples, **self.generation_params)

        # generate audio from tokens
        assert gen_tokens.dim() == 3
        gen_audio = self.compression_model.decode(gen_tokens, None)

        bench_end = time.time()
        gen_outputs = {
            &#39;rtf&#39;: (bench_end - bench_start) / gen_duration,
            &#39;ref_audio&#39;: audio,
            &#39;gen_audio&#39;: gen_audio,
            &#39;gen_tokens&#39;: gen_tokens,
            &#39;prompt_audio&#39;: prompt_audio,
            &#39;prompt_tokens&#39;: prompt_tokens,
        }
        return gen_outputs

    def generate_audio(self) -&gt; dict:
        &#34;&#34;&#34;Audio generation stage.&#34;&#34;&#34;
        generate_stage_name = f&#39;{self.current_stage}&#39;
        sample_manager = SampleManager(self.xp)
        self.logger.info(f&#34;Generating samples in {sample_manager.base_folder}&#34;)
        loader = self.dataloaders[&#39;generate&#39;]
        updates = len(loader)
        lp = self.log_progress(generate_stage_name, loader, total=updates, updates=self.log_updates)

        dataset = get_dataset_from_loader(loader)
        dataset_duration = dataset.segment_duration
        assert dataset_duration is not None
        assert isinstance(dataset, AudioDataset)
        target_duration = self.cfg.generate.lm.gen_duration
        prompt_duration = self.cfg.generate.lm.prompt_duration
        if target_duration is None:
            target_duration = dataset_duration
        if prompt_duration is None:
            prompt_duration = dataset_duration / 4
        assert prompt_duration &lt; dataset_duration, (
            f&#34;Specified prompt duration ({prompt_duration}s) is longer&#34;,
            f&#34; than reference audio duration ({dataset_duration}s)&#34;
        )

        def get_hydrated_conditions(meta: tp.List[SegmentWithAttributes]):
            hydrated_conditions = []
            for sample in [x.to_condition_attributes() for x in meta]:
                cond_dict = {}
                for cond_type in sample.__annotations__.keys():
                    for cond_key, cond_val in getattr(sample, cond_type).items():
                        if cond_key not in self.model.condition_provider.conditioners.keys():
                            continue
                        if is_jsonable(cond_val):
                            cond_dict[cond_key] = cond_val
                        elif isinstance(cond_val, WavCondition):
                            cond_dict[cond_key] = cond_val.path
                        elif isinstance(cond_val, JointEmbedCondition):
                            cond_dict[cond_key] = cond_val.text  # only support text at inference for now
                        else:
                            # if we reached this point, it is not clear how to log the condition
                            # so we just log the type.
                            cond_dict[cond_key] = str(type(cond_val))
                            continue
                hydrated_conditions.append(cond_dict)
            return hydrated_conditions

        metrics: dict = {}
        average = flashy.averager()
        for batch in lp:
            audio, meta = batch
            # metadata for sample manager
            hydrated_conditions = get_hydrated_conditions(meta)
            sample_generation_params = {
                **{f&#39;classifier_free_guidance_{k}&#39;: v for k, v in self.cfg.classifier_free_guidance.items()},
                **self.generation_params
            }
            if self.cfg.generate.lm.unprompted_samples:
                if self.cfg.generate.lm.gen_gt_samples:
                    # get the ground truth instead of generation
                    self.logger.warn(
                        &#34;Use ground truth instead of audio generation as generate.lm.gen_gt_samples=true&#34;)
                    gen_unprompted_audio = audio
                    rtf = 1.
                else:
                    gen_unprompted_outputs = self.run_generate_step(
                        batch, gen_duration=target_duration, prompt_duration=None,
                        **self.generation_params)
                    gen_unprompted_audio = gen_unprompted_outputs[&#39;gen_audio&#39;].cpu()
                    rtf = gen_unprompted_outputs[&#39;rtf&#39;]
                sample_manager.add_samples(
                    gen_unprompted_audio, self.epoch, hydrated_conditions,
                    ground_truth_wavs=audio, generation_args=sample_generation_params)

            if self.cfg.generate.lm.prompted_samples:
                gen_outputs = self.run_generate_step(
                    batch, gen_duration=target_duration, prompt_duration=prompt_duration,
                    **self.generation_params)
                gen_audio = gen_outputs[&#39;gen_audio&#39;].cpu()
                prompt_audio = gen_outputs[&#39;prompt_audio&#39;].cpu()
                sample_manager.add_samples(
                    gen_audio, self.epoch, hydrated_conditions,
                    prompt_wavs=prompt_audio, ground_truth_wavs=audio,
                    generation_args=sample_generation_params)

            metrics[&#39;rtf&#39;] = rtf
            metrics = average(metrics)

        flashy.distrib.barrier()
        return metrics

    def generate(self) -&gt; dict:
        &#34;&#34;&#34;Generate stage.&#34;&#34;&#34;
        self.model.eval()
        with torch.no_grad():
            return self.generate_audio()

    def run_epoch(self):
        if self.cfg.cache.write:
            if ((self.epoch - 1) % self.cfg.cache.write_num_shards) != self.cfg.cache.write_shard:
                return
        super().run_epoch()

    def train(self):
        &#34;&#34;&#34;Train stage.
        &#34;&#34;&#34;
        if self._cached_batch_writer is not None:
            self._cached_batch_writer.start_epoch(self.epoch)
        if self._cached_batch_loader is None:
            dataset = get_dataset_from_loader(self.dataloaders[&#39;train&#39;])
            assert isinstance(dataset, AudioDataset)
            dataset.current_epoch = self.epoch
        else:
            self._cached_batch_loader.start_epoch(self.epoch)
        return super().train()

    def evaluate_audio_generation(self) -&gt; dict:
        &#34;&#34;&#34;Evaluate audio generation with off-the-shelf metrics.&#34;&#34;&#34;
        evaluate_stage_name = f&#39;{self.current_stage}_generation&#39;
        # instantiate evaluation metrics, if at least one metric is defined, run audio generation evaluation
        fad: tp.Optional[eval_metrics.FrechetAudioDistanceMetric] = None
        kldiv: tp.Optional[eval_metrics.KLDivergenceMetric] = None
        text_consistency: tp.Optional[eval_metrics.TextConsistencyMetric] = None
        chroma_cosine: tp.Optional[eval_metrics.ChromaCosineSimilarityMetric] = None
        should_run_eval = False
        eval_chroma_wavs: tp.Optional[torch.Tensor] = None
        if self.cfg.evaluate.metrics.fad:
            fad = builders.get_fad(self.cfg.metrics.fad).to(self.device)
            should_run_eval = True
        if self.cfg.evaluate.metrics.kld:
            kldiv = builders.get_kldiv(self.cfg.metrics.kld).to(self.device)
            should_run_eval = True
        if self.cfg.evaluate.metrics.text_consistency:
            text_consistency = builders.get_text_consistency(self.cfg.metrics.text_consistency).to(self.device)
            should_run_eval = True
        if self.cfg.evaluate.metrics.chroma_cosine:
            chroma_cosine = builders.get_chroma_cosine_similarity(self.cfg.metrics.chroma_cosine).to(self.device)
            # if we have predefind wavs for chroma we should purge them for computing the cosine metric
            has_predefined_eval_chromas = &#39;self_wav&#39; in self.model.condition_provider.conditioners and \
                                          self.model.condition_provider.conditioners[&#39;self_wav&#39;].has_eval_wavs()
            if has_predefined_eval_chromas:
                warn_once(self.logger, &#34;Attempting to run cosine eval for config with pre-defined eval chromas! &#34;
                                       &#39;Resetting eval chromas to None for evaluation.&#39;)
                eval_chroma_wavs = self.model.condition_provider.conditioners.self_wav.eval_wavs  # type: ignore
                self.model.condition_provider.conditioners.self_wav.reset_eval_wavs(None)  # type: ignore
            should_run_eval = True

        def get_compressed_audio(audio: torch.Tensor) -&gt; torch.Tensor:
            audio_tokens, scale = self.compression_model.encode(audio.to(self.device))
            compressed_audio = self.compression_model.decode(audio_tokens, scale)
            return compressed_audio[..., :audio.shape[-1]]

        metrics: dict = {}
        if should_run_eval:
            loader = self.dataloaders[&#39;evaluate&#39;]
            updates = len(loader)
            lp = self.log_progress(f&#39;{evaluate_stage_name} inference&#39;, loader, total=updates, updates=self.log_updates)
            average = flashy.averager()
            dataset = get_dataset_from_loader(loader)
            assert isinstance(dataset, AudioDataset)
            self.logger.info(f&#34;Computing evaluation metrics on {len(dataset)} samples&#34;)

            for idx, batch in enumerate(lp):
                audio, meta = batch
                assert all([self.cfg.sample_rate == m.sample_rate for m in meta])

                target_duration = audio.shape[-1] / self.cfg.sample_rate
                if self.cfg.evaluate.fixed_generation_duration:
                    target_duration = self.cfg.evaluate.fixed_generation_duration

                gen_outputs = self.run_generate_step(
                    batch, gen_duration=target_duration,
                    **self.generation_params
                )
                y_pred = gen_outputs[&#39;gen_audio&#39;].detach()
                y_pred = y_pred[..., :audio.shape[-1]]

                normalize_kwargs = dict(self.cfg.generate.audio)
                normalize_kwargs.pop(&#39;format&#39;, None)
                y_pred = torch.stack([normalize_audio(w, **normalize_kwargs) for w in y_pred], dim=0).cpu()
                y = audio.cpu()  # should already be on CPU but just in case
                sizes = torch.tensor([m.n_frames for m in meta])  # actual sizes without padding
                sample_rates = torch.tensor([m.sample_rate for m in meta])  # sample rates for audio samples
                audio_stems = [Path(m.meta.path).stem + f&#34;_{m.seek_time}&#34; for m in meta]

                if fad is not None:
                    if self.cfg.metrics.fad.use_gt:
                        y_pred = get_compressed_audio(y).cpu()
                    fad.update(y_pred, y, sizes, sample_rates, audio_stems)
                if kldiv is not None:
                    if self.cfg.metrics.kld.use_gt:
                        y_pred = get_compressed_audio(y).cpu()
                    kldiv.update(y_pred, y, sizes, sample_rates)
                if text_consistency is not None:
                    texts = [m.description for m in meta]
                    if self.cfg.metrics.text_consistency.use_gt:
                        y_pred = y
                    text_consistency.update(y_pred, texts, sizes, sample_rates)
                if chroma_cosine is not None:
                    if self.cfg.metrics.chroma_cosine.use_gt:
                        y_pred = get_compressed_audio(y).cpu()
                    chroma_cosine.update(y_pred, y, sizes, sample_rates)
                    # restore chroma conditioner&#39;s eval chroma wavs
                    if eval_chroma_wavs is not None:
                        self.model.condition_provider.conditioners[&#39;self_wav&#39;].reset_eval_wavs(eval_chroma_wavs)

            flashy.distrib.barrier()
            if fad is not None:
                metrics[&#39;fad&#39;] = fad.compute()
            if kldiv is not None:
                kld_metrics = kldiv.compute()
                metrics.update(kld_metrics)
            if text_consistency is not None:
                metrics[&#39;text_consistency&#39;] = text_consistency.compute()
            if chroma_cosine is not None:
                metrics[&#39;chroma_cosine&#39;] = chroma_cosine.compute()
            metrics = average(metrics)
            metrics = flashy.distrib.average_metrics(metrics, len(loader))

        return metrics

    def evaluate(self) -&gt; dict:
        &#34;&#34;&#34;Evaluate stage.&#34;&#34;&#34;
        self.model.eval()
        with torch.no_grad():
            metrics: dict = {}
            if self.cfg.evaluate.metrics.base:
                metrics.update(self.common_train_valid(&#39;evaluate&#39;))
            gen_metrics = self.evaluate_audio_generation()
            return {**metrics, **gen_metrics}</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver"><code class="flex name class">
<span>class <span class="ident">MusicGenSolver</span></span>
<span>(</span><span>cfg: omegaconf.dictconfig.DictConfig)</span>
</code></dt>
<dd>
<div class="desc"><p>Solver for MusicGen training task.</p>
<p>Used in: <a href="https://arxiv.org/abs/2306.05284">https://arxiv.org/abs/2306.05284</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MusicGenSolver(base.StandardSolver):
    &#34;&#34;&#34;Solver for MusicGen training task.

    Used in: https://arxiv.org/abs/2306.05284
    &#34;&#34;&#34;
    DATASET_TYPE: builders.DatasetType = builders.DatasetType.MUSIC

    def __init__(self, cfg: omegaconf.DictConfig):
        super().__init__(cfg)
        # easier access to sampling parameters
        self.generation_params = {
            &#39;use_sampling&#39;: self.cfg.generate.lm.use_sampling,
            &#39;temp&#39;: self.cfg.generate.lm.temp,
            &#39;top_k&#39;: self.cfg.generate.lm.top_k,
            &#39;top_p&#39;: self.cfg.generate.lm.top_p,
        }
        self._best_metric_name: tp.Optional[str] = &#39;ce&#39;

        self._cached_batch_writer = None
        self._cached_batch_loader = None
        if cfg.cache.path:
            if cfg.cache.write:
                self._cached_batch_writer = CachedBatchWriter(Path(cfg.cache.path))
                if self.cfg.cache.write_num_shards:
                    self.logger.warning(&#34;Multiple shard cache, best_metric_name will be set to None.&#34;)
                    self._best_metric_name = None
            else:
                self._cached_batch_loader = CachedBatchLoader(
                    Path(cfg.cache.path), cfg.dataset.batch_size, cfg.dataset.num_workers,
                    min_length=self.cfg.optim.updates_per_epoch or 1)
                self.dataloaders[&#39;original_train&#39;] = self.dataloaders[&#39;train&#39;]
                self.dataloaders[&#39;train&#39;] = self._cached_batch_loader  # type: ignore

    @staticmethod
    def get_eval_solver_from_sig(sig: str, dtype: tp.Optional[str] = None,
                                 device: tp.Optional[str] = None, autocast: bool = True,
                                 batch_size: tp.Optional[int] = None,
                                 override_cfg: tp.Optional[tp.Union[dict, omegaconf.DictConfig]] = None,
                                 **kwargs):
        &#34;&#34;&#34;Mostly a convenience function around magma.train.get_solver_from_sig,
        populating all the proper param, deactivating EMA, FSDP, loading the best state,
        basically all you need to get a solver ready to &#34;play&#34; with in single GPU mode
        and with minimal memory overhead.

        Args:
            sig (str): signature to load.
            dtype (str or None): potential dtype, as a string, i.e. &#39;float16&#39;.
            device (str or None): potential device, as a string, i.e. &#39;cuda&#39;.
            override_cfg (dict or omegaconf.DictConfig or None): potential device, as a string, i.e. &#39;cuda&#39;.
        &#34;&#34;&#34;
        from audiocraft import train
        our_override_cfg: tp.Dict[str, tp.Any] = {&#39;optim&#39;: {&#39;ema&#39;: {&#39;use&#39;: False}}}
        our_override_cfg[&#39;autocast&#39;] = autocast
        if dtype is not None:
            our_override_cfg[&#39;dtype&#39;] = dtype
        if device is not None:
            our_override_cfg[&#39;device&#39;] = device
        if batch_size is not None:
            our_override_cfg[&#39;dataset&#39;] = {&#39;batch_size&#39;: batch_size}
        if override_cfg is None:
            override_cfg = {}
        override_cfg = omegaconf.OmegaConf.merge(
            omegaconf.DictConfig(override_cfg), omegaconf.DictConfig(our_override_cfg))  # type: ignore
        solver = train.get_solver_from_sig(
            sig, override_cfg=override_cfg,
            load_best=True, disable_fsdp=True,
            ignore_state_keys=[&#39;optimizer&#39;, &#39;ema&#39;], **kwargs)
        solver.model.eval()
        return solver

    def get_formatter(self, stage_name: str) -&gt; flashy.Formatter:
        return flashy.Formatter({
            &#39;lr&#39;: &#39;.2E&#39;,
            &#39;ce&#39;: &#39;.3f&#39;,
            &#39;ppl&#39;: &#39;.3f&#39;,
            &#39;grad_norm&#39;: &#39;.3E&#39;,
        }, exclude_keys=[&#39;ce_q*&#39;, &#39;ppl_q*&#39;])

    @property
    def best_metric_name(self) -&gt; tp.Optional[str]:
        return self._best_metric_name

    def build_model(self) -&gt; None:
        &#34;&#34;&#34;Instantiate models and optimizer.&#34;&#34;&#34;
        # we can potentially not use all quantizers with which the EnCodec model was trained
        # (e.g. we trained the model with quantizers dropout)
        self.compression_model = CompressionSolver.wrapped_model_from_checkpoint(
            self.cfg, self.cfg.compression_model_checkpoint, device=self.device)
        assert self.compression_model.sample_rate == self.cfg.sample_rate, (
            f&#34;Compression model sample rate is {self.compression_model.sample_rate} but &#34;
            f&#34;Solver sample rate is {self.cfg.sample_rate}.&#34;
            )
        # ensure we have matching configuration between LM and compression model
        assert self.cfg.transformer_lm.card == self.compression_model.cardinality, (
            &#34;Cardinalities of the LM and compression model don&#39;t match: &#34;,
            f&#34;LM cardinality is {self.cfg.transformer_lm.card} vs &#34;,
            f&#34;compression model cardinality is {self.compression_model.cardinality}&#34;
        )
        assert self.cfg.transformer_lm.n_q == self.compression_model.num_codebooks, (
            &#34;Numbers of codebooks of the LM and compression models don&#39;t match: &#34;,
            f&#34;LM number of codebooks is {self.cfg.transformer_lm.n_q} vs &#34;,
            f&#34;compression model numer of codebooks is {self.compression_model.num_codebooks}&#34;
        )
        self.logger.info(&#34;Compression model has %d codebooks with %d cardinality, and a framerate of %d&#34;,
                         self.compression_model.num_codebooks, self.compression_model.cardinality,
                         self.compression_model.frame_rate)
        # instantiate LM model
        self.model: models.LMModel = models.builders.get_lm_model(self.cfg).to(self.device)
        if self.cfg.fsdp.use:
            assert not self.cfg.autocast, &#34;Cannot use autocast with fsdp&#34;
            self.model = self.wrap_with_fsdp(self.model)
        self.register_ema(&#39;model&#39;)
        # initialize optimization
        self.optimizer = builders.get_optimizer(builders.get_optim_parameter_groups(self.model), self.cfg.optim)
        self.lr_scheduler = builders.get_lr_scheduler(self.optimizer, self.cfg.schedule, self.total_updates)
        self.register_stateful(&#39;model&#39;, &#39;optimizer&#39;, &#39;lr_scheduler&#39;)
        self.register_best_state(&#39;model&#39;)
        self.autocast_dtype = {
            &#39;float16&#39;: torch.float16, &#39;bfloat16&#39;: torch.bfloat16
        }[self.cfg.autocast_dtype]
        self.scaler: tp.Optional[torch.cuda.amp.GradScaler] = None
        if self.cfg.fsdp.use:
            need_scaler = self.cfg.fsdp.param_dtype == &#39;float16&#39;
        else:
            need_scaler = self.cfg.autocast and self.autocast_dtype is torch.float16
        if need_scaler:
            if self.cfg.fsdp.use:
                from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler
                self.scaler = ShardedGradScaler()  # type: ignore
            else:
                self.scaler = torch.cuda.amp.GradScaler()
            self.register_stateful(&#39;scaler&#39;)

    def build_dataloaders(self) -&gt; None:
        &#34;&#34;&#34;Instantiate audio dataloaders for each stage.&#34;&#34;&#34;
        self.dataloaders = builders.get_audio_datasets(self.cfg, dataset_type=self.DATASET_TYPE)

    def show(self) -&gt; None:
        &#34;&#34;&#34;Show the compression model and LM model.&#34;&#34;&#34;
        self.logger.info(&#34;Compression model:&#34;)
        self.log_model_summary(self.compression_model)
        self.logger.info(&#34;LM model:&#34;)
        self.log_model_summary(self.model)

    def load_state_dict(self, state: dict) -&gt; None:
        if &#39;condition_provider&#39; in state:
            model_state = state[&#39;model&#39;]
            condition_provider_state = state.pop(&#39;condition_provider&#39;)
            prefix = &#39;condition_provider.&#39;
            for key, value in condition_provider_state.items():
                key = prefix + key
                assert key not in model_state
                model_state[key] = value
        if &#39;compression_model&#39; in state:
            # We used to store the `compression_model` state in the checkpoint, however
            # this is in general not needed, as the compression model should always be readable
            # from the original `cfg.compression_model_checkpoint` location.
            compression_model_state = state.pop(&#39;compression_model&#39;)
            before_hash = model_hash(self.compression_model)
            self.compression_model.load_state_dict(compression_model_state)
            after_hash = model_hash(self.compression_model)
            if before_hash != after_hash:
                raise RuntimeError(
                    &#34;The compression model state inside the checkpoint is different&#34;
                    &#34; from the one obtained from compression_model_checkpoint...&#34;
                    &#34;We do not support altering the compression model inside the LM &#34;
                    &#34;checkpoint as parts of the code, in particular for running eval post-training &#34;
                    &#34;will use the compression_model_checkpoint as the source of truth.&#34;)

        super().load_state_dict(state)

    def load_from_pretrained(self, name: str):
        # TODO: support native HF versions of MusicGen.
        lm_pkg = models.loaders.load_lm_model_ckpt(name)
        state: dict = {
            &#39;best_state&#39;: {
                &#39;model&#39;: lm_pkg[&#39;best_state&#39;],
            },
        }
        return state

    def _compute_cross_entropy(
        self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor
    ) -&gt; tp.Tuple[torch.Tensor, tp.List[torch.Tensor]]:
        &#34;&#34;&#34;Compute cross entropy between multi-codebook targets and model&#39;s logits.
        The cross entropy is computed per codebook to provide codebook-level cross entropy.
        Valid timesteps for each of the codebook are pulled from the mask, where invalid
        timesteps are set to 0.

        Args:
            logits (torch.Tensor): Model&#39;s logits of shape [B, K, T, card].
            targets (torch.Tensor): Target codes, of shape [B, K, T].
            mask (torch.Tensor): Mask for valid target codes, of shape [B, K, T].
        Returns:
            ce (torch.Tensor): Cross entropy averaged over the codebooks
            ce_per_codebook (list of torch.Tensor): Cross entropy per codebook (detached).
        &#34;&#34;&#34;
        B, K, T = targets.shape
        assert logits.shape[:-1] == targets.shape
        assert mask.shape == targets.shape
        ce = torch.zeros([], device=targets.device)
        ce_per_codebook: tp.List[torch.Tensor] = []
        for k in range(K):
            logits_k = logits[:, k, ...].contiguous().view(-1, logits.size(-1))  # [B x T, card]
            targets_k = targets[:, k, ...].contiguous().view(-1)  # [B x T]
            mask_k = mask[:, k, ...].contiguous().view(-1)  # [B x T]
            ce_targets = targets_k[mask_k]
            ce_logits = logits_k[mask_k]
            q_ce = F.cross_entropy(ce_logits, ce_targets)
            ce += q_ce
            ce_per_codebook.append(q_ce.detach())
        # average cross entropy across codebooks
        ce = ce / K
        return ce, ce_per_codebook

    def _prepare_tokens_and_attributes(
        self, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]],
        check_synchronization_points: bool = False
    ) -&gt; tp.Tuple[dict, torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;Prepare input batchs for language model training.

        Args:
            batch (tuple[torch.Tensor, list[SegmentWithAttributes]]): Input batch with audio tensor of shape [B, C, T]
                and corresponding metadata as SegmentWithAttributes (with B items).
            check_synchronization_points (bool): Whether to check for synchronization points slowing down training.
        Returns:
            Condition tensors (dict[str, any]): Preprocessed condition attributes.
            Tokens (torch.Tensor): Audio tokens from compression model, of shape [B, K, T_s],
                with B the batch size, K the number of codebooks, T_s the token timesteps.
            Padding mask (torch.Tensor): Mask with valid positions in the tokens tensor, of shape [B, K, T_s].
        &#34;&#34;&#34;
        if self.model.training:
            warnings.warn(
                &#34;Up to version 1.0.1, the _prepare_tokens_and_attributes was evaluated with `torch.no_grad()`. &#34;
                &#34;This is inconsistent with how model were trained in the MusicGen paper. We removed the &#34;
                &#34;`torch.no_grad()` in version 1.1.0. Small changes to the final performance are expected. &#34;
                &#34;Really sorry about that.&#34;)
        if self._cached_batch_loader is None or self.current_stage != &#34;train&#34;:
            audio, infos = batch
            audio = audio.to(self.device)
            audio_tokens = None
            assert audio.size(0) == len(infos), (
                f&#34;Mismatch between number of items in audio batch ({audio.size(0)})&#34;,
                f&#34; and in metadata ({len(infos)})&#34;
            )
        else:
            audio = None
            # In that case the batch will be a tuple coming from the _cached_batch_writer bit below.
            infos, = batch  # type: ignore
            assert all([isinstance(info, AudioInfo) for info in infos])
            assert all([info.audio_tokens is not None for info in infos])  # type: ignore
            audio_tokens = torch.stack([info.audio_tokens for info in infos]).to(self.device)  # type: ignore
            audio_tokens = audio_tokens.long()
            for info in infos:
                if isinstance(info, MusicInfo):
                    # Careful here, if you want to use this condition_wav (e.b. chroma conditioning),
                    # then you must be using the chroma cache! otherwise the code will try
                    # to use this segment and fail (by that I mean you will see NaN everywhere).
                    info.self_wav = WavCondition(
                        torch.full([1, info.channels, info.total_frames], float(&#39;NaN&#39;)),
                        length=torch.tensor([info.n_frames]),
                        sample_rate=[info.sample_rate],
                        path=[info.meta.path],
                        seek_time=[info.seek_time])
                    dataset = get_dataset_from_loader(self.dataloaders[&#39;original_train&#39;])
                    assert isinstance(dataset, MusicDataset), type(dataset)
                    if dataset.paraphraser is not None and info.description is not None:
                        # Hackingly reapplying paraphraser when using cache.
                        info.description = dataset.paraphraser.sample_paraphrase(
                            info.meta.path, info.description)
        # prepare attributes
        attributes = [info.to_condition_attributes() for info in infos]
        attributes = self.model.cfg_dropout(attributes)
        attributes = self.model.att_dropout(attributes)
        tokenized = self.model.condition_provider.tokenize(attributes)

        # Now we should be synchronization free.
        if self.device == &#34;cuda&#34; and check_synchronization_points:
            torch.cuda.set_sync_debug_mode(&#34;warn&#34;)

        if audio_tokens is None:
            with torch.no_grad():
                audio_tokens, scale = self.compression_model.encode(audio)
                assert scale is None, &#34;Scaled compression model not supported with LM.&#34;

        with self.autocast:
            condition_tensors = self.model.condition_provider(tokenized)

        # create a padding mask to hold valid vs invalid positions
        padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)
        # replace encodec tokens from padded audio with special_token_id
        if self.cfg.tokens.padding_with_special_token:
            audio_tokens = audio_tokens.clone()
            padding_mask = padding_mask.clone()
            token_sample_rate = self.compression_model.frame_rate
            B, K, T_s = audio_tokens.shape
            for i in range(B):
                n_samples = infos[i].n_frames
                audio_sample_rate = infos[i].sample_rate
                # take the last token generated from actual audio frames (non-padded audio)
                valid_tokens = math.floor(float(n_samples) / audio_sample_rate * token_sample_rate)
                audio_tokens[i, :, valid_tokens:] = self.model.special_token_id
                padding_mask[i, :, valid_tokens:] = 0

        if self.device == &#34;cuda&#34; and check_synchronization_points:
            torch.cuda.set_sync_debug_mode(&#34;default&#34;)

        if self._cached_batch_writer is not None and self.current_stage == &#39;train&#39;:
            assert self._cached_batch_loader is None
            assert audio_tokens is not None
            for info, one_audio_tokens in zip(infos, audio_tokens):
                assert isinstance(info, AudioInfo)
                if isinstance(info, MusicInfo):
                    assert not info.joint_embed, &#34;joint_embed and cache not supported yet.&#34;
                    info.self_wav = None
                assert one_audio_tokens.max() &lt; 2**15, one_audio_tokens.max().item()
                info.audio_tokens = one_audio_tokens.short().cpu()
            self._cached_batch_writer.save(infos)

        return condition_tensors, audio_tokens, padding_mask

    def run_step(self, idx: int, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]], metrics: dict) -&gt; dict:
        &#34;&#34;&#34;Perform one training or valid step on a given batch.&#34;&#34;&#34;
        check_synchronization_points = idx == 1 and self.device == &#39;cuda&#39;

        condition_tensors, audio_tokens, padding_mask = self._prepare_tokens_and_attributes(
            batch, check_synchronization_points)

        self.deadlock_detect.update(&#39;tokens_and_conditions&#39;)

        if check_synchronization_points:
            torch.cuda.set_sync_debug_mode(&#39;warn&#39;)

        with self.autocast:
            model_output = self.model.compute_predictions(audio_tokens, [], condition_tensors)  # type: ignore
            logits = model_output.logits
            mask = padding_mask &amp; model_output.mask
            ce, ce_per_codebook = self._compute_cross_entropy(logits, audio_tokens, mask)
            loss = ce
        self.deadlock_detect.update(&#39;loss&#39;)

        if check_synchronization_points:
            torch.cuda.set_sync_debug_mode(&#39;default&#39;)

        if self.is_training:
            metrics[&#39;lr&#39;] = self.optimizer.param_groups[0][&#39;lr&#39;]
            if self.scaler is not None:
                loss = self.scaler.scale(loss)
            self.deadlock_detect.update(&#39;scale&#39;)
            if self.cfg.fsdp.use:
                loss.backward()
                flashy.distrib.average_tensors(self.model.buffers())
            elif self.cfg.optim.eager_sync:
                with flashy.distrib.eager_sync_model(self.model):
                    loss.backward()
            else:
                # this should always be slower but can be useful
                # for weird use cases like multiple backwards.
                loss.backward()
                flashy.distrib.sync_model(self.model)
            self.deadlock_detect.update(&#39;backward&#39;)

            if self.scaler is not None:
                self.scaler.unscale_(self.optimizer)
            if self.cfg.optim.max_norm:
                if self.cfg.fsdp.use:
                    metrics[&#39;grad_norm&#39;] = self.model.clip_grad_norm_(self.cfg.optim.max_norm)  # type: ignore
                else:
                    metrics[&#39;grad_norm&#39;] = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), self.cfg.optim.max_norm
                    )
            if self.scaler is None:
                self.optimizer.step()
            else:
                self.scaler.step(self.optimizer)
                self.scaler.update()
            if self.lr_scheduler:
                self.lr_scheduler.step()
            self.optimizer.zero_grad()
            self.deadlock_detect.update(&#39;optim&#39;)
            if self.scaler is not None:
                scale = self.scaler.get_scale()
                metrics[&#39;grad_scale&#39;] = scale
            if not loss.isfinite().all():
                raise RuntimeError(&#34;Model probably diverged.&#34;)

        metrics[&#39;ce&#39;] = ce
        metrics[&#39;ppl&#39;] = torch.exp(ce)
        for k, ce_q in enumerate(ce_per_codebook):
            metrics[f&#39;ce_q{k + 1}&#39;] = ce_q
            metrics[f&#39;ppl_q{k + 1}&#39;] = torch.exp(ce_q)

        return metrics

    @torch.no_grad()
    def run_generate_step(self, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]],
                          gen_duration: float, prompt_duration: tp.Optional[float] = None,
                          remove_prompt: bool = False,
                          **generation_params) -&gt; dict:
        &#34;&#34;&#34;Run generate step on a batch of optional audio tensor and corresponding attributes.

        Args:
            batch (tuple[torch.Tensor, list[SegmentWithAttributes]]):
            use_prompt (bool): Whether to do audio continuation generation with prompt from audio batch.
            gen_duration (float): Target audio duration for the generation.
            prompt_duration (float, optional): Duration for the audio prompt to use for continuation.
            remove_prompt (bool, optional): Whether to remove the prompt from the generated audio.
            generation_params: Additional generation parameters.
        Returns:
            gen_outputs (dict): Generation outputs, consisting in audio, audio tokens from both the generation
                and the prompt along with additional information.
        &#34;&#34;&#34;
        bench_start = time.time()
        audio, meta = batch
        assert audio.size(0) == len(meta), (
            f&#34;Mismatch between number of items in audio batch ({audio.size(0)})&#34;,
            f&#34; and in metadata ({len(meta)})&#34;
        )
        # prepare attributes
        attributes = [x.to_condition_attributes() for x in meta]
        # TODO: Add dropout for chroma?

        # prepare audio prompt
        if prompt_duration is None:
            prompt_audio = None
        else:
            assert prompt_duration &lt; gen_duration, &#34;Prompt duration must be lower than target generation duration&#34;
            prompt_audio_frames = int(prompt_duration * self.compression_model.sample_rate)
            prompt_audio = audio[..., :prompt_audio_frames]

        # get audio tokens from compression model
        if prompt_audio is None or prompt_audio.nelement() == 0:
            num_samples = len(attributes)
            prompt_tokens = None
        else:
            num_samples = None
            prompt_audio = prompt_audio.to(self.device)
            prompt_tokens, scale = self.compression_model.encode(prompt_audio)
            assert scale is None, &#34;Compression model in MusicGen should not require rescaling.&#34;

        # generate by sampling from the LM
        with self.autocast:
            total_gen_len = math.ceil(gen_duration * self.compression_model.frame_rate)
            gen_tokens = self.model.generate(
                prompt_tokens, attributes, max_gen_len=total_gen_len,
                num_samples=num_samples, **self.generation_params)

        # generate audio from tokens
        assert gen_tokens.dim() == 3
        gen_audio = self.compression_model.decode(gen_tokens, None)

        bench_end = time.time()
        gen_outputs = {
            &#39;rtf&#39;: (bench_end - bench_start) / gen_duration,
            &#39;ref_audio&#39;: audio,
            &#39;gen_audio&#39;: gen_audio,
            &#39;gen_tokens&#39;: gen_tokens,
            &#39;prompt_audio&#39;: prompt_audio,
            &#39;prompt_tokens&#39;: prompt_tokens,
        }
        return gen_outputs

    def generate_audio(self) -&gt; dict:
        &#34;&#34;&#34;Audio generation stage.&#34;&#34;&#34;
        generate_stage_name = f&#39;{self.current_stage}&#39;
        sample_manager = SampleManager(self.xp)
        self.logger.info(f&#34;Generating samples in {sample_manager.base_folder}&#34;)
        loader = self.dataloaders[&#39;generate&#39;]
        updates = len(loader)
        lp = self.log_progress(generate_stage_name, loader, total=updates, updates=self.log_updates)

        dataset = get_dataset_from_loader(loader)
        dataset_duration = dataset.segment_duration
        assert dataset_duration is not None
        assert isinstance(dataset, AudioDataset)
        target_duration = self.cfg.generate.lm.gen_duration
        prompt_duration = self.cfg.generate.lm.prompt_duration
        if target_duration is None:
            target_duration = dataset_duration
        if prompt_duration is None:
            prompt_duration = dataset_duration / 4
        assert prompt_duration &lt; dataset_duration, (
            f&#34;Specified prompt duration ({prompt_duration}s) is longer&#34;,
            f&#34; than reference audio duration ({dataset_duration}s)&#34;
        )

        def get_hydrated_conditions(meta: tp.List[SegmentWithAttributes]):
            hydrated_conditions = []
            for sample in [x.to_condition_attributes() for x in meta]:
                cond_dict = {}
                for cond_type in sample.__annotations__.keys():
                    for cond_key, cond_val in getattr(sample, cond_type).items():
                        if cond_key not in self.model.condition_provider.conditioners.keys():
                            continue
                        if is_jsonable(cond_val):
                            cond_dict[cond_key] = cond_val
                        elif isinstance(cond_val, WavCondition):
                            cond_dict[cond_key] = cond_val.path
                        elif isinstance(cond_val, JointEmbedCondition):
                            cond_dict[cond_key] = cond_val.text  # only support text at inference for now
                        else:
                            # if we reached this point, it is not clear how to log the condition
                            # so we just log the type.
                            cond_dict[cond_key] = str(type(cond_val))
                            continue
                hydrated_conditions.append(cond_dict)
            return hydrated_conditions

        metrics: dict = {}
        average = flashy.averager()
        for batch in lp:
            audio, meta = batch
            # metadata for sample manager
            hydrated_conditions = get_hydrated_conditions(meta)
            sample_generation_params = {
                **{f&#39;classifier_free_guidance_{k}&#39;: v for k, v in self.cfg.classifier_free_guidance.items()},
                **self.generation_params
            }
            if self.cfg.generate.lm.unprompted_samples:
                if self.cfg.generate.lm.gen_gt_samples:
                    # get the ground truth instead of generation
                    self.logger.warn(
                        &#34;Use ground truth instead of audio generation as generate.lm.gen_gt_samples=true&#34;)
                    gen_unprompted_audio = audio
                    rtf = 1.
                else:
                    gen_unprompted_outputs = self.run_generate_step(
                        batch, gen_duration=target_duration, prompt_duration=None,
                        **self.generation_params)
                    gen_unprompted_audio = gen_unprompted_outputs[&#39;gen_audio&#39;].cpu()
                    rtf = gen_unprompted_outputs[&#39;rtf&#39;]
                sample_manager.add_samples(
                    gen_unprompted_audio, self.epoch, hydrated_conditions,
                    ground_truth_wavs=audio, generation_args=sample_generation_params)

            if self.cfg.generate.lm.prompted_samples:
                gen_outputs = self.run_generate_step(
                    batch, gen_duration=target_duration, prompt_duration=prompt_duration,
                    **self.generation_params)
                gen_audio = gen_outputs[&#39;gen_audio&#39;].cpu()
                prompt_audio = gen_outputs[&#39;prompt_audio&#39;].cpu()
                sample_manager.add_samples(
                    gen_audio, self.epoch, hydrated_conditions,
                    prompt_wavs=prompt_audio, ground_truth_wavs=audio,
                    generation_args=sample_generation_params)

            metrics[&#39;rtf&#39;] = rtf
            metrics = average(metrics)

        flashy.distrib.barrier()
        return metrics

    def generate(self) -&gt; dict:
        &#34;&#34;&#34;Generate stage.&#34;&#34;&#34;
        self.model.eval()
        with torch.no_grad():
            return self.generate_audio()

    def run_epoch(self):
        if self.cfg.cache.write:
            if ((self.epoch - 1) % self.cfg.cache.write_num_shards) != self.cfg.cache.write_shard:
                return
        super().run_epoch()

    def train(self):
        &#34;&#34;&#34;Train stage.
        &#34;&#34;&#34;
        if self._cached_batch_writer is not None:
            self._cached_batch_writer.start_epoch(self.epoch)
        if self._cached_batch_loader is None:
            dataset = get_dataset_from_loader(self.dataloaders[&#39;train&#39;])
            assert isinstance(dataset, AudioDataset)
            dataset.current_epoch = self.epoch
        else:
            self._cached_batch_loader.start_epoch(self.epoch)
        return super().train()

    def evaluate_audio_generation(self) -&gt; dict:
        &#34;&#34;&#34;Evaluate audio generation with off-the-shelf metrics.&#34;&#34;&#34;
        evaluate_stage_name = f&#39;{self.current_stage}_generation&#39;
        # instantiate evaluation metrics, if at least one metric is defined, run audio generation evaluation
        fad: tp.Optional[eval_metrics.FrechetAudioDistanceMetric] = None
        kldiv: tp.Optional[eval_metrics.KLDivergenceMetric] = None
        text_consistency: tp.Optional[eval_metrics.TextConsistencyMetric] = None
        chroma_cosine: tp.Optional[eval_metrics.ChromaCosineSimilarityMetric] = None
        should_run_eval = False
        eval_chroma_wavs: tp.Optional[torch.Tensor] = None
        if self.cfg.evaluate.metrics.fad:
            fad = builders.get_fad(self.cfg.metrics.fad).to(self.device)
            should_run_eval = True
        if self.cfg.evaluate.metrics.kld:
            kldiv = builders.get_kldiv(self.cfg.metrics.kld).to(self.device)
            should_run_eval = True
        if self.cfg.evaluate.metrics.text_consistency:
            text_consistency = builders.get_text_consistency(self.cfg.metrics.text_consistency).to(self.device)
            should_run_eval = True
        if self.cfg.evaluate.metrics.chroma_cosine:
            chroma_cosine = builders.get_chroma_cosine_similarity(self.cfg.metrics.chroma_cosine).to(self.device)
            # if we have predefind wavs for chroma we should purge them for computing the cosine metric
            has_predefined_eval_chromas = &#39;self_wav&#39; in self.model.condition_provider.conditioners and \
                                          self.model.condition_provider.conditioners[&#39;self_wav&#39;].has_eval_wavs()
            if has_predefined_eval_chromas:
                warn_once(self.logger, &#34;Attempting to run cosine eval for config with pre-defined eval chromas! &#34;
                                       &#39;Resetting eval chromas to None for evaluation.&#39;)
                eval_chroma_wavs = self.model.condition_provider.conditioners.self_wav.eval_wavs  # type: ignore
                self.model.condition_provider.conditioners.self_wav.reset_eval_wavs(None)  # type: ignore
            should_run_eval = True

        def get_compressed_audio(audio: torch.Tensor) -&gt; torch.Tensor:
            audio_tokens, scale = self.compression_model.encode(audio.to(self.device))
            compressed_audio = self.compression_model.decode(audio_tokens, scale)
            return compressed_audio[..., :audio.shape[-1]]

        metrics: dict = {}
        if should_run_eval:
            loader = self.dataloaders[&#39;evaluate&#39;]
            updates = len(loader)
            lp = self.log_progress(f&#39;{evaluate_stage_name} inference&#39;, loader, total=updates, updates=self.log_updates)
            average = flashy.averager()
            dataset = get_dataset_from_loader(loader)
            assert isinstance(dataset, AudioDataset)
            self.logger.info(f&#34;Computing evaluation metrics on {len(dataset)} samples&#34;)

            for idx, batch in enumerate(lp):
                audio, meta = batch
                assert all([self.cfg.sample_rate == m.sample_rate for m in meta])

                target_duration = audio.shape[-1] / self.cfg.sample_rate
                if self.cfg.evaluate.fixed_generation_duration:
                    target_duration = self.cfg.evaluate.fixed_generation_duration

                gen_outputs = self.run_generate_step(
                    batch, gen_duration=target_duration,
                    **self.generation_params
                )
                y_pred = gen_outputs[&#39;gen_audio&#39;].detach()
                y_pred = y_pred[..., :audio.shape[-1]]

                normalize_kwargs = dict(self.cfg.generate.audio)
                normalize_kwargs.pop(&#39;format&#39;, None)
                y_pred = torch.stack([normalize_audio(w, **normalize_kwargs) for w in y_pred], dim=0).cpu()
                y = audio.cpu()  # should already be on CPU but just in case
                sizes = torch.tensor([m.n_frames for m in meta])  # actual sizes without padding
                sample_rates = torch.tensor([m.sample_rate for m in meta])  # sample rates for audio samples
                audio_stems = [Path(m.meta.path).stem + f&#34;_{m.seek_time}&#34; for m in meta]

                if fad is not None:
                    if self.cfg.metrics.fad.use_gt:
                        y_pred = get_compressed_audio(y).cpu()
                    fad.update(y_pred, y, sizes, sample_rates, audio_stems)
                if kldiv is not None:
                    if self.cfg.metrics.kld.use_gt:
                        y_pred = get_compressed_audio(y).cpu()
                    kldiv.update(y_pred, y, sizes, sample_rates)
                if text_consistency is not None:
                    texts = [m.description for m in meta]
                    if self.cfg.metrics.text_consistency.use_gt:
                        y_pred = y
                    text_consistency.update(y_pred, texts, sizes, sample_rates)
                if chroma_cosine is not None:
                    if self.cfg.metrics.chroma_cosine.use_gt:
                        y_pred = get_compressed_audio(y).cpu()
                    chroma_cosine.update(y_pred, y, sizes, sample_rates)
                    # restore chroma conditioner&#39;s eval chroma wavs
                    if eval_chroma_wavs is not None:
                        self.model.condition_provider.conditioners[&#39;self_wav&#39;].reset_eval_wavs(eval_chroma_wavs)

            flashy.distrib.barrier()
            if fad is not None:
                metrics[&#39;fad&#39;] = fad.compute()
            if kldiv is not None:
                kld_metrics = kldiv.compute()
                metrics.update(kld_metrics)
            if text_consistency is not None:
                metrics[&#39;text_consistency&#39;] = text_consistency.compute()
            if chroma_cosine is not None:
                metrics[&#39;chroma_cosine&#39;] = chroma_cosine.compute()
            metrics = average(metrics)
            metrics = flashy.distrib.average_metrics(metrics, len(loader))

        return metrics

    def evaluate(self) -&gt; dict:
        &#34;&#34;&#34;Evaluate stage.&#34;&#34;&#34;
        self.model.eval()
        with torch.no_grad():
            metrics: dict = {}
            if self.cfg.evaluate.metrics.base:
                metrics.update(self.common_train_valid(&#39;evaluate&#39;))
            gen_metrics = self.evaluate_audio_generation()
            return {**metrics, **gen_metrics}</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="audiocraft.solvers.base.StandardSolver" href="base.html#audiocraft.solvers.base.StandardSolver">StandardSolver</a></li>
<li>abc.ABC</li>
<li>flashy.solver.BaseSolver</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="audiocraft.solvers.audiogen.AudioGenSolver" href="audiogen.html#audiocraft.solvers.audiogen.AudioGenSolver">AudioGenSolver</a></li>
<li><a title="audiocraft.solvers.magnet.MagnetSolver" href="magnet.html#audiocraft.solvers.magnet.MagnetSolver">MagnetSolver</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.DATASET_TYPE"><code class="name">var <span class="ident">DATASET_TYPE</span> : <a title="audiocraft.solvers.builders.DatasetType" href="builders.html#audiocraft.solvers.builders.DatasetType">DatasetType</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.get_eval_solver_from_sig"><code class="name flex">
<span>def <span class="ident">get_eval_solver_from_sig</span></span>(<span>sig: str, dtype: Optional[str] = None, device: Optional[str] = None, autocast: bool = True, batch_size: Optional[int] = None, override_cfg: Union[dict, omegaconf.dictconfig.DictConfig, None] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Mostly a convenience function around magma.train.get_solver_from_sig,
populating all the proper param, deactivating EMA, FSDP, loading the best state,
basically all you need to get a solver ready to "play" with in single GPU mode
and with minimal memory overhead.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sig</code></strong> :&ensp;<code>str</code></dt>
<dd>signature to load.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>potential dtype, as a string, i.e. 'float16'.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>potential device, as a string, i.e. 'cuda'.</dd>
<dt><strong><code>override_cfg</code></strong> :&ensp;<code>dict</code> or <code>omegaconf.DictConfig</code> or <code>None</code></dt>
<dd>potential device, as a string, i.e. 'cuda'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_eval_solver_from_sig(sig: str, dtype: tp.Optional[str] = None,
                             device: tp.Optional[str] = None, autocast: bool = True,
                             batch_size: tp.Optional[int] = None,
                             override_cfg: tp.Optional[tp.Union[dict, omegaconf.DictConfig]] = None,
                             **kwargs):
    &#34;&#34;&#34;Mostly a convenience function around magma.train.get_solver_from_sig,
    populating all the proper param, deactivating EMA, FSDP, loading the best state,
    basically all you need to get a solver ready to &#34;play&#34; with in single GPU mode
    and with minimal memory overhead.

    Args:
        sig (str): signature to load.
        dtype (str or None): potential dtype, as a string, i.e. &#39;float16&#39;.
        device (str or None): potential device, as a string, i.e. &#39;cuda&#39;.
        override_cfg (dict or omegaconf.DictConfig or None): potential device, as a string, i.e. &#39;cuda&#39;.
    &#34;&#34;&#34;
    from audiocraft import train
    our_override_cfg: tp.Dict[str, tp.Any] = {&#39;optim&#39;: {&#39;ema&#39;: {&#39;use&#39;: False}}}
    our_override_cfg[&#39;autocast&#39;] = autocast
    if dtype is not None:
        our_override_cfg[&#39;dtype&#39;] = dtype
    if device is not None:
        our_override_cfg[&#39;device&#39;] = device
    if batch_size is not None:
        our_override_cfg[&#39;dataset&#39;] = {&#39;batch_size&#39;: batch_size}
    if override_cfg is None:
        override_cfg = {}
    override_cfg = omegaconf.OmegaConf.merge(
        omegaconf.DictConfig(override_cfg), omegaconf.DictConfig(our_override_cfg))  # type: ignore
    solver = train.get_solver_from_sig(
        sig, override_cfg=override_cfg,
        load_best=True, disable_fsdp=True,
        ignore_state_keys=[&#39;optimizer&#39;, &#39;ema&#39;], **kwargs)
    solver.model.eval()
    return solver</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.build_dataloaders"><code class="name flex">
<span>def <span class="ident">build_dataloaders</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Instantiate audio dataloaders for each stage.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_dataloaders(self) -&gt; None:
    &#34;&#34;&#34;Instantiate audio dataloaders for each stage.&#34;&#34;&#34;
    self.dataloaders = builders.get_audio_datasets(self.cfg, dataset_type=self.DATASET_TYPE)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.build_model"><code class="name flex">
<span>def <span class="ident">build_model</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Instantiate models and optimizer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_model(self) -&gt; None:
    &#34;&#34;&#34;Instantiate models and optimizer.&#34;&#34;&#34;
    # we can potentially not use all quantizers with which the EnCodec model was trained
    # (e.g. we trained the model with quantizers dropout)
    self.compression_model = CompressionSolver.wrapped_model_from_checkpoint(
        self.cfg, self.cfg.compression_model_checkpoint, device=self.device)
    assert self.compression_model.sample_rate == self.cfg.sample_rate, (
        f&#34;Compression model sample rate is {self.compression_model.sample_rate} but &#34;
        f&#34;Solver sample rate is {self.cfg.sample_rate}.&#34;
        )
    # ensure we have matching configuration between LM and compression model
    assert self.cfg.transformer_lm.card == self.compression_model.cardinality, (
        &#34;Cardinalities of the LM and compression model don&#39;t match: &#34;,
        f&#34;LM cardinality is {self.cfg.transformer_lm.card} vs &#34;,
        f&#34;compression model cardinality is {self.compression_model.cardinality}&#34;
    )
    assert self.cfg.transformer_lm.n_q == self.compression_model.num_codebooks, (
        &#34;Numbers of codebooks of the LM and compression models don&#39;t match: &#34;,
        f&#34;LM number of codebooks is {self.cfg.transformer_lm.n_q} vs &#34;,
        f&#34;compression model numer of codebooks is {self.compression_model.num_codebooks}&#34;
    )
    self.logger.info(&#34;Compression model has %d codebooks with %d cardinality, and a framerate of %d&#34;,
                     self.compression_model.num_codebooks, self.compression_model.cardinality,
                     self.compression_model.frame_rate)
    # instantiate LM model
    self.model: models.LMModel = models.builders.get_lm_model(self.cfg).to(self.device)
    if self.cfg.fsdp.use:
        assert not self.cfg.autocast, &#34;Cannot use autocast with fsdp&#34;
        self.model = self.wrap_with_fsdp(self.model)
    self.register_ema(&#39;model&#39;)
    # initialize optimization
    self.optimizer = builders.get_optimizer(builders.get_optim_parameter_groups(self.model), self.cfg.optim)
    self.lr_scheduler = builders.get_lr_scheduler(self.optimizer, self.cfg.schedule, self.total_updates)
    self.register_stateful(&#39;model&#39;, &#39;optimizer&#39;, &#39;lr_scheduler&#39;)
    self.register_best_state(&#39;model&#39;)
    self.autocast_dtype = {
        &#39;float16&#39;: torch.float16, &#39;bfloat16&#39;: torch.bfloat16
    }[self.cfg.autocast_dtype]
    self.scaler: tp.Optional[torch.cuda.amp.GradScaler] = None
    if self.cfg.fsdp.use:
        need_scaler = self.cfg.fsdp.param_dtype == &#39;float16&#39;
    else:
        need_scaler = self.cfg.autocast and self.autocast_dtype is torch.float16
    if need_scaler:
        if self.cfg.fsdp.use:
            from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler
            self.scaler = ShardedGradScaler()  # type: ignore
        else:
            self.scaler = torch.cuda.amp.GradScaler()
        self.register_stateful(&#39;scaler&#39;)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.evaluate_audio_generation"><code class="name flex">
<span>def <span class="ident">evaluate_audio_generation</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate audio generation with off-the-shelf metrics.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_audio_generation(self) -&gt; dict:
    &#34;&#34;&#34;Evaluate audio generation with off-the-shelf metrics.&#34;&#34;&#34;
    evaluate_stage_name = f&#39;{self.current_stage}_generation&#39;
    # instantiate evaluation metrics, if at least one metric is defined, run audio generation evaluation
    fad: tp.Optional[eval_metrics.FrechetAudioDistanceMetric] = None
    kldiv: tp.Optional[eval_metrics.KLDivergenceMetric] = None
    text_consistency: tp.Optional[eval_metrics.TextConsistencyMetric] = None
    chroma_cosine: tp.Optional[eval_metrics.ChromaCosineSimilarityMetric] = None
    should_run_eval = False
    eval_chroma_wavs: tp.Optional[torch.Tensor] = None
    if self.cfg.evaluate.metrics.fad:
        fad = builders.get_fad(self.cfg.metrics.fad).to(self.device)
        should_run_eval = True
    if self.cfg.evaluate.metrics.kld:
        kldiv = builders.get_kldiv(self.cfg.metrics.kld).to(self.device)
        should_run_eval = True
    if self.cfg.evaluate.metrics.text_consistency:
        text_consistency = builders.get_text_consistency(self.cfg.metrics.text_consistency).to(self.device)
        should_run_eval = True
    if self.cfg.evaluate.metrics.chroma_cosine:
        chroma_cosine = builders.get_chroma_cosine_similarity(self.cfg.metrics.chroma_cosine).to(self.device)
        # if we have predefind wavs for chroma we should purge them for computing the cosine metric
        has_predefined_eval_chromas = &#39;self_wav&#39; in self.model.condition_provider.conditioners and \
                                      self.model.condition_provider.conditioners[&#39;self_wav&#39;].has_eval_wavs()
        if has_predefined_eval_chromas:
            warn_once(self.logger, &#34;Attempting to run cosine eval for config with pre-defined eval chromas! &#34;
                                   &#39;Resetting eval chromas to None for evaluation.&#39;)
            eval_chroma_wavs = self.model.condition_provider.conditioners.self_wav.eval_wavs  # type: ignore
            self.model.condition_provider.conditioners.self_wav.reset_eval_wavs(None)  # type: ignore
        should_run_eval = True

    def get_compressed_audio(audio: torch.Tensor) -&gt; torch.Tensor:
        audio_tokens, scale = self.compression_model.encode(audio.to(self.device))
        compressed_audio = self.compression_model.decode(audio_tokens, scale)
        return compressed_audio[..., :audio.shape[-1]]

    metrics: dict = {}
    if should_run_eval:
        loader = self.dataloaders[&#39;evaluate&#39;]
        updates = len(loader)
        lp = self.log_progress(f&#39;{evaluate_stage_name} inference&#39;, loader, total=updates, updates=self.log_updates)
        average = flashy.averager()
        dataset = get_dataset_from_loader(loader)
        assert isinstance(dataset, AudioDataset)
        self.logger.info(f&#34;Computing evaluation metrics on {len(dataset)} samples&#34;)

        for idx, batch in enumerate(lp):
            audio, meta = batch
            assert all([self.cfg.sample_rate == m.sample_rate for m in meta])

            target_duration = audio.shape[-1] / self.cfg.sample_rate
            if self.cfg.evaluate.fixed_generation_duration:
                target_duration = self.cfg.evaluate.fixed_generation_duration

            gen_outputs = self.run_generate_step(
                batch, gen_duration=target_duration,
                **self.generation_params
            )
            y_pred = gen_outputs[&#39;gen_audio&#39;].detach()
            y_pred = y_pred[..., :audio.shape[-1]]

            normalize_kwargs = dict(self.cfg.generate.audio)
            normalize_kwargs.pop(&#39;format&#39;, None)
            y_pred = torch.stack([normalize_audio(w, **normalize_kwargs) for w in y_pred], dim=0).cpu()
            y = audio.cpu()  # should already be on CPU but just in case
            sizes = torch.tensor([m.n_frames for m in meta])  # actual sizes without padding
            sample_rates = torch.tensor([m.sample_rate for m in meta])  # sample rates for audio samples
            audio_stems = [Path(m.meta.path).stem + f&#34;_{m.seek_time}&#34; for m in meta]

            if fad is not None:
                if self.cfg.metrics.fad.use_gt:
                    y_pred = get_compressed_audio(y).cpu()
                fad.update(y_pred, y, sizes, sample_rates, audio_stems)
            if kldiv is not None:
                if self.cfg.metrics.kld.use_gt:
                    y_pred = get_compressed_audio(y).cpu()
                kldiv.update(y_pred, y, sizes, sample_rates)
            if text_consistency is not None:
                texts = [m.description for m in meta]
                if self.cfg.metrics.text_consistency.use_gt:
                    y_pred = y
                text_consistency.update(y_pred, texts, sizes, sample_rates)
            if chroma_cosine is not None:
                if self.cfg.metrics.chroma_cosine.use_gt:
                    y_pred = get_compressed_audio(y).cpu()
                chroma_cosine.update(y_pred, y, sizes, sample_rates)
                # restore chroma conditioner&#39;s eval chroma wavs
                if eval_chroma_wavs is not None:
                    self.model.condition_provider.conditioners[&#39;self_wav&#39;].reset_eval_wavs(eval_chroma_wavs)

        flashy.distrib.barrier()
        if fad is not None:
            metrics[&#39;fad&#39;] = fad.compute()
        if kldiv is not None:
            kld_metrics = kldiv.compute()
            metrics.update(kld_metrics)
        if text_consistency is not None:
            metrics[&#39;text_consistency&#39;] = text_consistency.compute()
        if chroma_cosine is not None:
            metrics[&#39;chroma_cosine&#39;] = chroma_cosine.compute()
        metrics = average(metrics)
        metrics = flashy.distrib.average_metrics(metrics, len(loader))

    return metrics</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.generate_audio"><code class="name flex">
<span>def <span class="ident">generate_audio</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Audio generation stage.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_audio(self) -&gt; dict:
    &#34;&#34;&#34;Audio generation stage.&#34;&#34;&#34;
    generate_stage_name = f&#39;{self.current_stage}&#39;
    sample_manager = SampleManager(self.xp)
    self.logger.info(f&#34;Generating samples in {sample_manager.base_folder}&#34;)
    loader = self.dataloaders[&#39;generate&#39;]
    updates = len(loader)
    lp = self.log_progress(generate_stage_name, loader, total=updates, updates=self.log_updates)

    dataset = get_dataset_from_loader(loader)
    dataset_duration = dataset.segment_duration
    assert dataset_duration is not None
    assert isinstance(dataset, AudioDataset)
    target_duration = self.cfg.generate.lm.gen_duration
    prompt_duration = self.cfg.generate.lm.prompt_duration
    if target_duration is None:
        target_duration = dataset_duration
    if prompt_duration is None:
        prompt_duration = dataset_duration / 4
    assert prompt_duration &lt; dataset_duration, (
        f&#34;Specified prompt duration ({prompt_duration}s) is longer&#34;,
        f&#34; than reference audio duration ({dataset_duration}s)&#34;
    )

    def get_hydrated_conditions(meta: tp.List[SegmentWithAttributes]):
        hydrated_conditions = []
        for sample in [x.to_condition_attributes() for x in meta]:
            cond_dict = {}
            for cond_type in sample.__annotations__.keys():
                for cond_key, cond_val in getattr(sample, cond_type).items():
                    if cond_key not in self.model.condition_provider.conditioners.keys():
                        continue
                    if is_jsonable(cond_val):
                        cond_dict[cond_key] = cond_val
                    elif isinstance(cond_val, WavCondition):
                        cond_dict[cond_key] = cond_val.path
                    elif isinstance(cond_val, JointEmbedCondition):
                        cond_dict[cond_key] = cond_val.text  # only support text at inference for now
                    else:
                        # if we reached this point, it is not clear how to log the condition
                        # so we just log the type.
                        cond_dict[cond_key] = str(type(cond_val))
                        continue
            hydrated_conditions.append(cond_dict)
        return hydrated_conditions

    metrics: dict = {}
    average = flashy.averager()
    for batch in lp:
        audio, meta = batch
        # metadata for sample manager
        hydrated_conditions = get_hydrated_conditions(meta)
        sample_generation_params = {
            **{f&#39;classifier_free_guidance_{k}&#39;: v for k, v in self.cfg.classifier_free_guidance.items()},
            **self.generation_params
        }
        if self.cfg.generate.lm.unprompted_samples:
            if self.cfg.generate.lm.gen_gt_samples:
                # get the ground truth instead of generation
                self.logger.warn(
                    &#34;Use ground truth instead of audio generation as generate.lm.gen_gt_samples=true&#34;)
                gen_unprompted_audio = audio
                rtf = 1.
            else:
                gen_unprompted_outputs = self.run_generate_step(
                    batch, gen_duration=target_duration, prompt_duration=None,
                    **self.generation_params)
                gen_unprompted_audio = gen_unprompted_outputs[&#39;gen_audio&#39;].cpu()
                rtf = gen_unprompted_outputs[&#39;rtf&#39;]
            sample_manager.add_samples(
                gen_unprompted_audio, self.epoch, hydrated_conditions,
                ground_truth_wavs=audio, generation_args=sample_generation_params)

        if self.cfg.generate.lm.prompted_samples:
            gen_outputs = self.run_generate_step(
                batch, gen_duration=target_duration, prompt_duration=prompt_duration,
                **self.generation_params)
            gen_audio = gen_outputs[&#39;gen_audio&#39;].cpu()
            prompt_audio = gen_outputs[&#39;prompt_audio&#39;].cpu()
            sample_manager.add_samples(
                gen_audio, self.epoch, hydrated_conditions,
                prompt_wavs=prompt_audio, ground_truth_wavs=audio,
                generation_args=sample_generation_params)

        metrics[&#39;rtf&#39;] = rtf
        metrics = average(metrics)

    flashy.distrib.barrier()
    return metrics</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.get_formatter"><code class="name flex">
<span>def <span class="ident">get_formatter</span></span>(<span>self, stage_name: str) ‑> flashy.formatter.Formatter</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_formatter(self, stage_name: str) -&gt; flashy.Formatter:
    return flashy.Formatter({
        &#39;lr&#39;: &#39;.2E&#39;,
        &#39;ce&#39;: &#39;.3f&#39;,
        &#39;ppl&#39;: &#39;.3f&#39;,
        &#39;grad_norm&#39;: &#39;.3E&#39;,
    }, exclude_keys=[&#39;ce_q*&#39;, &#39;ppl_q*&#39;])</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.load_from_pretrained"><code class="name flex">
<span>def <span class="ident">load_from_pretrained</span></span>(<span>self, name: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_from_pretrained(self, name: str):
    # TODO: support native HF versions of MusicGen.
    lm_pkg = models.loaders.load_lm_model_ckpt(name)
    state: dict = {
        &#39;best_state&#39;: {
            &#39;model&#39;: lm_pkg[&#39;best_state&#39;],
        },
    }
    return state</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.load_state_dict"><code class="name flex">
<span>def <span class="ident">load_state_dict</span></span>(<span>self, state: dict) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_state_dict(self, state: dict) -&gt; None:
    if &#39;condition_provider&#39; in state:
        model_state = state[&#39;model&#39;]
        condition_provider_state = state.pop(&#39;condition_provider&#39;)
        prefix = &#39;condition_provider.&#39;
        for key, value in condition_provider_state.items():
            key = prefix + key
            assert key not in model_state
            model_state[key] = value
    if &#39;compression_model&#39; in state:
        # We used to store the `compression_model` state in the checkpoint, however
        # this is in general not needed, as the compression model should always be readable
        # from the original `cfg.compression_model_checkpoint` location.
        compression_model_state = state.pop(&#39;compression_model&#39;)
        before_hash = model_hash(self.compression_model)
        self.compression_model.load_state_dict(compression_model_state)
        after_hash = model_hash(self.compression_model)
        if before_hash != after_hash:
            raise RuntimeError(
                &#34;The compression model state inside the checkpoint is different&#34;
                &#34; from the one obtained from compression_model_checkpoint...&#34;
                &#34;We do not support altering the compression model inside the LM &#34;
                &#34;checkpoint as parts of the code, in particular for running eval post-training &#34;
                &#34;will use the compression_model_checkpoint as the source of truth.&#34;)

    super().load_state_dict(state)</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.run_generate_step"><code class="name flex">
<span>def <span class="ident">run_generate_step</span></span>(<span>self, batch: Tuple[torch.Tensor, List[<a title="audiocraft.modules.conditioners.SegmentWithAttributes" href="../modules/conditioners.html#audiocraft.modules.conditioners.SegmentWithAttributes">SegmentWithAttributes</a>]], gen_duration: float, prompt_duration: Optional[float] = None, remove_prompt: bool = False, **generation_params) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Run generate step on a batch of optional audio tensor and corresponding attributes.</p>
<h2 id="args">Args</h2>
<dl>
<dt>batch (tuple[torch.Tensor, list[SegmentWithAttributes]]):</dt>
<dt><strong><code>use_prompt</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to do audio continuation generation with prompt from audio batch.</dd>
<dt><strong><code>gen_duration</code></strong> :&ensp;<code>float</code></dt>
<dd>Target audio duration for the generation.</dd>
<dt><strong><code>prompt_duration</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Duration for the audio prompt to use for continuation.</dd>
<dt><strong><code>remove_prompt</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to remove the prompt from the generated audio.</dd>
<dt><strong><code>generation_params</code></strong></dt>
<dd>Additional generation parameters.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>gen_outputs (dict): Generation outputs, consisting in audio, audio tokens from both the generation
and the prompt along with additional information.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def run_generate_step(self, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]],
                      gen_duration: float, prompt_duration: tp.Optional[float] = None,
                      remove_prompt: bool = False,
                      **generation_params) -&gt; dict:
    &#34;&#34;&#34;Run generate step on a batch of optional audio tensor and corresponding attributes.

    Args:
        batch (tuple[torch.Tensor, list[SegmentWithAttributes]]):
        use_prompt (bool): Whether to do audio continuation generation with prompt from audio batch.
        gen_duration (float): Target audio duration for the generation.
        prompt_duration (float, optional): Duration for the audio prompt to use for continuation.
        remove_prompt (bool, optional): Whether to remove the prompt from the generated audio.
        generation_params: Additional generation parameters.
    Returns:
        gen_outputs (dict): Generation outputs, consisting in audio, audio tokens from both the generation
            and the prompt along with additional information.
    &#34;&#34;&#34;
    bench_start = time.time()
    audio, meta = batch
    assert audio.size(0) == len(meta), (
        f&#34;Mismatch between number of items in audio batch ({audio.size(0)})&#34;,
        f&#34; and in metadata ({len(meta)})&#34;
    )
    # prepare attributes
    attributes = [x.to_condition_attributes() for x in meta]
    # TODO: Add dropout for chroma?

    # prepare audio prompt
    if prompt_duration is None:
        prompt_audio = None
    else:
        assert prompt_duration &lt; gen_duration, &#34;Prompt duration must be lower than target generation duration&#34;
        prompt_audio_frames = int(prompt_duration * self.compression_model.sample_rate)
        prompt_audio = audio[..., :prompt_audio_frames]

    # get audio tokens from compression model
    if prompt_audio is None or prompt_audio.nelement() == 0:
        num_samples = len(attributes)
        prompt_tokens = None
    else:
        num_samples = None
        prompt_audio = prompt_audio.to(self.device)
        prompt_tokens, scale = self.compression_model.encode(prompt_audio)
        assert scale is None, &#34;Compression model in MusicGen should not require rescaling.&#34;

    # generate by sampling from the LM
    with self.autocast:
        total_gen_len = math.ceil(gen_duration * self.compression_model.frame_rate)
        gen_tokens = self.model.generate(
            prompt_tokens, attributes, max_gen_len=total_gen_len,
            num_samples=num_samples, **self.generation_params)

    # generate audio from tokens
    assert gen_tokens.dim() == 3
    gen_audio = self.compression_model.decode(gen_tokens, None)

    bench_end = time.time()
    gen_outputs = {
        &#39;rtf&#39;: (bench_end - bench_start) / gen_duration,
        &#39;ref_audio&#39;: audio,
        &#39;gen_audio&#39;: gen_audio,
        &#39;gen_tokens&#39;: gen_tokens,
        &#39;prompt_audio&#39;: prompt_audio,
        &#39;prompt_tokens&#39;: prompt_tokens,
    }
    return gen_outputs</code></pre>
</details>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.show"><code class="name flex">
<span>def <span class="ident">show</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Show the compression model and LM model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show(self) -&gt; None:
    &#34;&#34;&#34;Show the compression model and LM model.&#34;&#34;&#34;
    self.logger.info(&#34;Compression model:&#34;)
    self.log_model_summary(self.compression_model)
    self.logger.info(&#34;LM model:&#34;)
    self.log_model_summary(self.model)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="audiocraft.solvers.base.StandardSolver" href="base.html#audiocraft.solvers.base.StandardSolver">StandardSolver</a></b></code>:
<ul class="hlist">
<li><code><a title="audiocraft.solvers.base.StandardSolver.autocast" href="base.html#audiocraft.solvers.base.StandardSolver.autocast">autocast</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.best_metric_name" href="base.html#audiocraft.solvers.base.StandardSolver.best_metric_name">best_metric_name</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.commit" href="base.html#audiocraft.solvers.base.StandardSolver.commit">commit</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.common_train_valid" href="base.html#audiocraft.solvers.base.StandardSolver.common_train_valid">common_train_valid</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.evaluate" href="base.html#audiocraft.solvers.base.StandardSolver.evaluate">evaluate</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.generate" href="base.html#audiocraft.solvers.base.StandardSolver.generate">generate</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.initialize_ema" href="base.html#audiocraft.solvers.base.StandardSolver.initialize_ema">initialize_ema</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.load_checkpoints" href="base.html#audiocraft.solvers.base.StandardSolver.load_checkpoints">load_checkpoints</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.log_model_summary" href="base.html#audiocraft.solvers.base.StandardSolver.log_model_summary">log_model_summary</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.register_best_state" href="base.html#audiocraft.solvers.base.StandardSolver.register_best_state">register_best_state</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.register_ema" href="base.html#audiocraft.solvers.base.StandardSolver.register_ema">register_ema</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.restore" href="base.html#audiocraft.solvers.base.StandardSolver.restore">restore</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.run" href="base.html#audiocraft.solvers.base.StandardSolver.run">run</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.run_epoch" href="base.html#audiocraft.solvers.base.StandardSolver.run_epoch">run_epoch</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.run_one_stage" href="base.html#audiocraft.solvers.base.StandardSolver.run_one_stage">run_one_stage</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.run_step" href="base.html#audiocraft.solvers.base.StandardSolver.run_step">run_step</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.save_checkpoints" href="base.html#audiocraft.solvers.base.StandardSolver.save_checkpoints">save_checkpoints</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.should_run_stage" href="base.html#audiocraft.solvers.base.StandardSolver.should_run_stage">should_run_stage</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.should_stop_training" href="base.html#audiocraft.solvers.base.StandardSolver.should_stop_training">should_stop_training</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.train" href="base.html#audiocraft.solvers.base.StandardSolver.train">train</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.update_best_state_from_stage" href="base.html#audiocraft.solvers.base.StandardSolver.update_best_state_from_stage">update_best_state_from_stage</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.valid" href="base.html#audiocraft.solvers.base.StandardSolver.valid">valid</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="audiocraft.solvers" href="index.html">audiocraft.solvers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="audiocraft.solvers.musicgen.MusicGenSolver" href="#audiocraft.solvers.musicgen.MusicGenSolver">MusicGenSolver</a></code></h4>
<ul class="">
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.DATASET_TYPE" href="#audiocraft.solvers.musicgen.MusicGenSolver.DATASET_TYPE">DATASET_TYPE</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.build_dataloaders" href="#audiocraft.solvers.musicgen.MusicGenSolver.build_dataloaders">build_dataloaders</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.build_model" href="#audiocraft.solvers.musicgen.MusicGenSolver.build_model">build_model</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.evaluate_audio_generation" href="#audiocraft.solvers.musicgen.MusicGenSolver.evaluate_audio_generation">evaluate_audio_generation</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.generate_audio" href="#audiocraft.solvers.musicgen.MusicGenSolver.generate_audio">generate_audio</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.get_eval_solver_from_sig" href="#audiocraft.solvers.musicgen.MusicGenSolver.get_eval_solver_from_sig">get_eval_solver_from_sig</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.get_formatter" href="#audiocraft.solvers.musicgen.MusicGenSolver.get_formatter">get_formatter</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.load_from_pretrained" href="#audiocraft.solvers.musicgen.MusicGenSolver.load_from_pretrained">load_from_pretrained</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.load_state_dict" href="#audiocraft.solvers.musicgen.MusicGenSolver.load_state_dict">load_state_dict</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.run_generate_step" href="#audiocraft.solvers.musicgen.MusicGenSolver.run_generate_step">run_generate_step</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.show" href="#audiocraft.solvers.musicgen.MusicGenSolver.show">show</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>